---
title: 深度学习模型评估
toc: true
categories:
  - DeepLearning
tags:
  - deeplearning
  - 模型评估
abbrlink: b75ba217
date: 2019-09-21 00:26:10
---

![题图:https://pixabay.com/illustrations/feedback-satisfaction-employee-3240007/](http://image.shuiyujie.com/feedback-3240007_1920.jpg)

如何衡量一个模型的质量？

<!-- more -->

# 分类评测指标

图像分类是计算机视觉中最基础的一个任务，也是几乎所有的基准模型进行比较的任务，从最开始比较简单的10分类的灰度图像手写数字识别mnist，到后来更大一点的10分类的cifar10和100分类的cifar100，到后来的imagenet，图像分类任务伴随着数据库的增长，一步一步提升到了今天的水平。现在在Imagenet这样的超过1000万图像，2万类的数据集中，计算机的图像分类水准已经超过了人类。

图像分类，顾名思义就是一个模式分类问题，它的目标是将不同的图像，划分到不同的类别，实现最小的分类误差，这里我们只考虑单标签分类问题，即每一个图片都有唯一的类别。

**对于单个标签分类的问题，评价指标主要有Accuracy，Precision，Recall，F-score，PR曲线，ROC和AUC。**

在计算这些指标之前，我们先计算几个基本指标，这些指标是基于二分类的任务，也可以拓展到多分类。

- 标签为正样本，分类为正样本的数目为True Positive，简称TP。
- 标签为正样本，分类为负样本的数目为False Negative，简称FN。
- 标签为负样本，分类为正样本的数目为False Positive，简称FP。
- 标签为负样本，分类为负样本的数目为True Negative，简称TN。

判别是否为正例只需要设一个概率阈值T，预测概率大于阈值T的为正类，小于阈值T的为负类，默认就是0.5。如果我们减小这个阀值T，更多的样本会被识别为正类，这样可以提高正类的召回率，但同时也会带来更多的负类被错分为正类。如果增加阈值T，则正类的召回率降低，精度增加。

如果是多类，比如ImageNet1000分类比赛中的1000类，预测类别就是预测概率最大的那一类。

## 准确率Accuracy

单标签分类任务中每一个样本都只有一个确定的类别，预测到该类别就是分类正确，没有预测到就是分类错误，因此最直观的指标就是Accuracy，也就是准确率。

$Accuracy=(TP+TN)/(TP+FP+TN+FN)$，表示的就是所有样本都正确分类的概率，可以使用不同的阈值T。

在ImageNet中使用的Accuracy指标包括Top_1 Accuracy和Top_5 Accuracy，Top_1 Accuracy就是前面计算的Accuracy。

记样本$x_{i}$的类别为$y_{i}$，类别种类为$(0,1,…,C)$，预测类别函数为$f$，则Top-1的计算方法如下：

<img src="http://image.shuiyujie.com/2019-09-21-00-50-52.png" style="zoom:25%;" />

如果给出概率最大的5个预测类别，只要包含了真实的类别，则判定预测正确，计算出来的指标就是Top-5。

目前在ImageNet上，Top-5的指标已经超过95%，而Top-1的指标还在80%左右。

## 精确度Precision和召回率Recall

如果我们只考虑正样本的指标，有两个很常用的指标，精确度和召回率。

|              | 公式                   | 说明                                         |
| ------------ | ---------------------- | -------------------------------------------- |
| 正样本精确率 | $Precision=TP/(TP+FP)$ | 所有被分类为正样本的数据，有多少真的是正样本 |
| 正样本召回率 | $Recall=TP/(TP+FN)$    | 所有正样本，被正确分为正样本的比例           |

## PR曲线

通常召回率越高，精确度越低，根据不同的值可以绘制Recall-Precision曲线，如下。

![PR曲线](http://image.shuiyujie.com/2019-09-21-00-38-29.png)

横轴就是recall，纵轴就是precision，曲线越接近右上角，说明其性能越好，可以用该曲线与坐标轴包围的面积来定量评估，值在0～1之间。

## F1 score

有的时候我们不仅关注正样本的准确率，也关心其召回率，但是又不想用Accuracy来进行衡量，一个折中的指标是采用F-score。

$F1 score=2·Precision·Recall/(Precision+Recall)$，只有在召回率Recall和精确率Precision都高的情况下，F1 score才会很高，因此F1 score是一个综合性能的指标。

## ROC曲线与AUC指标

以上的准确率Accuracy，精确度Precision，召回率Recall，F1 score，混淆矩阵都只是一个单一的数值指标，如果我们想观察分类算法在不同的参数下的表现情况，就可以使用一条曲线，即ROC曲线，全称为receiver operating characteristic。

ROC曲线可以用于评价一个分类器在不同阈值下的表现情况。

在ROC曲线中，每个点的横坐标是false positive rate(FPR)，纵坐标是true positive rate(TPR)，描绘了分类器在True Positive和False Positive间的平衡，两个指标的计算如下：

$TPR=TP/(TP+FN)$，代表分类器预测的正类中实际正实例占所有正实例的比例。

$FPR=FP/(FP+TN)$，代表分类器预测的正类中实际负实例占所有负实例的比例，FPR越大，预测正类中实际负类越多。

ROC曲线通常如下：

![ROC曲线](http://image.shuiyujie.com/2019-09-21-00-40-45.png)

其中有4个关键的点：

- 点(0,0)：FPR=TPR=0，分类器预测所有的样本都为负样本。
- 点(1,1)：FPR=TPR=1，分类器预测所有的样本都为正样本。
- 点(0,1)：FPR=0, TPR=1，此时FN＝0且FP＝0，所有的样本都正确分类。
- 点(1,0)：FPR=1，TPR=0，此时TP＝0且TN＝0，最差分类器，避开了所有正确答案。

**ROC曲线相对于PR曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变，即对正负样本不均衡问题不敏感。**

比如负样本的数量增加到原来的10倍，TPR不受影响，FPR的各项也是成比例的增加，并不会有太大的变化。所以不均衡样本问题通常选用ROC作为评价标准。

ROC曲线越接近左上角，该分类器的性能越好，若一个分类器的ROC曲线完全包住另一个分类器，那么可以判断前者的性能更好。

如果我们想通过两条ROC曲线来定量评估两个分类器的性能，就可以使用**AUC指标**。

AUC（Area Under Curve）为ROC曲线下的面积，它表示的就是一个概率，这个面积的数值不会大于1。随机挑选一个正样本以及一个负样本，AUC表征的就是有多大的概率，分类器会对正样本给出的预测值高于负样本，当然前提是正样本的预测值的确应该高于负样本。

# 检索与回归指标

## IOU

![IOU](http://image.shuiyujie.com/2019-09-21-00-45-23.png)

IoU全称Intersection-over-Union， 即交并比，在目标检测领域中，定义为两个矩形框面积的交集和并集的比值，$IoU=A∩B/A∪B$。

如果完全重叠，则IoU等于1，是最理想的情况。一般在检测任务中，IoU大于等于0.5就认为召回，如果设置更高的IoU阈值，则召回率下降，同时定位框也越更加精确。

在图像分割中也会经常使用IoU，此时就不必限定为两个矩形框的面积。比如对于二分类的前背景分割，那么IoU=(真实前景像素面积∩预测前景像素面积)/(真实前景像素面积∪预测前景像素面积)，这一个指标，通常比直接计算每一个像素的分类正确概率要低，也对错误分类更加敏感。

## AP和mAP

Average Precision简称AP，这是一个在检索任务和回归任务中经常使用的指标，实际等于Precision-Recall曲线下的面积，这个曲线在上一小节已经说过。

在PASCAL VOC 2010年以前的比赛中，AP的具体计算方法如下：

设置11个阈值c$[0, 0.1, 0.2, ... , 1]$，计算recall大于等于每一个阈值时的最大 precision，AP就是这 11 个值的平均值。根据上表中的计算方法，选择不同的N时会有不同的 precision 和 recall，所以也有可能有不同的N落在同样的recall区间，此时就需要选择其中最大的精度值，这时候曲线上的点就不一定对应同一个阈值时的recall。

AP就是这11个precision的平均值，将所有类别的AP再取平均，就得到了mAP。

PASCAL VOC 2010年提出了一个更好的指标，去除了11点的设定，对于样本不均衡的类的计算更加有效。

假设有N个id，其中有M个label，则取M个recall节点，从0到1按照1/M的等间距，对于每个recall值，计算出大于该recall值的最大precision，然后对这M个precision值取平均得到最后的AP值，mAP的计算方法不变。

AP衡量的是学出来的模型在一个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏。

# 参考文献

[1] Yuan Y, Guo Q, Lu X, et al. Image quality assessment[J]. Neurocomputing, 2015: 227-241.

[2] Kamble V, Bhurchandi K M. No-reference image quality assessment algorithms: A survey[J]. Optik, 2015, 126(11): 1090-1097.

[3] 龙鹏. MRI医学图像增强与分割新方法[D]. 中国科学院大学, 2015.

[4] Xu Q , Huang G , Yuan Y , et al. An empirical study on evaluation metrics of generative adversarial networks[J]. 2018.
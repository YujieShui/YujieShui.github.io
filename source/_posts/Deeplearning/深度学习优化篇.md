---
title: 深度学习优化篇
toc: true
categories:
  - DeepLearning
tags:
  - deeplearning
abbrlink: 3f932fee
date: 2019-09-20 21:37:14
---

![深度学习优化篇](https://image.shuiyujie.com/36baea38fd0b45ee.png)

模型要想训得好，参数优化不可少。

<!-- more -->

# 激活函数

一个复杂的神经网络，是有许多层的，其中最基本的单位便是神经元。一个线性神经元不断堆叠下去，永远都是线性关系。这就无法解决非线性问题，而不同的非线性映射函数的选择，就是激活函数的研究课题了。

为了解决非线性的问题，在感知机的基础上发展出了多层神经网络的概念。多层神经网络由输入层、多个中间层，以及输出层组成。

![激活函数](http://image.shuiyujie.com/2019-09-20-21-39-00.png)

解决非线性问题的还可以使用非线性的激活函数，比如 sigmoid function。在 Logistic Regression 中，我们将一个线性函数输入到 sigmoid function 中，能够很好地解决二分类的问题，使用 Softmax 还能进一步解决多分类问题。

最常用激活函数主要有这样几种：

1. sigmoid 和 tanh 激活函数。
2. ReLU 激活函数
3. ReLU 的一大堆变种(LReLU，PReLU，RReLU，ELU，SELU，GELU等等)

# 参数初始化

2006年Hinton等人在science期刊上发表了论文“Reducing the dimensionality of data with neural networks”，揭开了新的训练深层神经网络算法的序幕。

利用无监督的[RBM网络](https://www.jiqizhixin.com/articles/2018-05-07-7)来进行预训练，进行图像的降维，取得比[PCA](https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247498222&idx=2&sn=bf68b0a5e54ec6ea4af2dccfc11de34e&scene=0#wechat_redirect)更好的结果，通常这被认为是**深度学习兴起的开篇**。

**这么看来，是因为好的初始化方法的出现，才有了深层神经网络工程化落地的可能性。**

常用初始化的方法包括：

- 随机初始化
- 标准初始化
- Xavier初始化
- He初始化

# 归一化方法

数据经过归一化和标准化后可以加快梯度下降的求解速度，这就是Batch Normalization等技术非常流行的原因，它使得可以使用更大的学习率更稳定地进行梯度传播，甚至增加网络的泛化能力。常见的归一化的方法有：

- Batch Normalization
- Layer Normalization
- Instance Normalization
- Group Normalization
- Weight Normalization

# 池化

![最大池化与平均池化](http://image.shuiyujie.com/2019-09-20-21-43-19.png)

- 平均池化
- 最大池化
- 随机池化
- 从数据中学习池化

# 正则化方法

![正则化方法](http://image.shuiyujie.com/2019-09-20-22-04-03.png)

- L1/L2参数惩罚
- Dropout与Drop Connect
- 提前终止准则
- 样本扩充

# 最优化方法

![最优化方法](http://image.shuiyujie.com/2019-09-20-22-05-46.png)

- SGD 梯度下降法
- Momentum 动量法
- Nesterov 加速梯度下降法
- Adagrad 算法
- Adadelta/Rmsprop算法
- Adam/Nadam/AdaMax算法
- AMSgrad
- Adafoctor
- 牛顿法/拟牛顿法
- 共轭梯度法

# 参考

[【AI初识境】激活函数：从人工设计到自动搜索](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649031541&idx=1&sn=b1fac1a1bce8cb27727ffea2b77b1689&chksm=8712bd08b065341e0b4078dbd994f864dbd274571668968961881efb4a52ed0822c32a4742ba&mpshare=1&scene=1&srcid=0511JsxVU0ipRGfu2w9cbwpF#rd)

[【AI初识境】什么是深度学习成功的开始？参数初始化](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649031561&idx=1&sn=8de2f0e398c1df0bdaebda99138dc22b&chksm=8712bdf4b06534e2979cca8558f2817d4547676a768f3fc895dd578afda941999e48efd3cafb&mpshare=1&scene=1&srcid=0511qclAEbWkL9nxOQPBGuXW#rd)

[【AI初识境】深度学习模型中的Normalization，你懂了多少？](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649031599&idx=1&sn=f06df4fe57024e7652ac6f6062253b32&chksm=8712bdd2b06534c456f046d76f5f71696f294de6ce0f84736e0cea173eaa970c0a2d0015d72b&mpshare=1&scene=1&srcid=0511A8FHNvsL8uUZk6Kucmzz#rd)

[【AI初识境】被Hinton，DeepMind和斯坦福嫌弃的池化，到底是什么？](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649031740&idx=1&sn=2766cf718daf57a9c7f1556885cf35e9&chksm=8712ba41b065335751aa0a50b6bbb1d6e230ed2f3d9a72914f1eb178ba0c2ecd9f77068fc0c0&mpshare=1&scene=1&srcid=0511pC4Wg0sHIIKiyf7Yoihx#rd)


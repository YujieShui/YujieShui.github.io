{"meta":{"title":"深页","subtitle":"克制而极致","description":null,"author":"深页","url":"http://shuiyujie.com","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-10-02T08:26:14.697Z","updated":"2019-10-02T08:26:14.686Z","comments":true,"path":"404.html","permalink":"http://shuiyujie.com/404.html","excerpt":"","text":"404 Not Found 很抱歉，您访问的页面不存在可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2019-10-02T11:40:41.178Z","updated":"2019-10-02T08:27:05.671Z","comments":true,"path":"About/index.html","permalink":"http://shuiyujie.com/About/index.html","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这是测试博客，请随意留言，但是我不会回复 关于主题本站主题是：Material X One More Thing懒加载占位图如果喜欢请拿去：图片链接 欢迎灌水"},{"title":"Project","date":"2019-01-20T16:04:25.062Z","updated":"2018-05-28T13:33:55.939Z","comments":true,"path":"project/index.html","permalink":"http://shuiyujie.com/project/index.html","excerpt":"","text":""},{"title":"所有文章","date":"2019-10-08T12:52:36.495Z","updated":"2019-10-02T08:31:01.678Z","comments":true,"path":"blog/archives/index.html","permalink":"http://shuiyujie.com/blog/archives/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-10-02T08:19:37.447Z","updated":"2019-10-02T08:19:37.433Z","comments":true,"path":"blog/tags/index.html","permalink":"http://shuiyujie.com/blog/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2019-10-08T12:48:30.834Z","updated":"2019-10-02T08:19:00.518Z","comments":true,"path":"blog/categories/index.html","permalink":"http://shuiyujie.com/blog/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"自定义Bean初始化和销毁方法","slug":"Java/Spring/自定义Bean初始化和销毁方法","date":"2019-10-19T12:22:57.000Z","updated":"2019-10-19T12:39:07.902Z","comments":true,"path":"post/9beaa7c8.html","link":"","permalink":"http://shuiyujie.com/post/9beaa7c8.html","excerpt":"Bean的生命周期指 Bean创建、初始化、销毁的过程。我们可以自定义Bean初始化和销毁方法。容器在bean进行到当前生命周期的时候, 来调用自定义的初始化和销毁方法。 指定 init-method 和 destory-method 实现 InitializingBean 和 DisposableBean 接口 JSR250 规范，使用 @PostConstruct 和 @PreDestroy 使用后置处理器 BeanPostProcessor","text":"Bean的生命周期指 Bean创建、初始化、销毁的过程。我们可以自定义Bean初始化和销毁方法。容器在bean进行到当前生命周期的时候, 来调用自定义的初始化和销毁方法。 指定 init-method 和 destory-method 实现 InitializingBean 和 DisposableBean 接口 JSR250 规范，使用 @PostConstruct 和 @PreDestroy 使用后置处理器 BeanPostProcessor 指定 init-method 和 destory-method@Bean(initMethod=&quot;init&quot;, destroyMethod=&quot;destory&quot;)主要通过 initMethod 和 destroyMethod 来指定 Bean 初始化和销毁的方法。 注: 对于单例的 bean, 可以正常调用初始化和销毁方法、对于多实例的 bean,容器只负责初始化, 但不会管理bean, 容器关闭时不会调用销毁方法 1234567@Configurationpublic class Cap7MainConfigOfLifeCycle &#123; @Bean(initMethod=\"init\", destroyMethod=\"destory\") public Bike bike()&#123; return new Bike(); &#125;&#125; 具体的方法在类中自己自定义。 1234567891011121314public class Bike &#123; // 构造方法，单例作用域下，在创建容器的时候就会调用 public Bike()&#123; System.out.println(\"Bike constructor..............\"); &#125; // 初始化的方法 public void init()&#123; System.out.println(\"Bike .....init.....\"); &#125; // 销毁方法 public void destory()&#123; System.out.println(\"Bike.....destory\"); &#125;&#125; 测试类： 12345678910public class Cap7Test &#123; @Test public void test01()&#123; AnnotationConfigApplicationContext app = new AnnotationConfigApplicationContext(Cap7MainConfigOfLifeCycle.class); System.out.println(\"IOC容器创建完成........\"); // 关闭容器，销毁 bean app.close(); &#125;&#125; 实现 InitializingBean 和 DisposableBean 接口 实现 InitializingBean 接口的 afterPropertiesSet() 方法,当 beanFactory 创建好对象,且把 bean 所有属性设置好之后,会调这个方法,相当于初始化方法 实现 DisposableBean 的 destory() 方法,当 bean 销毁时,会把单实例 bean 进行销毁 12345678910111213141516171819@Componentpublic class Train implements InitializingBean, DisposableBean&#123; public Train()&#123; System.out.println(\"Train......constructor............\"); &#125; //当我们bean销毁时,调用此方法 @Override public void destroy() throws Exception &#123; System.out.println(\"Train......destory......\"); //logger.error &#125; //当我们的bean属性赋值和初始化完成时调用 @Override public void afterPropertiesSet() throws Exception &#123; System.out.println(\"Train.......afterPropertiesSet()...\"); &#125;&#125; 使用 @PostConstruct 和 @PreDestroy可以使用 JSR250 规则定义的(java规范)两个注解来实现 @PostConstruct: 在Bean创建完成,且属于赋值完成后进行初始化,属于JDK规范的注解 @PreDestroy: 在bean将被移除之前进行通知, 在容器销毁之前进行清理工作 提示: JSR是由JDK提供的一组规范 123456789101112131415@Componentpublic class Jeep &#123; public Jeep()&#123; System.out.println(\"Jeep.....constructor........\"); &#125; @PostConstruct public void init()&#123; System.out.println(\"Jeep.....@PostConstruct........\"); &#125; @PreDestroy public void destory()&#123; System.out.println(\"Jeep.....@PreDestroy......\"); &#125;&#125; 使用后置处理器 BeanPostProcessorbean 的后置处理器,在 bean 初始化之前调用进行拦截,在 bean 初始化前后进行一些处理工作。使用BeanPostProcessors 如何控制 Bean 的生命周期;实现 BeanPostProcessors 的两个接口 postProcessBeforeInitialization() 和 postProcessAfterInitialization()。 12345678910111213141516@Componentpublic class JamesBeanPostProcessor implements BeanPostProcessor&#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; //返回一个的对象(传过来的对象) //在初始化方法调用之前进行后置处理工作, //什么时候调用它: init-method=init之前调用 System.out.println(\"postProcessBeforeInitialization....\"+beanName+\"...\"+bean); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println(\"postProcessAfterInitialization....\"+beanName+\"...\"+bean); return bean; &#125;&#125;","categories":[],"tags":[]},{"title":"Spring Bean 的生命周期","slug":"Spring-Bean-的生命周期","date":"2019-10-19T10:52:49.000Z","updated":"2019-10-19T10:52:49.953Z","comments":true,"path":"post/6433d4b8.html","link":"","permalink":"http://shuiyujie.com/post/6433d4b8.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Spring常用注解","slug":"Java/Spring/Spring常用注解","date":"2019-10-19T05:57:48.000Z","updated":"2019-10-19T10:37:12.145Z","comments":true,"path":"post/1a003b7b.html","link":"","permalink":"http://shuiyujie.com/post/1a003b7b.html","excerpt":"本文介绍一些 Spring 中常用注解的使用方式。","text":"本文介绍一些 Spring 中常用注解的使用方式。 @Configuration声明配置类@Configuration用来申明一个配置类，相当于写了一个配置文件。我们可以使用 Spring 的容器 AnnotationConfigApplicationContext 可以加载这个配置来获取 Bean。 Spring 中有两种容器 AnnotationConfigApplicationContext 是我们使用注解方式进行配置时使用的容器，ClassPathXmlApplicationContext 是我们使用 xml 方式进行配置时使用的容器。 接下来，举例说明如何使用这两种方式。 XML 方式进行配置最开始的时候我们使用 xml 的方式来配置 bean，然后用ClassPathXmlApplicationContext 初始化容器： 12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"person\" class=\"com.enjoy.cap1.Person\"&gt; &lt;property name=\"name\" value=\"james\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"19\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 12345678910public class MainTest1 &#123; public static void main(String args[])&#123; // 加载 beans.xml ApplicationContext app = new ClassPathXmlApplicationContext(\"beans.xml\"); // 获取 bean Person person = (Person) app.getBean(\"person\"); System.out.println(person); &#125;&#125; 注解方式进行配置使用 @Configuration 注解来申明一个配置类，接着使用 AnnotationConfigApplicationContext 来初始化容器。 1234567@Configurationpublic class MainConfig &#123; @Bean public Person person()&#123; return new Person(\"james\",20); &#125;&#125; 1234567891011public class MainTest2 &#123; public static void main(String args[])&#123; ApplicationContext app = new AnnotationConfigApplicationContext(MainConfig.class); String[] namesForBean = app.getBeanNamesForType(Person.class); for(String name:namesForBean)&#123; System.out.println(name); &#125; &#125;&#125; @ComponentScan 扫描规则@ComponentScan会去扫描 @Component、@Controller、@Service、@Respostry注解的类。同时，它也可以指定扫描范围、配置扫描过滤器、自定义扫描规则。 指定扫描范围，使用 value=&quot;com.shuiyujie.xxx&quot; 配置扫描过滤器，通过 includeFilters 来指定扫描哪些类，excludeFilters 来指定不扫描哪些类。指定时要配置 @Filter 和 useDefaultFilters 参数。 自定义扫描规则，自定义一个类实现 TypeFilter 接口，自定义过滤规则。 12345678910111213141516171819@Configuration/** * FilterType 有多种类型： * - ANNOTATION, * - ASSIGNABLE_TYPE, * - ASPECTJ, * - REGEX, * - CUSTOM; */@ComponentScan(value=\"com.enjoy.cap2\", includeFilters=&#123; @Filter(type=FilterType.CUSTOM, classes=&#123;JamesTypeFilter.class&#125;) &#125;, useDefaultFilters=false)public class Cap2MainConfig &#123; @Bean public Person person01()&#123; return new Person(\"james\",20); &#125;&#125; 123456789101112131415161718192021222324252627public class JamesTypeFilter implements TypeFilter&#123; private ClassMetadata classMetadata; /* * MetadataReader:读取到当前正在扫描类的信息 * MetadataReaderFactory:可以获取到其他任何类信息 */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; //获取当前类注解的信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); //获取当前正在扫描的类信息 classMetadata = metadataReader.getClassMetadata(); //获取当前类资源(类的路径) Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println(\"-----&gt;\"+className); if(className.contains(\"order\"))&#123;//当类包含er字符, 则匹配成功,返回true return true; &#125; return false; &#125;&#125; @Scope 指定 Bean 的作用域范围给容器中注册一个 Bean, 类型为返回值的类型, 默认是单例。singleton 是默认创建 Bean 的方式，它会在 Spring 容器初始化的时候就创建 Bean，这个 Bean 有且只会创建这样一次。prototype 则不会在 Spring 容器初始化的时候创建，而是在使用这个 Bean (比如getBean())的时候实例化化一个 Bean。 prototype:多实例: IOC容器启动的时候,IOC容器启动并不会去调用方法创建对象, 而是每次获取的时候才会调用方法创建对象 singleton:单实例(默认):IOC容器启动的时候会调用方法创建对象并放到IOC容器中,以后每次获取的就是直接从容器中拿(大Map.get)的同一个bean request: 主要针对web应用, 递交一次请求创建一个实例 session:同一个session创建一个实例 12345678@Configurationpublic class Cap3MainConfig &#123; @Scope(\"prototype\") @Bean public Person person()&#123; return new Person(\"james\",20); &#125;&#125; @Lazy 懒加载我们知道 Bean 默认是 Singleton 的，它会在容器启动的时候就创建这个对象。如果我们不希望 Singleton 的 Bean 在容器初始化的时候就创建，而是希望它在使用对象的时候创建，就可以使用懒加载的方式，具体来说就是在 Bean 上添加 @Lazy 。 123456789@Configurationpublic class Cap4MainConfig &#123; @Lazy @Bean public Person person()&#123; System.out.println(\"给容器中添加person.......\"); return new Person(\"james\",20); &#125;&#125; @Conditional条件注册bean添加@Conditional的 Bean 可以自定义加载条件，比如说在不同的操作系统中加载不同的 Bean。关键在于配置的类需要实现 Condition 接口，接着可以在 matches() 中获取到更多信息。 12345678910111213141516171819202122@Configurationpublic class Cap5MainConfig &#123; @Bean(\"person\") public Person person()&#123; System.out.println(\"给容器中添加person.......\"); return new Person(\"person\",20); &#125; @Conditional(WinCondition.class) @Bean(\"lison\") public Person lison()&#123; System.out.println(\"给容器中添加lison.......\"); return new Person(\"Lison\",58); &#125; @Conditional(LinCondition.class) @Bean(\"james\")//bean在容器中的ID为james, IOC容器MAP, map.put(\"id\",value) public Person james()&#123; System.out.println(\"给容器中添加james.......\"); return new Person(\"james\",20); &#125;&#125; WinCondition 实现了 Condition 接口，通过 context 获取操作系统信息。 1234567891011121314151617181920public class WinCondition implements Condition&#123; /* * ConditionContext: 判断条件可以使用的上下文(环境) * AnnotatedTypeMetadata: 注解的信息 */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; // TODO 是否为WINDOW系统 //能获取到IOC容器正在使用的 beanFactory ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); //获取当前环境变量(包括我们操作系统是WIN还是LINUX??) Environment environment = context.getEnvironment(); String os_name = environment.getProperty(\"os.name\"); if(os_name.contains(\"Windows\"))&#123; return true; &#125; return false; &#125;&#125; @Import注册 Bean在容器中注册组件的方式有： @Bean，导入第三方的类或包的组件，比如说我们自己定义一个 Person 类 包扫描+组件注解，即@ComponentScan+Component、@Controller、@Service、@Reponsitory，一般用于我们自己写的类 @Import快速给容器导入一个类，能做比@Bean更加复杂的操作 @Import 要导入到容器中的组件):容器会自动注册这个组件,bean 的 id为全类名 ImportSelector:是一个接口,返回需要导入到容器的组件的全类名数组 ImportBeanDefinitionRegistrar:可以手动添加组件到IOC容器, 所有Bean的注册可以使用BeanDifinitionRegistry 使用Spring提供的FactoryBean(工厂bean)进行注册 123456789101112131415161718192021222324252627@Configuration@Import(value = &#123; Dog.class,Cat.class, JamesImportSelector.class,JamesImportBeanDefinitionRegistrar.class &#125;)public class Cap6MainConfig &#123; /* * 给容器中注册组件的方式 * 1,@Bean: [导入第三方的类或包的组件],比如Person为第三方的类, 需要在我们的IOC容器中使用 * 2,包扫描+组件的标注注解(@ComponentScan: @Controller, @Service @Reponsitory @ Componet),一般是针对 我们自己写的类,使用这个 * 3,@Import:[快速给容器导入一个组件] 注意:@Bean有点简单 * a,@Import(要导入到容器中的组件):容器会自动注册这个组件,bean 的 id为全类名 * b,ImportSelector:是一个接口,返回需要导入到容器的组件的全类名数组 * c,ImportBeanDefinitionRegistrar:可以手动添加组件到IOC容器, 所有Bean的注册可以使用BeanDifinitionRegistry * 写JamesImportBeanDefinitionRegistrar实现ImportBeanDefinitionRegistrar接口即可 * 4,使用Spring提供的FactoryBean(工厂bean)进行注册 * * */ //容器启动时初始化person的bean实例 @Bean(\"person\") public Person person()&#123; return new Person(\"james\",20); &#125; @Bean public JamesFactoryBean jamesFactoryBean()&#123; return new JamesFactoryBean(); &#125;&#125; 1234567public class JamesImportSelector implements ImportSelector&#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata)&#123; //返回全类名的bean return new String[]&#123;\"com.enjoy.cap6.bean.Fish\",\"com.enjoy.cap6.bean.Tiger\"&#125;; &#125;&#125; 123456789101112131415161718192021public class JamesImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /* *AnnotationMetadata:当前类的注解信息 *BeanDefinitionRegistry:BeanDefinition注册类 * 把所有需要添加到容器中的bean加入; * @Scope */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean bean1 = registry.containsBeanDefinition(\"com.enjoy.cap6.bean.Dog\"); boolean bean2 = registry.containsBeanDefinition(\"com.enjoy.cap6.bean.Cat\"); //如果Dog和Cat同时存在于我们IOC容器中,那么创建Pig类, 加入到容器 //对于我们要注册的bean, 给bean进行封装, if(bean1 &amp;&amp; bean2)&#123; RootBeanDefinition beanDefinition = new RootBeanDefinition(Pig.class); registry.registerBeanDefinition(\"pig\", beanDefinition); &#125; &#125;&#125; 12345678910111213141516171819public class JamesFactoryBean implements FactoryBean&lt;Monkey&gt;&#123; @Override public Monkey getObject() throws Exception &#123; // TODO Auto-generated method stub return new Monkey(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; // TODO Auto-generated method stub return Monkey.class; &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://shuiyujie.com/tags/spring/"}]},{"title":"Spring MVC执行流程","slug":"Java/Spring/Spring-MVC执行流程","date":"2019-10-18T14:08:26.000Z","updated":"2019-10-18T14:48:05.143Z","comments":true,"path":"post/8cca277c.html","link":"","permalink":"http://shuiyujie.com/post/8cca277c.html","excerpt":"简单来说：客户端发送请求 -&gt; 前端控制器 DispatcherServlet 接受客户端请求 -&gt; 找到处理器映射 HandlerMapping 解析请求对应的 Handler -&gt; HandlerAdapter 会根据 Handler 来调用真正的处理器来处理请求，并处理相应的业务逻辑 -&gt; 处理器返回一个模型视图 ModelAndView -&gt; 视图解析器进行解析 -&gt; 返回一个视图对象 -&gt; 前端控制器 DispatcherServlet 渲染数据（Model）-&gt; 将得到视图对象返回给用户。","text":"简单来说：客户端发送请求 -&gt; 前端控制器 DispatcherServlet 接受客户端请求 -&gt; 找到处理器映射 HandlerMapping 解析请求对应的 Handler -&gt; HandlerAdapter 会根据 Handler 来调用真正的处理器来处理请求，并处理相应的业务逻辑 -&gt; 处理器返回一个模型视图 ModelAndView -&gt; 视图解析器进行解析 -&gt; 返回一个视图对象 -&gt; 前端控制器 DispatcherServlet 渲染数据（Model）-&gt; 将得到视图对象返回给用户。 1、DispatcherServlet：前端控制器 用户请求到达前端控制器，它就相当于 MVC 模式中的 C，DispatcherServlet 是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet 的存在降低了组件之间的耦合性。 2、HandlerMapping：处理器映射器 HandlerMapping 负责根据用户请求找到 Handler 即处理器，SpringMVC 提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、Handler：处理器 Handler 是继 DispatcherServlet 前端控制器的后端控制器，在 DispatcherServlet 的控制下Handler 对具体的用户请求进行处理。 由于 Handler 涉及到具体的用户业务请求，所以一般情况需要程序员根据业务需求开发Handler。 4、HandlAdapter：处理器适配器 通过 HandlerAdapter 对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 5、View Resolver：视图解析器 View Resolve r负责将处理结果生成 View 视图，View Resolver 首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成 View 视图对象，最后对 View 进行渲染将处理结果通过页面展示给用户。 6、View：视图 SpringMVC 框架提供了很多的 View 视图类型的支持，包括：jstlView、freemarkerView、pdfView 等。我们最常用的视图就是jsp。 第二十三章：深入了解SpringMVC执行流程","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"spring mvc","slug":"spring-mvc","permalink":"http://shuiyujie.com/tags/spring-mvc/"}]},{"title":"Java中的内部类","slug":"Java中的内部类","date":"2019-10-14T15:17:52.000Z","updated":"2019-10-18T14:44:00.901Z","comments":true,"path":"post/eab6ab72.html","link":"","permalink":"http://shuiyujie.com/post/eab6ab72.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"MySQL读写分离原理","slug":"MySQL读写分离原理","date":"2019-10-13T07:38:55.000Z","updated":"2019-10-18T14:44:00.902Z","comments":true,"path":"post/ce64ad4b.html","link":"","permalink":"http://shuiyujie.com/post/ce64ad4b.html","excerpt":"","text":"业务上的层级拆分：关联查询如何进行跨库的查询？ 主从复制，数据不一致问题 第三方插件，半同步 一主一从，并不能提高程序的性能，但是能容灾，mater 挂了，slave 顶上。不需要考虑数据一致性问题，因为 slave 只是用来做备份的 作者：Gary Chen 读写分离指的是，通过增加一些节点，扩展读的能力。这些节点可以是主节点的全部内容的副本或者部分内容的副本，也可以是缓存产品。读写分离一般配合负载均衡产品使用。 对于读多写少（非更新查询为主）的负载，特别适合做读写分离。你需要留意的是，保证用户感知到自己所做的变更生效即可，用户很多情况下并不需要知道其他用户的改变。如果用户对于数据的一致性要求在某个时刻高，那么这部分数据，建议不要使用读写分离，MySQL的复制可能出现延时，无法满足业务需要。你可以采取变通的方式，比如，在用户修改了内容后，临时强制用户访问主节点，以获取一致性的数据，在过一段时间后，再让用户访问副本的数据，一般在此时，副本的数据已经同步到最新状态了。 读写分离往往和负载均衡技术配合使用。负载均衡软硬件产品有F5、Haproxy以及一些自己设计的MySQL Proxy代理等，负载均衡可以更高效的利用硬件，你可以设置权重，分配更多的流量给性能更好的机器，负载均衡产品一般还有故障检测、自动冗余切换功能，这可以大大提高你的可用性。 读写分离技术的一个难点在于延时的影响，你需要有一个手段确保你没有读取到太旧的数据，写操作和一些不能容忍延时的查询，需要指向主库。对于数据延时敏感度不高的数据，你需要定义延时的阈值，通过自动或者手工的方式处理延时数据对于用户体体验的影响。 你可以通过监控SHOW SLAVE STATUS里的输出Seconds_behind_master的方式判断是否有延时，但这种方式不太可靠。监控复制滞后（replication lag）的更稳健的方式是通过心跳表的方式。 我们很难确保MySQL的延时，因为网络波动、复制异常、性能问题都可能导致复制中断，而往往需要人工来进行干预，毕竟有能力开发专用的Proxy代理的公司很少，所以，不建议使用读写分离，采用读写分离是基于一个前提，主库已经出现读瓶颈，如果出现读瓶颈，使用缓存一般是更有效、更成熟的解决方案。 由于没有好的读写分离的方案，如果你一定要使用读写分离，推荐应用程序自身实现读写分离，把读的流量指向负载均衡产品或者Proxy代理。 读写分离，又往往和拆分数据有关联。部分数据更适合读写分离，部分数据更适合拆分。对于一致性要求很高的数据，往往很难以去拆分，使用读写分离保证适当的用户体验更可取。 如果一定要拆分，很可能导致架构、运维异常复杂。 在做数据架构的时候，要清楚数据的用户，细分各种数据，针对各种数据，进行各种处理，确保异常情况下的可用、可恢复，而不是简单的考虑 读写分离、水平扩展、垂直扩展这些更简单的分解。 https://zhuanlan.zhihu.com/p/24698266","categories":[],"tags":[]},{"title":"图解GC的过程","slug":"Java/JVM/图解GC的过程","date":"2019-10-10T14:44:13.000Z","updated":"2019-10-10T14:45:59.385Z","comments":true,"path":"post/98255841.html","link":"","permalink":"http://shuiyujie.com/post/98255841.html","excerpt":"本文想要聊一聊 GC 流程这样一个话题。具体来说就是 Minor GC 的过程是怎么样的？Full GC 的过程又是怎么样的？这一切都要从「压死骆驼的最后一根稻草」说起。 后面的文字请配合流程图来食用。","text":"本文想要聊一聊 GC 流程这样一个话题。具体来说就是 Minor GC 的过程是怎么样的？Full GC 的过程又是怎么样的？这一切都要从「压死骆驼的最后一根稻草」说起。 后面的文字请配合流程图来食用。 挤满新生代的最后一个对象我们应当知道，新创建的对象一般会被分配在新生代中。常用的新生代的垃圾回收器是 ParNew 垃圾回收器，它按照 8:1:1 将新生代分成 Eden 区，以及两个 Survivor 区。 某一时刻，我们创建的对象将 Eden 区全部挤满，这个对象就是「挤满新生代的最后一个对象」。此时，Minor GC 就触发了。 正式 Minor GC 前的检查在正式 Minor GC 前，JVM 会先检查新生代中对象，是比老年代中剩余空间大还是小。为什么要做这样的检查呢？原因很简单，假如 Minor GC 之后 Survivor 区放不下剩余对象，这些对象就要进入到老年代，所以要提前检查老年代是不是够用。这样就有两种情况： 老年代剩余空间大于新生代中的对象大小，那就直接 Minor GC，GC 完 survivor 不够放，老年代也绝对够放 老年代剩余空间小于新生代中的对象大小，这个时候就要查看是否启用了「老年代空间分配担保规则」，具体来说就是看-XX:-HandlePromotionFailure参数是否设置了（一般都会设置） 老年代空间分配担保规则是这样的。如果老年代中剩余空间大小，大于历次 Minor GC 之后剩余对象的大小，那就允许进行 Minor GC。因为从概率上来说，以前的放的下，这次的也应该放的下。那就有两种情况： 老年代中剩余空间大小，大于历次 Minor GC 之后剩余对象的大小，进行 Minor GC 老年代中剩余空间大小，小于历次 Minor GC 之后剩余对象的大小，进行 Full GC，把老年代空出来再检查 Minor GC 后的处境前面说了，开启老年代空间分配担保规则只能说是大概率上来说，Minor GC 剩余后的对象够放到老年代，所以当然也会有万一，Minor GC 后会有这样三种情况： Minor GC 之后的对象足够放到 Survivor 区，皆大欢喜，GC 结束 Minor GC 之后的对象不够放到 Survivor 区，接着进入到老年代，老年代能放下，那也可以，GC 结束 Minor GC 之后的对象不够放到 Survivor 区，老年代也放不下，那就只能 Full GC 实在不行只能 OOM前面都是成功 GC 的例子，还有 3 中情况，会导致 GC 失败，报 OOM： 紧接上一节 Full GC 之后，老年代任然放不下剩余对象，就只能 OOM 未开启老年代分配担保机制，且一次 Full GC 后，老年代任然放不下剩余对象，也只能 OOM 开启老年代分配担保机制，但是担保不通过，一次 Full GC 后，老年代任然放不下剩余对象，也是能 OOM","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"},{"name":"gc","slug":"gc","permalink":"http://shuiyujie.com/tags/gc/"}]},{"title":"JVM中的垃圾回收算法","slug":"Java/JVM/JVM中的垃圾回收算法","date":"2019-10-08T14:53:49.000Z","updated":"2019-10-08T15:24:57.268Z","comments":true,"path":"post/e2ebbcff.html","link":"","permalink":"http://shuiyujie.com/post/e2ebbcff.html","excerpt":"我们知道当堆中内存满了的时候，JVM就会使用可达性分析算法，检查对象是否有被 GC Root 引用。如果一个对象没有被任何 GC Root 引用，那就说明它是一个无效的对象，就会被垃圾回收器回收。此外，即使是被一个 GC Root 引用，但如果是弱引用，那这个对象任然有被回收的风险。 现在我们知道了什么情况下一个对象会被回收，那 JVM 是怎么回收一个对象的呢？","text":"我们知道当堆中内存满了的时候，JVM就会使用可达性分析算法，检查对象是否有被 GC Root 引用。如果一个对象没有被任何 GC Root 引用，那就说明它是一个无效的对象，就会被垃圾回收器回收。此外，即使是被一个 GC Root 引用，但如果是弱引用，那这个对象任然有被回收的风险。 现在我们知道了什么情况下一个对象会被回收，那 JVM 是怎么回收一个对象的呢？ 前面说了没有被 GC Root 引用的对象将会被回收。现在我们模拟新生代满了，然后标记出没有被 GC Roots 引用的对象，并进行回收： 假如说我们将无效的对象直接回收，那么内存中就会出现上图所示这样一个个空缺，这些空缺叫做「内存碎片」。出现过多的内存碎片会产生什么问题么？ 可以想象我们总的剩余的内存很多，但是都是一块块不连续的空间。这时候进来一个新的对象，即使总的剩余内存足够多，但是这个对象可能无法找到一块足够大的连续内存。总的来说就是内存碎片越多，内存的利用率就越低。 复制算法复制算法就是为了解决内存碎片问题，它将新生代分成两块区域。使用其中一块内存来存放对象，当它满了之后，任然回收其中无效的对象。不过在回收无效对象之后，它会把一块内存中所有存活的对象，紧挨着拷贝到另一个块内存中。如下图所示： 不过我们可以思考一下这样使用复制算法有什么弊端？弊端其实也是很明显的，就是只能使用一半的内存，解决了内存碎片问题，空间利用率还是没有上去。 优化的复制算法优化的复制算法将新生代按照 8:1:1 分成 Eden区和两个 Survivor 区。当 Eden 区满的时候就会触发 Minor GC，回收 Eden 区及其中一个 Survivor 区中的对象，并将回收之后存活的对象都复制到另一个 Survivor 区。 由于新生代中最终能存活下来的对象在少数，也就是说在一次垃圾回收一直大部分的对象都会消失，所以一个不是很大的 Survivor 区就足够容纳幸存的对象了。Eden 区又比之前普通的复制算法可以利用的区域大不少，大大提高了新生代的内存利用率。 标记清理算法前面介绍了新生代常用的垃圾回收算法，复制算法。再来看看老年代常用的垃圾回收算法，标记清理算法。 老年代一般选择的垃圾回收器是 CMS，采用标记清理算法。简单来说，标记清理算法分成两步：标记出哪些对象是垃圾对象，再一次性把这些对象清理掉。 举例来说，当程序触发了Full GC 回收老年代的垃圾对象。 先通过追踪 GC Roots 的方法，看老年代中的各个对象是不是被 GC Roots 引用，如果被引用就是存活对象，否则就是垃圾对象。这些垃圾对象就会被标记出来。 然后再一次性清理被标记出来的垃圾对象。 但是由于 Full GC 的时间比 Minor GC 的时间差不多长 10 倍，如果和 Minor GC 一样，Full GC 采用先 Stop the World 再用标记清理算法回收垃圾，就会导致系统卡死时间过长，很多响应无法处理。 所以 CMS 让系统一边工作，一边进行垃圾回收。CMS 垃圾回收的过程一共分为 4 个阶段： 初始标记。Stop the World 停止所有工作线程，仅标记 GC Roots 直接引用的对象。这一步速度很快。 并发标记。此时允许系统线程创建对象，继续运行。垃圾回收线程则会追踪老年代中的对象是否从根源上被 GC Roots 引用。这一步是最耗时的，但是和系统程序并发运行，不会对系统运行造成影响。 重新标记。Stop the World 重新标记在第二阶段里新创建的对象，以及原本被引用现在失去引用的对象。由于只对第二阶段有变动的对象进行标记，速度也比较快。 并发清理。系统随意运行，同时清理之前被标记为垃圾的对象。这个阶段很耗时，但是由于和系统程序并发运行，并不影响系统程序的运行。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"JVM分代模型","slug":"Java/JVM/JVM分代模型","date":"2019-10-08T12:53:49.000Z","updated":"2019-10-08T14:28:30.936Z","comments":true,"path":"post/7c59c32c.html","link":"","permalink":"http://shuiyujie.com/post/7c59c32c.html","excerpt":"重新回过头来看一些 JVM，思考一个问题：分出一个永久代我可以理解，为什么又要把堆分成新生代和老年代呢？私以为划分成新生代和老年代是因为它们里面对象的性质不同，要用不同的垃圾回收算法来回收效率才高，是为垃圾回收器服务才分出新生代和老年代的。 本文说明新生代、老年代和永久代中对象的特性，以及何时它们何时触发 GC。","text":"重新回过头来看一些 JVM，思考一个问题：分出一个永久代我可以理解，为什么又要把堆分成新生代和老年代呢？私以为划分成新生代和老年代是因为它们里面对象的性质不同，要用不同的垃圾回收算法来回收效率才高，是为垃圾回收器服务才分出新生代和老年代的。 本文说明新生代、老年代和永久代中对象的特性，以及何时它们何时触发 GC。 12345堆的大小可以用这几个参数来设置：1. “-XX:InitialHeapSize”和“-XX:MaxHeapSize” 就是初始堆大小和最大堆大小2. “-XX:NewSize”和“-XX:MaxNewSize” 是初始新生代大小和最大新生代大小3. “-XX:PretenureSizeThreshold=10485760” 指定了大对象阈值是10MB。 新生代大部分对象存活的周期很短，就会放在新生代中。ParNew垃圾收集器会使用复制算法，回收新生代中的对象。复制算法中会将新生代分成 Eden 区和两个 Survivor 区，当 Eden 区满的时候就会触发垃圾回收算法。 老年代少部分对象存活周期比较长，或者因为一些特殊原因会进入到老年代。那么什么样的情况下，对象会进入老年代呢？ 一个实例对象在新生代中，成功躲过 15 次垃圾，就说明他已经15岁了。成为老人的它就能进入到老年代了。 具体几岁算是一个老人可以通过 -XX:MaxTenuringThreshold 参数进行设置。 动态年龄判断，当前放对象的 Survivor 区域里，一批对象的总大小大于了这块 Survivor区域内存大小的50%，那么此时大于等于这批对象年龄的对象，就可以直接进入老年代。 大对象直接进入老年代，可以通过 -XX:PretenureSizeThreshold 参数进行设置 Minor GC 之后发现剩余对象太多，无法放到另一个 Survivor 区，这些对象会直接进入老年代 老年代什么时候会触发 Old GC 呢？其实总结起来有 2 种情况： 老年代未开启分配担保，老年代对象大小小于新生代中对象大小；老年代开启分配担保，但是老年代对象大小小于历次 Minor GC 之后进入老年代对象的大小。以上情况会进行 Old GC。 经过 Minor GC 之前的检查，正式进行 Minor GC 后，Survivor 放不下剩余对象，老年代也放不下剩余对象，就会进行 Old GC。 如果 Old GC 之后老年代还是放不下剩余对象，就会 OOM。 具体关于老年代未开启分配机制，以及触发 Minor GC 和 Old GC 流程的内容，可以看《图解GC的过程》 永久代JVM 里的永久代其实就是方法区，可以认为永久代就是放一些类信息的。 永久代一般不会发生 GC，具体情况还不是很清楚，以后会补充。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"什么是内存溢出？在哪些区域会发生内存溢出？","slug":"Java/JVM/什么是内存溢出？在哪些区域会发生内存溢出？","date":"2019-10-08T12:09:27.000Z","updated":"2019-10-10T14:46:20.107Z","comments":true,"path":"post/8bc388ce.html","link":"","permalink":"http://shuiyujie.com/post/8bc388ce.html","excerpt":"什么是内存溢出？在哪些区域会发生内存溢出？回答这个问题，我们需要先看一看Java代码是怎么运行的。现在计入说我们写了一个 HelloWorld.class： 123456public class HelloWorld &#123; public static void main(String[] args)&#123; String message = \"Hello World\"; System.out.println(message); &#125;&#125; 我们来分析一下这段代码的运行情况：","text":"什么是内存溢出？在哪些区域会发生内存溢出？回答这个问题，我们需要先看一看Java代码是怎么运行的。现在计入说我们写了一个 HelloWorld.class： 123456public class HelloWorld &#123; public static void main(String[] args)&#123; String message = \"Hello World\"; System.out.println(message); &#125;&#125; 我们来分析一下这段代码的运行情况： 源文件 HelloWorld.java 将会被编译成可执行文件 HelloWorld.class 类加载加载可执行文件到 Metaspace，Metaspace 保存类的基本信息，如果加载太多就会 OOM Java是多线程的，运行代码的时候会启动一个线程。main()是Java程序的入口，首先会启动一个 main 线程。每个线程都有 Java 虚拟机栈，每执行一个方法都会有一个栈帧入栈，栈帧中包含参数、局部变量、返回值地址等信息。如果代码层次太深，不断有方法入栈却没有出栈，Java虚拟机栈就会 OOM。 栈中的局部变量如果是一个对象，那就会在初始化的时候在堆中创建对象。堆中创建的对象过多就会触发 GC，GC 的速度赶不上新建对象的速度也会发生 OOM。 从 Java 代码的运行过程来看，有三个区域会发生 OOM，它们分别是：Metaspace、Java 虚拟机栈、堆内存。 Metaspace 是如何溢出的？Metaspace 中会加载类，保存各种类信息，它的大小可以通过 -XX:MetaspaceSize=512m 和 -XX:MaxMetaspaceSize=512m 来设置。 当我们不断地创建类，不断地创建类，把 Metaspace 的内存都给占满了。这个时候就会触发 Full GC，Full GC 一般会顺带着进行 Minor GC 回收年轻带，也会进行 Old GC 回收老年代。 如果 Full GC 回收 Metespace 中的空间之后，任然无法存放新建的对象，此时 OOM 就会发生了。 不过 Metaspace 一般很少发生 OOM，如果发生了一般是出于下面两个原因： 上线系统的时候对 Metaspace 区域直接用默认的参数，即根本不设置其大小。默认的Metaspace区域可能才几十MB，对于稍微大型的系统，他自己有很多类，还依赖了很多外部的jar包，几十MB的Metaspace很容易就不够了。 写代码的时候用到了类似与 cglib 的动态代理技术，代码写得不好就会导致创建太多的类。 线程中的栈是如何溢出的？虚拟机中的栈内存也是有限的，我们调用方法的时候会创建一个栈帧，紧接着方法入栈。如果一个线程一直调用方法入栈，栈内存终归是要满的，此时线程的栈中就会发生 OOM。 发生这种情况一般就是代码除了问题，比如写了个递归调用，和 Metaspace 的内存溢出一样，也很少发生。 堆内存是如何溢出的？最常见的就是堆内存溢出的问题了。堆内存有限的情况下，不断创建对象并触发 GC，但是 GC 过后任然有大量大象存活，此时还要放入大量新的对象，这就会导致堆内存溢出问题。 一般来说对内存溢出有这样两个场景： 高并发场景下，请求量太大，创建了大量新的对象，且这些都是有用的、存活的。堆中无法放入更多对象就会导致堆内存溢出 内存泄漏问题，长生命周期的对象引用了大量短生命周期的对象，没有及时取消对它们的引用，导致 GC 无法回收这些理应被回收的对象，就导致了堆内存溢出","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"上手Spring Cloud","slug":"Java/Spring Cloud/上手Spring-Cloud","date":"2019-10-02T12:33:21.000Z","updated":"2019-10-06T07:54:46.995Z","comments":true,"path":"post/a22e1643.html","link":"","permalink":"http://shuiyujie.com/post/a22e1643.html","excerpt":"记得一年前 Spring Cloud、Docker + k8s 才刚才火起来呀，当时公司的系统刚转成 Docker 部署，使用的分布式框架任然是 Dubbo。 现在也就相隔一年半， JD 上 Spring Cloud 已经成标配了，份额看着比 Dubbo 还大。Docker + k8s 也已经是一个加分项被写在 JD 上了。 为了知识体系的完整性，今天撸了一个 Spring Cloud 的 Demo。","text":"记得一年前 Spring Cloud、Docker + k8s 才刚才火起来呀，当时公司的系统刚转成 Docker 部署，使用的分布式框架任然是 Dubbo。 现在也就相隔一年半， JD 上 Spring Cloud 已经成标配了，份额看着比 Dubbo 还大。Docker + k8s 也已经是一个加分项被写在 JD 上了。 为了知识体系的完整性，今天撸了一个 Spring Cloud 的 Demo。 Demo 用到了 Spring Cloud 最核心的几个功能，上图来说明系统架构。 服务注册中心 Eureka：用于服务的注册与发现，服务注册中心保存每一个系统服务名称、IP 地址和端口等信息。服务调用的时候根据服务名称到注册中心找注册信息，就可以实现远程服务的调用。 负载均衡 Ribbon：Ribbon 是 Netflix 发布的负载均衡器。Ribbon 和 Eureka 配合使用，Ribbon 从 EurekaServer 获取一份注册表，再使用负责均衡算法选择调用哪个服务。比如说这里的订单系统要调用库存系统，库存系统在 Eureka 注册了两个服务，Ribbon 就会帮我们选择调用哪一个库存服务。 接口调用 Fegin：微服务之间通过 Rest 接口通讯，Spring Cloud 中使用 Feign 来调用 Rest 接口。 微服务网关 Zuul：由于我们使用的是微服务，可能有成百上千的接口，不可能让前端记住这么多接口然后来调用。而且这么多接口暴露出来也存在很大的安全分线。所以使用 Zuul 来做转发服务、安全认证的事情。 以上，一套 Spring Cloud 的核心架构就齐活了，当然还有服务熔断、配置管理的功能没有加上，作为一个 Demo 是够了。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"spring cloud","slug":"spring-cloud","permalink":"http://shuiyujie.com/tags/spring-cloud/"}]},{"title":"深度学习环境的搭建","slug":"Deeplearning/深度学习环境的搭建","date":"2019-09-20T16:57:39.000Z","updated":"2019-10-02T11:43:30.673Z","comments":true,"path":"post/bb0e0670.html","link":"","permalink":"http://shuiyujie.com/post/bb0e0670.html","excerpt":"一份深度学习环境配置的备忘。","text":"一份深度学习环境配置的备忘。 安装 Ubuntu16.04推荐使用 Linux 系统，可以少给自己找些麻烦，既然要吃程序员这口饭，使用 Linux 这关是逃不开的。 Linux 有许多发行版，具体用哪个版本倒不必过分细究，不过还是推荐使用稳定的、使用人数多的发行版，比如 Ubuntu16.04 LST。 首先需要做一个 U 盘启动盘，接着分区。最好上 SSD 用来装系统，再挂一个 4T 的机械硬盘，分区可以如下： 系统区用 efi 格式，200M 或者 300M 左右差不多了 swap 分区，32G 左右差不多 剩余部分 EXT4 格式挂到根目录/下 最后把机械硬盘挂载上 系统安装上之后还有些动作是需要做的： 更换默认的源 安装个搜狗输入法，打字方便不少 安装个 open ssh，肯定会远程连的 其实还有的折腾，装机先到这里吧。 常出版本问题的库有几个库经常出一些问题，比如说 numpy、cuda/cudnn 和 protobuf。 numpyNumpy是不得不用的python科学计算基础库，被几乎所有的深度学习框架依赖，目前版本已经到1.6。 Tips：Numpy出问题，基本上就是版本问题，框架未必支持最新版，最好不要装最新版本的，要自己制定版本安装。 cuda/cudnn搞深度学习得有块 GPU 吧，我看有很多人用 cuda8，不过还是比较推荐安装 cuda10，最近刚出的 TensorFlow2.0 就必须使用 cuda10。先安装 nvidia-driver ，再安装 cuda。 注:NVIDIM 的驱动要在 cuda 之前安装，见NVIDIA 环境配置这一小节。 官网下载链接 https://developer.nvidia.com/cuda-downloads 123456789101112# 显卡驱动安装1.下载官网内Linux版本的.run安装包2. cd xxx/xxx #以目录usr/ser为例,通过cd usr/ser进入安装包存放目录3. sudo service lightdm stop #关闭图形界面4. ctrl+alt+F1 #若此时长时间黑屏，通过此命令进入命令行5. cd xxx/xxx #在命令行进入安装包存放目录6. sudo bash NVIDIA-Linux-x86_64-xx.xx.run --no-opengl-files #只安装驱动文件，默认enter选项，进入命令行 #–no-opengl-files 只安装驱动文件，不安装OpenGL文件。这个参数最重要7. sudo reboot #重启电脑。8. nvidia-smi或nvidia-settings #显示驱动详细信息，即安装成功 protobuf当初安装 caffe 的时候，因为 protobuf 的版本问题被折腾得够呛，可以用 protobuf --version 看看当前的版本，不出意外因该是 2.6.1，有点低。 建议自己找个目录安装 3.4 以上的版本，和系统隔离，让一些包比如caffe编译的时候依赖上自己的这个库。 https://github.com/protocolbuffers/protobuf/releases 常用软件接着可以安装 opencv、caffe、tensorflow、pytorch、anaconda 了。 OpenCVOpenCV 安装写过一篇：Ubuntu16.04配置OpenCV环境。 还有视频教学的: http://space.bilibili.com/365916694? NVIDIA 环境配置 配置 nvidia-driver 环境，官网下载链接： https://www.nvidia.com/Download/index.aspx?lang=cn。 12345678910111213141516171819202122# 命令行下载sudo ubuntu-drivers devices# 以上命令可能会遇到问题The program &apos;ubuntu-drivers&apos; is currently not installed. You can install it by typing:sudo apt install ubuntu-drivers-common# 进入命令行界面 Ctrl + ALt + F1# 比如当我们安装 NVIDIA 的驱动程序时，就需要先关闭 X serversudo service lightdm stop# 以上命令可能遇到问题Failed to stop lightdm.service: Unit lightdm.service not loaded.how-to-install-nvidia-run# 安装命令# 只安装驱动文件，不安装 opengl 文件sudo ./NVIDIA*.run -no-opengl-files # 重启 X servicesudo service lightdm start# 查看驱动安装结果nvdia-smi Ubuntu 16.04 关闭x server Ubuntu中LightDM是什么（转） nvdia 驱动安装常见问题 笔记本双显卡系统，登录界面无限循环，无法进入桌面 普通笔记本默认采用集显作为视频输出，此时没有关闭 opengl 文件的安装，会继续使用 ubuntu 默认的 nouveau 驱动，而后者已经被禁掉 sudo ./NVIDIA*.run -no-opengl-files 只安装驱动文件，不安装 opengl 文件 The Nouveau Kernel driver is currently in use by your system 禁用 ubuntu 默认的驱动 nouveau vim /etc/modprobe.d/blacklist.conf blacklist nouveau # 禁用 nouveau 驱动 sudo update0initramfs -u # 更新 kernel lsmod | grep nouveau # 查看是否更新 https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07 CaffeCaffe 挺不好装的，可以 fork 一下这个项目 https://github.com/longpeng2008/Caffe_Long 然后参照 这个 Makefile。这里最好指定 opencv 和 protobuf 的位置。 nvidia-driver, cuda, caffe 是递进的依赖关系，后者必须依赖于前置，另外 cudnn 的安装时可选的 本文安装方法只使用与 ubuntu 系统 一般安装过程不顺利，问题出在 nvida-driver 上 Caffe 安装Caffe 安装可以参见 Caffe Installation，高于 17.04 的 ubuntu 可以一键安装 caffe，不过我们将在 ubuntu16.04 上安装 caffe。 安装 caffe 需要依赖很多东西，安装顺序也是有讲究的： 升级 pip 和 pip3，理顺 ubuntu 上自带的的 python 环境 自行编译 protobuf https://github.com/protocolbuffers/protobuf/blob/master/src/README.md 自行编译 openblas https://blog.csdn.net/y5492853/article/details/79558194 安装 opencv http://wykvictor.github.io/2018/08/01/OpenCV-6.html 这里需要自己再扩展一下，哪些装哪些不装 安装 NVIDIA 和 cuda 安装 caffe 最后安装 Anaconda，不然会对编译 opencv 造成影响 How To Install the Anaconda Python Distribution on Ubuntu 16.04 123456789101112131415161718sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-devprotobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libatlas-base-devsudo install OpenBLASsudo apt-get install libopenblas-dev==============================================# 使用三哥定制版本的 caffegit clone https://github.com/YujieShui/Caffe_Long# 接着修改 makefile==============================================# 这个是使用官网版本的 caffe，我这里使用三哥定制版本的 caffegit clone https://github.com/BVLC/caffe.gitcd caffe# 此时编译就会遇到很多的问题,看接下里给出的解决方案,再编译cp Makefile.config.example Makefile.configmake -j4 Caffe 配置文件解析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697## Refer to http://caffe.berkeleyvision.org/installation.html # Contributions simplifying and improving our build system are welcome! # cuDNN acceleration switch (uncomment to build with cuDNN). # 是否使用 cudnn 加速# USE_CUDNN := 1 # CPU-only switch (uncomment to build without GPU support). # CPU_ONLY := 1 # uncomment to disable IO dependencies and corresponding data layers # 是否使用功能 opencv# USE_OPENCV := 0 # 是否使用 LEVELDB 输入格式# USE_LEVELDB := 0 # 是否使用 USE_LMDB 输入格式# USE_LMDB := 0 # uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary) # You should not set this flag if you will be reading LMDBs with any # possibility of simultaneous read and write # ALLOW_LMDB_NOLOCK := 1 # Uncomment if you're using OpenCV 3 # 是否使用 opencv3，这里开启OPENCV_VERSION := 3 # To customize your choice of compiler, uncomment and set the following. # N.B. the default for Linux is g++ and the default for OSX is clang++ # CUSTOM_CXX := g++ # CUDA directory contains bin/ and lib/ directories that we need. # cuda 的目录CUDA_DIR := /usr/local/cuda-8.0 # On Ubuntu 14.04, if cuda tools are installed via # \"sudo apt-get install nvidia-cuda-toolkit\" then use this instead: # CUDA_DIR := /usr # CUDA architecture setting: going with all of them. # For CUDA &lt; 6.0, comment the *_50 lines for compatibility. # 与 cuda 架构有关 http://www.caffecn.cn/?/question/1077# cuda10 要注释掉前两个CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\ -gencode arch=compute_20,code=sm_21 \\ -gencode arch=compute_30,code=sm_30 \\ -gencode arch=compute_35,code=sm_35 \\ -gencode arch=compute_50,code=sm_50 \\ -gencode arch=compute_50,code=compute_50 # BLAS choice: # 矩阵加速库选择# atlas for ATLAS (default) # mkl for MKL # open for OpenBlas BLAS := open # Custom (MKL/ATLAS/OpenBLAS) include and lib directories. # Leave commented to accept the defaults for your choice of BLAS # (which should work)! BLAS_INCLUDE := /opt/OpenBLAS/include BLAS_LIB := /opt/OpenBLAS/lib # Homebrew puts openblas in a directory that is not on the standard search path # BLAS_INCLUDE := $(shell brew --prefix openblas)/include # BLAS_LIB := $(shell brew --prefix openblas)/lib # This is required only if you will compile the matlab interface. # MATLAB directory should contain the mex binary in /bin. # MATLAB_DIR := /usr/local/MATLAB/R2014b # MATLAB_DIR := /Applications/MATLAB_R2012b.app # NOTE: this is required only if you will compile the python interface. # We need to be able to find Python.h and numpy/arrayobject.h. # python 路径和 python 接口，PYTHON_INCLUDE PYTHON_LIB， WITH_PYTHON_LAYERPYTHON_INCLUDE := /usr/include/python2.7 \\ /usr/lib/python2.7/dist-packages/numpy/core/include # Anaconda Python distribution is quite popular. Include path: # Verify anaconda location, sometimes it's in root. # ANACONDA_HOME := $(HOME)/anaconda # PYTHON_INCLUDE := $(ANACONDA_HOME)/include \\ # $(ANACONDA_HOME)/include/python2.7 \\ # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \\ # We need to be able to find libpythonX.X.so or .dylib. PYTHON_LIB := /usr/lib # PYTHON_LIB := $(ANACONDA_HOME)/lib # Homebrew installs numpy in a non standard path (keg only) # PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include # PYTHON_LIB += $(shell brew --prefix numpy)/lib # Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1 # Whatever else you find you need goes here. # 其它依赖库，这里依赖了自己编译的 protobuf 和 opencvINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/local/include /usr/include/hdf5/serial /home/longpeng/opts/opencv3.2/include /home/longpeng/opts/protobuf3.1/include /usr/local/lib/python2.7/dist-packages/numpy/core/include/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/local/lib64 /usr/lib/x86_64-linux-gnu/hdf5/serial /usr/local/cuda-8.0/lib64 /home/longpeng/opts/opencv3.2/lib /home/longpeng/opts/protobuf3.1/lib # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies # INCLUDE_DIRS += $(shell brew --prefix)/include # LIBRARY_DIRS += $(shell brew --prefix)/lib # Uncomment to use `pkg-config` to specify OpenCV library paths. # (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.) # USE_PKG_CONFIG := 1 BUILD_DIR := build DISTRIBUTE_DIR := distribute #build with support for Python layers WITH_PYTHON_LAYER:=1 # Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171 # DEBUG := 1 # The ID of the GPU that 'make runtest' will use to run unit tests. TEST_GPUID := 0 # enable pretty build (comment to see full commands) Q ?= @ Caffe 编译问题列表1234567891011121314151617# without using a build folder, for me there will be linking problem says libcaffe.o cant not find “xxx&quot;https://github.com/BVLC/caffe/issues/2348#issuecomment-97093859# fatal error: hdf5.h: No such file or directoryhttp://homeway.me/2018/01/25/setup-caffe-for-deep-learning/# Unsupported gpu architecture ‘compute_20&apos;https://blog.csdn.net/kemgine/article/details/78781377http://www.caffecn.cn/?/question/1077# 使用 opencv3OPENCV_VERSION := 3https://github.com/BVLC/caffe/issues/3517# 是否使用 python 定义网络,要打开# Uncomment to support layers written in Python (will link against Python libs)WITH_PYTHON_LAYER := 1 AnacondaAnaconda 在 OpenCV 和 Caffe 装好之后装，不然会导致 OpenCV 和 Caffe 的 Python 库依赖到 Anaconda 上去，我们希望它们使用系统的 Python。 具体安装我有写过一篇: 如何使用Anaconda TensorFlow现在 TensorFlow2.0 已经出了，不过没有发行正式版，无法通过 anaconda 安装，需要通过 pip 进行安装。 前面的环境配置好之后，TensorFlow 的安装并不难。 其它其它可能还会安装一下 Docker，日常用的软件等。原则就是用到啥就下载啥，用啥自己心里要有数，别把环境搞乱就行。 不过也别害怕，不是生产环境顶多重新配，就是会浪费点时间，还是谨慎点好。 Ubuntu平台实录——CUDA安装","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]},{"title":"深度学习模型评估","slug":"Deeplearning/深度学习模型评估","date":"2019-09-20T16:26:10.000Z","updated":"2019-09-20T16:55:36.633Z","comments":true,"path":"post/b75ba217.html","link":"","permalink":"http://shuiyujie.com/post/b75ba217.html","excerpt":"如何衡量一个模型的质量？","text":"如何衡量一个模型的质量？ 分类评测指标图像分类是计算机视觉中最基础的一个任务，也是几乎所有的基准模型进行比较的任务，从最开始比较简单的10分类的灰度图像手写数字识别mnist，到后来更大一点的10分类的cifar10和100分类的cifar100，到后来的imagenet，图像分类任务伴随着数据库的增长，一步一步提升到了今天的水平。现在在Imagenet这样的超过1000万图像，2万类的数据集中，计算机的图像分类水准已经超过了人类。 图像分类，顾名思义就是一个模式分类问题，它的目标是将不同的图像，划分到不同的类别，实现最小的分类误差，这里我们只考虑单标签分类问题，即每一个图片都有唯一的类别。 对于单个标签分类的问题，评价指标主要有Accuracy，Precision，Recall，F-score，PR曲线，ROC和AUC。 在计算这些指标之前，我们先计算几个基本指标，这些指标是基于二分类的任务，也可以拓展到多分类。 标签为正样本，分类为正样本的数目为True Positive，简称TP。 标签为正样本，分类为负样本的数目为False Negative，简称FN。 标签为负样本，分类为正样本的数目为False Positive，简称FP。 标签为负样本，分类为负样本的数目为True Negative，简称TN。 判别是否为正例只需要设一个概率阈值T，预测概率大于阈值T的为正类，小于阈值T的为负类，默认就是0.5。如果我们减小这个阀值T，更多的样本会被识别为正类，这样可以提高正类的召回率，但同时也会带来更多的负类被错分为正类。如果增加阈值T，则正类的召回率降低，精度增加。 如果是多类，比如ImageNet1000分类比赛中的1000类，预测类别就是预测概率最大的那一类。 准确率Accuracy单标签分类任务中每一个样本都只有一个确定的类别，预测到该类别就是分类正确，没有预测到就是分类错误，因此最直观的指标就是Accuracy，也就是准确率。 $Accuracy=(TP+TN)/(TP+FP+TN+FN)$，表示的就是所有样本都正确分类的概率，可以使用不同的阈值T。 在ImageNet中使用的Accuracy指标包括Top_1 Accuracy和Top_5 Accuracy，Top_1 Accuracy就是前面计算的Accuracy。 记样本$x_{i}$的类别为$y_{i}$，类别种类为$(0,1,…,C)$，预测类别函数为$f$，则Top-1的计算方法如下： 如果给出概率最大的5个预测类别，只要包含了真实的类别，则判定预测正确，计算出来的指标就是Top-5。 目前在ImageNet上，Top-5的指标已经超过95%，而Top-1的指标还在80%左右。 精确度Precision和召回率Recall如果我们只考虑正样本的指标，有两个很常用的指标，精确度和召回率。 公式 说明 正样本精确率 $Precision=TP/(TP+FP)$ 所有被分类为正样本的数据，有多少真的是正样本 正样本召回率 $Recall=TP/(TP+FN)$ 所有正样本，被正确分为正样本的比例 PR曲线通常召回率越高，精确度越低，根据不同的值可以绘制Recall-Precision曲线，如下。 横轴就是recall，纵轴就是precision，曲线越接近右上角，说明其性能越好，可以用该曲线与坐标轴包围的面积来定量评估，值在0～1之间。 F1 score有的时候我们不仅关注正样本的准确率，也关心其召回率，但是又不想用Accuracy来进行衡量，一个折中的指标是采用F-score。 $F1 score=2·Precision·Recall/(Precision+Recall)$，只有在召回率Recall和精确率Precision都高的情况下，F1 score才会很高，因此F1 score是一个综合性能的指标。 ROC曲线与AUC指标以上的准确率Accuracy，精确度Precision，召回率Recall，F1 score，混淆矩阵都只是一个单一的数值指标，如果我们想观察分类算法在不同的参数下的表现情况，就可以使用一条曲线，即ROC曲线，全称为receiver operating characteristic。 ROC曲线可以用于评价一个分类器在不同阈值下的表现情况。 在ROC曲线中，每个点的横坐标是false positive rate(FPR)，纵坐标是true positive rate(TPR)，描绘了分类器在True Positive和False Positive间的平衡，两个指标的计算如下： $TPR=TP/(TP+FN)$，代表分类器预测的正类中实际正实例占所有正实例的比例。 $FPR=FP/(FP+TN)$，代表分类器预测的正类中实际负实例占所有负实例的比例，FPR越大，预测正类中实际负类越多。 ROC曲线通常如下： 其中有4个关键的点： 点(0,0)：FPR=TPR=0，分类器预测所有的样本都为负样本。 点(1,1)：FPR=TPR=1，分类器预测所有的样本都为正样本。 点(0,1)：FPR=0, TPR=1，此时FN＝0且FP＝0，所有的样本都正确分类。 点(1,0)：FPR=1，TPR=0，此时TP＝0且TN＝0，最差分类器，避开了所有正确答案。 ROC曲线相对于PR曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变，即对正负样本不均衡问题不敏感。 比如负样本的数量增加到原来的10倍，TPR不受影响，FPR的各项也是成比例的增加，并不会有太大的变化。所以不均衡样本问题通常选用ROC作为评价标准。 ROC曲线越接近左上角，该分类器的性能越好，若一个分类器的ROC曲线完全包住另一个分类器，那么可以判断前者的性能更好。 如果我们想通过两条ROC曲线来定量评估两个分类器的性能，就可以使用AUC指标。 AUC（Area Under Curve）为ROC曲线下的面积，它表示的就是一个概率，这个面积的数值不会大于1。随机挑选一个正样本以及一个负样本，AUC表征的就是有多大的概率，分类器会对正样本给出的预测值高于负样本，当然前提是正样本的预测值的确应该高于负样本。 检索与回归指标IOU IoU全称Intersection-over-Union， 即交并比，在目标检测领域中，定义为两个矩形框面积的交集和并集的比值，$IoU=A∩B/A∪B$。 如果完全重叠，则IoU等于1，是最理想的情况。一般在检测任务中，IoU大于等于0.5就认为召回，如果设置更高的IoU阈值，则召回率下降，同时定位框也越更加精确。 在图像分割中也会经常使用IoU，此时就不必限定为两个矩形框的面积。比如对于二分类的前背景分割，那么IoU=(真实前景像素面积∩预测前景像素面积)/(真实前景像素面积∪预测前景像素面积)，这一个指标，通常比直接计算每一个像素的分类正确概率要低，也对错误分类更加敏感。 AP和mAPAverage Precision简称AP，这是一个在检索任务和回归任务中经常使用的指标，实际等于Precision-Recall曲线下的面积，这个曲线在上一小节已经说过。 在PASCAL VOC 2010年以前的比赛中，AP的具体计算方法如下： 设置11个阈值c$[0, 0.1, 0.2, … , 1]$，计算recall大于等于每一个阈值时的最大 precision，AP就是这 11 个值的平均值。根据上表中的计算方法，选择不同的N时会有不同的 precision 和 recall，所以也有可能有不同的N落在同样的recall区间，此时就需要选择其中最大的精度值，这时候曲线上的点就不一定对应同一个阈值时的recall。 AP就是这11个precision的平均值，将所有类别的AP再取平均，就得到了mAP。 PASCAL VOC 2010年提出了一个更好的指标，去除了11点的设定，对于样本不均衡的类的计算更加有效。 假设有N个id，其中有M个label，则取M个recall节点，从0到1按照1/M的等间距，对于每个recall值，计算出大于该recall值的最大precision，然后对这M个precision值取平均得到最后的AP值，mAP的计算方法不变。 AP衡量的是学出来的模型在一个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏。 参考文献[1] Yuan Y, Guo Q, Lu X, et al. Image quality assessment[J]. Neurocomputing, 2015: 227-241. [2] Kamble V, Bhurchandi K M. No-reference image quality assessment algorithms: A survey[J]. Optik, 2015, 126(11): 1090-1097. [3] 龙鹏. MRI医学图像增强与分割新方法[D]. 中国科学院大学, 2015. [4] Xu Q , Huang G , Yuan Y , et al. An empirical study on evaluation metrics of generative adversarial networks[J]. 2018.","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"模型评估","slug":"模型评估","permalink":"http://shuiyujie.com/tags/模型评估/"}]},{"title":"计算机视觉中的数据增强","slug":"Deeplearning/计算机视觉中的数据增强","date":"2019-09-20T14:26:22.000Z","updated":"2019-09-30T14:49:08.609Z","comments":true,"path":"post/8e94d850.html","link":"","permalink":"http://shuiyujie.com/post/8e94d850.html","excerpt":"在实际工作中有很大一个难题就是收集到足够的数据，而数据就像是燃料，没有足够的数据训练的模型效果不会太好。所以要保证完美完成任务，有两件事必须要做好： 寻找更多的数据 充分利用已有的数据进行数据增强 本文就来聊聊数据增强的话题。 覆盖主流数据增强算法 https://github.com/aleju/imgaug 数据增强注意事项： 在线使用，不要离线生产样本 不要过度使用，过多的数据增强可能增加训练难度和事件 有些任务某些增强不能做，比如姿态估计中随机反转，旋转","text":"在实际工作中有很大一个难题就是收集到足够的数据，而数据就像是燃料，没有足够的数据训练的模型效果不会太好。所以要保证完美完成任务，有两件事必须要做好： 寻找更多的数据 充分利用已有的数据进行数据增强 本文就来聊聊数据增强的话题。 覆盖主流数据增强算法 https://github.com/aleju/imgaug 数据增强注意事项： 在线使用，不要离线生产样本 不要过度使用，过多的数据增强可能增加训练难度和事件 有些任务某些增强不能做，比如姿态估计中随机反转，旋转 为什么要了解数据增强Q: 多少数据才能满足项目要求？ 任务类型有关，越精细任务要求越多（分类 &gt; 检测 &gt; 分割） 任务难度 ImageNet（每个类约500个），Place365（每个类约5000个） 精度要求有关（学术任务，工业级产品） Q: 数据增强和泛化能力的关系？增强模型泛化能力的方法： 显示正则化（模型集成，参数正则化等） 隐式正则化（数据增强，随机梯度下降等） Q: 什么是数据增强？数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。 eg. 分辨率大小为 256x256 的图像，随机裁剪成 224x224 的代销，一张图就可以产生 32x32 张不同的图，数据量将近扩充 1000 倍。 Q: 通过裁剪得到的很多图像相似度很高，那么这样的数据增强有效果么？操作虽然简单，但是有效果，如果辅助其他数据增强的方法，将获得更好的多样性，这就是数据增强的本质。 常见数据增强的方法单样本几何变换 翻转，旋转，裁剪，缩放 没有改变图像本身的内容，选择了图像的一部分或者像素的空间重分布 单样本像素内容变换 噪声，模糊，颜色扰动 内容改动大，多样性强 多样本插值 SMOT Synthetic Minority Over-sampling Technique 方法，近邻样本(Xn,Yn)插值 Sample Pairing 随机抽取两张图片分别经过基础数据增强操作（如随机翻转等）处理后，直接叠加合成一个新的样本，标签为原样本标签中的一种 多样本插值 Mixup ICLR2018, Facebook 的研究 Mixup，图像和标签都进行线性插值 自动选择数据增强方法 学习已有数据增强操作的组合，不同的任务，需要不同的数据增强操作。 AutoAugmentation 原理 从 16 个常用数据增强操作中选择 5 个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个 sub-policy，一共产生 5 个 sub-polices 对训练过程中每一个 batch 的图片，随机采用 5 个 sub-polices 操作中的一种 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法 经过 80~100 个 epoch 后网络开始学习到有效的 sub-policies 之后串接这 5 个 sub-policies，然后再进行最后的训练 从零制作新的样本 使用生成对抗网络，生成新的样本。 开源框架中的数据增强 caffe 中数据增强： caffe.proto 中增加参数 Datatransform 中 .cpp 函数实现 Datatransform 中 .cpp 函数调用 train.prototxt 中网络配置 在 caffe.proto 中增加参数 TensorFlow 数据增强 —— tf.image API pytorch 数据增强 —— torchvision.transforms API mxnet 数据增强 —— mxnet.image API 参考文献[1] Chawla N V, Bowyer K W, Hall L O, et al. SMOTE: synthetic minority over-sampling technique[J]. Journal of Artificial Intelligence Research, 2002, 16(1):321-357. [2] Inoue H. Data Augmentation by Pairing Samples for Images Classification[J]. 2018. [3] Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond Empirical Risk Minimization[J]. 2017. [4] Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative Adversarial Networks[J]. Advances in Neural Information Processing Systems, 2014, 3:2672-2680. [5] Cubuk E D, Zoph B, Mane D, et al. AutoAugment: Learning Augmentation Policies from Data.[J]. arXiv: Computer Vision and Pattern Recognition, 2018.","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"数据增强","slug":"数据增强","permalink":"http://shuiyujie.com/tags/数据增强/"}]},{"title":"数据集-ImageNet","slug":"Deeplearning/数据集-ImageNet","date":"2019-09-20T14:11:54.000Z","updated":"2019-09-30T14:50:09.612Z","comments":true,"path":"post/9dd18af0.html","link":"","permalink":"http://shuiyujie.com/post/9dd18af0.html","excerpt":"ImageNet 是李飞飞实验室主导的一个项目,目标是构建一个计算机视觉研究的大型数据库,关键词从 WordNet 中选取.完整的 ImageNet 数据集有 1400 多万幅图片,涵盖 2 万多个类别的标注与超过百万的边界狂标注,每一个类别大概是 500~1000 张图片,标注采用亚马逊的众包平台,这个平台之后被用于构建各种数据集。 2009年CVPR会议室李飞飞实验室正式发布了 ImageNet，此后从2010年到2017年共举办了8届 Large Scale Visual Recognition Challenge，即为人熟知的 yILSVRC比赛，包括图像分类，目标检测，目标定位单元。 数据集地址: http://www.image-net.org/.","text":"ImageNet 是李飞飞实验室主导的一个项目,目标是构建一个计算机视觉研究的大型数据库,关键词从 WordNet 中选取.完整的 ImageNet 数据集有 1400 多万幅图片,涵盖 2 万多个类别的标注与超过百万的边界狂标注,每一个类别大概是 500~1000 张图片,标注采用亚马逊的众包平台,这个平台之后被用于构建各种数据集。 2009年CVPR会议室李飞飞实验室正式发布了 ImageNet，此后从2010年到2017年共举办了8届 Large Scale Visual Recognition Challenge，即为人熟知的 yILSVRC比赛，包括图像分类，目标检测，目标定位单元。 数据集地址: http://www.image-net.org/. 在最开始的时候， ImageNetR是一个连经费都申请不到的整理数据的项目，为很多专注于算法研究的学术人员不耻，但科技公司的竞赛热情以及数据集史无前例的多样性，让陷身于过拟合的算法，从数据集本身看到了新的出路，之后极大的推进了计算机视觉任务的发展。 由于类别多， ImageNet MNIST数据集整理过程中那样简单的将所有图片分成互不相干的10类，而是采用了中树形结构的组织方式。比如从哺乳动物关键词开始整理，然后不断细分到有胎盘哺乳动物，到肉食动物，到犬科动物，到狗，到工作中的狗，到哈士奇。 起源2005年，李飞飞从加州理工大学拿到电子工程学博士学位后进入了学术界，开始在伊利诺伊州香槟分校担任教职。那时她看到整个学术界和工业界重心都放在如何做出更好的算法，认为无论数据如何，只要算法好就会有好的决策。她意识到了这样做的局限，并且想到了一个解决方法，就是做一个能详细描绘出整个世界物体的数据集。她回忆起当时的情景深情地说道；“当时每个人对此都是一副怀疑的态度，但是 Kai Li（ 李凯，普林斯顿Princeton大学教授，美国工程院院士） 做了两件厉害的事情，他说：‘飞飞，你的教授生涯刚刚开始，你想要做的事情，我实验室的所有的机器都可以拿来帮你，而且我还会给你一个学生。’如果没有这样的支持，我是没办法开始做 ImageNet 的。”于是李飞飞、Jia Deng（ 邓嘉，李飞飞的学生） 等研究员在 CVPR 2009 上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文，没过多久，这个数据集就迅速发展成一项竞赛，通过对数据集中的物体进行识别，选出识别错误率最低的算法。 比赛自2010年以来，每年度ImageNet大规模视觉识别挑战赛（ILSVRC），研究团队在给定的数据集上评估其算法，并在几项视觉识别任务中争夺更高的准确性。 ILSVRC旨在“追踪2005年建立的规模较小的PASCAL VOC挑战”，该挑战仅包含大约20000个图像和20个目标类别。 ILSVRC使用仅包含1000个图像类别或“类别”的“修剪”列表，其中120个品种中有90个由完整的ImageNet架构分类。 2010年在图像处理方面取得了显着进展。 2011年左右，ILSVRC分类错误率为25%。 2012年，深卷积神经网络达到了16%;在接下来的几年中，错误率下降到几个百分点。虽然2012年的突破是“前所未有的组合”，但大幅量化的改进标志着全行业人工智能繁荣的开始。到2015年，研究人员报告说，软件在狭窄的ILSVRC任务中超出人类能力。然而，作为挑战组织者之一的Olga Russakovsky在2015年指出，这些计划只需将图像识别为属于千分之一的图像;人类可以识别更多的类别，并且（不像程序）可以判断图像的上下文。2017年，38个竞争团队中有29个错误率低于5%。识别错误率已经远远低于人类的5.1%， ImageNet宣布ILSVRC比赛不再举办，原因是已经失去意义，大家都这么高啦，差别都都很小，再比下去有点浪费大师们的时间。另外就是这个比赛已经证明了在视觉感知这块计算机已经可以战胜人类了，ImageNet竞赛初衷已经实现。 影响力ImageNet竞赛极大的推动深度学习在计算机视觉领域的应用，同时也是更多的认识到数据与算法是同等重要，改变过去了只是偏面强调算法的做法，提升了数据在深度学习中的地位。李飞飞独辟蹊径的研究之路从数据开始，颠覆与革新了很多人的认知，也让她自己功成名就成为享誉世界的学术大神。同时让深度学习走到到舞台的中心，推动了学术发展与工程技术的进步，很多中国的公司与高校也积极参加了此项赛事并且屡次都有斩获，但是这其中媒体报道的比较多的有两件事情：一是在2015年的百度多次提交违反比赛规则问题，百度解除了深度学习研究院杰出科学家吴韧的职务以平息此次事件影响。二是在最后一届2017年的比赛中国家千人计划学者，新加坡国立大学副教授、360集团副总裁、人工智能研究院院长颜水成团队获得多个项目比赛的第一名，用他自己的话说，抗日战争打八年都胜利了，ImageNet他也参加了八次比赛终于来到第一的位置上，可以说是名至实归。 启示关于ImageNet竞赛给我们带来的启示是任何成功都不是偶然的，都是长期坚持与努力的付出，李飞飞从创建ImageNet到正式发布开始2010年的第一届比赛用了三年时间、Hinton大神专注神经网络学习与研究，为了回应外界对他质疑，派其高徒Alex应用卷积神经网络参加ImageNet数据大赛，结果大获全胜，从此开创了深度神经网络空前的高潮。而在此之前，Hinton大神的团队曾经面临着研究经费都没有着落的窘境。颜水成从开始参加竞赛到最后笑傲群雄用了八年时间。这个世界哪有随随便便就可以成功的事情，深处在这个浮躁的时代，守得了自己的内心、坚持住自己的努力，不负光阴岁月，才有资格说失败两字。不是一番寒彻骨、哪有梅花扑鼻香！ 【数据】深度学习从“数据集”开始 分类标签123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3an01440764 鱼n01443537 鱼n01484850 鱼n01491361 鱼n01494475 鱼n01496331 鱼n01498041 鱼n01514668 鸡n01514859 鸡n01518878 鸵鸟n01530575 鸟n01531178 鸟n01532829 鸟n01534433 鸟n01537544 鸟n01558993 鸟n01560419 鸟n01580077 鸟n01582220 鸟n01592084 鸟n01601694 鸟n01608432 鸟n01614925 鹰n01616318 鹰n01622779 猫头鹰n01629819 壁虎n01630670 壁虎n01631663 壁虎n01632458 壁虎n01632777 壁虎n01641577 蛤蟆n01644373 青蛙n01644900 青蛙n01664065 龟n01665541 龟n01667114 龟n01667778 龟n01669191 龟n01675722 蜥蜴n01677366 蜥蜴n01682714 蜥蜴n01685808 蜥蜴n01687978 蜥蜴n01688243 蜥蜴n01689811 蜥蜴n01692333 蜥蜴n01693334 蜥蜴n01694178 蜥蜴n01695060 蜥蜴n01697457 鳄鱼n01698640 鳄鱼n01704323 恐龙n01728572 蛇n01728920 蛇n01729322 蛇n01729977 蛇n01734418 蛇n01735189 蛇n01737021 蛇n01739381 蛇n01740131 蛇n01742172 蛇n01744401 蛇n01748264 蛇n01749939 蛇n01751748 蛇n01753488 蛇n01755581 蛇n01756291 蛇n01768244 化石n01770081 蜘蛛n01770393 蝎子n01773157 蜘蛛n01773549 蜘蛛n01773797 蜘蛛n01774384 蜘蛛n01774750 蜘蛛n01775062 蜘蛛n01776313 蜘蛛n01784675 蜈蚣n01795545 鸟n01796340 鸟n01797886 鸟n01798484 鸟n01806143 孔雀n01806567 鸟n01807496 鸟n01817953 鸟n01818515 鸟n01819313 鸟n01820546 鸟n01824575 鸟n01828970 鸟n01829413 鸟n01833805 鸟n01843065 鸟n01843383 鸟n01847000 鸭子n01855032 鹅n01855672 鹅n01860187 鹅n01871265 大象n01872401 刺猬n01873310 鸭嘴兽n01877812 袋鼠n01882714 考拉n01883070 土拨鼠n01910747 水母n01914609 珊瑚n01917289 珊瑚n01924916 海洋生物n01930112 海蛇n01943899 海螺n01944390 蜗牛n01945685 蜗牛n01950731 海洋生物n01955084 海洋生物n01968897 海螺n01978287 螃蟹n01978455 螃蟹n01980166 螃蟹n01981276 螃蟹n01983481 龙虾n01984695 龙虾n01985128 龙虾n01986214 寄居蟹n01990800 海洋生物n02002556 鸟n02002724 鸟n02006656 鸟n02007558 鸟n02009229 鸟n02009912 鸟n02011460 鸟n02012849 鸟n02013706 鸟n02017213 鸟n02018207 鸟n02018795 鸟n02025239 鸟n02027492 鸟n02028035 鸟n02033041 鸟n02037110 鸟n02051845 鸟n02056570 企鹅n02058221 鸟n02066245 鲸鱼n02071294 鲸鱼n02074367 海象n02077923 海狮n02085620 狗n02085782 狗n02085936 狗n02086079 狗n02086240 狗n02086646 狗n02086910 狗n02087046 狗n02087394 狗n02088094 狗n02088238 狗n02088364 狗n02088466 狗n02088632 狗n02089078 狗n02089867 狗n02089973 狗n02090379 狗n02090622 狗n02090721 狗n02091032 狗n02091134 狗n02091244 狗n02091467 狗n02091635 狗n02091831 狗n02092002 狗n02092339 狗n02093256 狗n02093428 狗n02093647 狗n02093754 狗n02093859 狗n02093991 狗n02094114 狗n02094258 狗n02094433 狗n02095314 狗n02095570 狗n02095889 狗n02096051 狗n02096177 狗n02096294 狗n02096437 狗n02096585 狗n02097047 狗n02097130 狗n02097209 狗n02097298 狗n02097474 狗n02097658 狗n02098105 狗n02098286 狗n02098413 狗n02099267 狗n02099429 狗n02099601 狗n02099712 狗n02099849 狗n02100236 狗n02100583 狗n02100735 狗n02100877 狗n02101006 狗n02101388 狗n02101556 狗n02102040 狗n02102177 狗n02102318 狗n02102480 狗n02102973 狗n02104029 狗n02104365 狗n02105056 狗n02105162 狗n02105251 狗n02105412 狗n02105505 狗n02105641 狗n02105855 狗n02106030 狗n02106166 狗n02106382 狗n02106550 狗n02106662 狗n02107142 狗n02107312 狗n02107574 狗n02107683 狗n02107908 狗n02108000 狗n02108089 狗n02108422 狗n02108551 狗n02108915 狗n02109047 狗n02109525 狗n02109961 狗n02110063 狗n02110185 狗n02110341 狗n02110627 狗n02110806 狗n02110958 狗n02111129 狗n02111277 狗n02111500 狗n02111889 狗n02112018 狗n02112137 狗n02112350 狗n02112706 狗n02113023 狗n02113186 狗n02113624 狗n02113712 狗n02113799 狗n02113978 狗n02114367 狼n02114548 狼n02114712 狼n02114855 狼n02115641 狼n02115913 狼n02116738 狼n02117135 狼n02119022 狐狸n02119789 狐狸n02120079 狐狸n02120505 狐狸n02123045 猫n02123159 猫n02123394 猫n02123597 猫n02124075 猫n02125311 猫n02127052 猫n02128385 豹n02128757 豹n02128925 豹n02129165 狮子n02129604 老虎n02130308 豹n02132136 熊n02133161 熊n02134084 熊n02134418 熊n02137549 猫鼬n02138441 猫鼬n02165105 昆虫n02165456 昆虫n02167151 昆虫n02168699 昆虫n02169497 昆虫n02172182 昆虫n02174001 昆虫n02177972 昆虫n02190166 昆虫n02206856 昆虫n02219486 昆虫n02226429 昆虫n02229544 昆虫n02231487 昆虫n02233338 昆虫n02236044 昆虫n02256656 昆虫n02259212 昆虫n02264363 昆虫n02268443 蜻蜓n02268853 蜻蜓n02276258 蝴蝶n02277742 蝴蝶n02279972 蝴蝶n02280649 蝴蝶n02281406 蝴蝶n02281787 蝴蝶n02317335 海星n02319095 海胆n02321529 海洋生物n02325366 兔子n02326432 兔子n02328150 兔子n02342885 鼠n02346627 鼠n02356798 松鼠n02361337 鼠n02363005 鼠n02364673 鼠n02389026 马n02391049 斑马n02395406 猪n02396427 猪n02397096 猪n02398521 河马n02403003 牛n02408429 牛n02410509 牛n02412080 羊n02415577 羊n02417914 羊n02422106 羊n02422699 羊n02423022 羊n02437312 骆驼n02437616 羊驼n02441942 狸n02442845 狸n02443114 狸n02443484 狸n02444819 狸n02445715 狸n02447366 狸n02454379 穿山甲n02457408 树懒n02480495 狒狒n02480855 猩猩n02481823 猴子n02483362 猴子n02483708 猴子n02484975 猴子n02486261 猴子n02486410 猴子n02487347 猴子n02488291 猴子n02488702 猴子n02489166 猴子n02490219 猴子n02492035 猴子n02492660 猴子n02493509 猴子n02493793 猴子n02494079 猴子n02497673 猴子n02500267 猴子n02504013 大象n02504458 大象n02509815 浣熊n02510455 熊猫n02514041 鱼n02526121 鱼n02536864 鱼n02606052 鱼n02607072 鱼n02640242 鱼n02641379 鱼n02643566 鱼n02655020 鱼n02666196 算盘n02667093 穆斯林n02669723 学士服n02672831 手风琴n02676566 吉他n02687172 航空母舰n02690373 飞机n02692877 飞艇n02699494 教堂n02701002 救护车 checkn02704792 水陆两用车 checkn02708093 钟n02727426 箱子n02730930 围裙n02747177 垃圾箱n02749479 枪n02769748 背包n02776631 面包柜n02777292 体操n02782093 热气球n02783161 钢笔n02786058 邦迪n02787622 乐器n02788148 楼梯n02790996 杠铃n02791124 座椅n02791270 理发n02793495 木屋n02794156 表n02795169 酒桶n02797295 手推车n02799071 棒球n02802426 篮球n02804414 婴儿n02804610 乐器n02807133 游泳n02808304 婴儿毛巾n02808440 浴缸n02814533 轿车 checkn02814860 灯塔n02815834 烧杯n02817516 守卫n02823428 啤酒n02823750 啤酒n02825657 建筑n02834397 围兜n02835271 双人自行车n02837789 比基尼n02840245 笔记本n02841315 望远镜n02843684 信箱n02859443 小屋n02860847 雪橇n02865351 项链n02869837 帽子n02870880 书柜n02871525 书店n02877765 瓶盖n02879718 弓箭n02883205 领结n02892201 墓碑n02892767 胸罩n02894605 海岸n02895154 盔甲n02906734 扫帚n02909870 水桶n02910353 皮带n02916936 防弹背心n02917067 火车n02927161 肉铺n02930766 出租车 checkn02939185 锅n02948072 蜡烛n02950826 炮n02951358 艇n02951585 订书机n02963159 毛衣n02965783 反光镜n02966193 旋转木马n02966687 工具箱n02971356 盒子n02974003 轮胎n02977058 取款机n02978881 磁带n02979186 磁带n02980441 城堡n02981792 帆船n02988304 cd播放器n02992211 大提琴n02992529 手机n02999410 铁链n03000134 铁丝网n03000247 铁丝网n03000684 电锯n03014705 木箱n03016953 木柜n03017168 锣鼓n03018349 柜子n03026506 袜子n03028079 教堂n03032252 剧院n03041632 刀n03042490 堡垒n03045698 斗篷n03047690 鞋子n03062245 瓶子n03063599 杯子n03063689 壶n03065424 螺旋n03075370 锁n03085013 键盘n03089624 糖果n03095699 船n03100240 轿车 checkn03109150 开瓶器n03110669 号（乐器）n03124043 靴子n03124170 帽子n03125729 婴儿床n03126707 起重机n03127747 头盔n03127925 木箱n03131574 婴儿床n03133878 电饭锅n03134739 推球n03141823 拐杖n03146219 盔甲n03160309 水库n03179701 办公桌n03180011 电脑n03187595 电话机n03188531 尿布n03196217 闹钟n03197337 手表n03201208 餐桌n03207743 垫子n03207941 洗碗柜n03208938 车轮n03216828 港口n03218198 雪橇n03220513 穹顶n03223299 地毯n03240683 油田n03249569 鼓n03250847 鼓棒n03255030 哑铃n03259280 锅n03271574 风扇n03272010 电吉他n03272562 火车 checkn03290653 电视机n03291819 信n03297495 咖啡机n03314780 化妆品n03325584 绒毛n03337140 柜子n03344393 喷泉n03345487 消防车 checkn03347037 壁炉n03355925 旗杆n03372029 笛子n03376595 座椅n03379051 橄榄球n03384352 叉车n03388043 喷泉n03388183 钢笔n03388549 床n03393912 火车n03394916 圆号n03400231 平底锅n03404251 裘皮n03417042 卡车 checkn03424325 面具n03425413 加油n03443371 酒杯n03444034 卡丁车n03445777 高尔夫n03445924 高尔夫车n03447447 小船n03447721 锣鼓n03450230 婚纱n03452741 钢琴n03457902 大棚n03459775 轿车 车标 checkn03461385 菜场n03467068 断头台n03476684 发饰n03476991 发蜡n03478589 坦克n03481172 榔头n03482405 竹筒n03483316 吹风机n03485407 pos机n03485794 手帕n03492542 硬盘n03494278 口风琴n03495258 竖琴n03496892 起重机n03498962 斧头n03527444 手枪n03529860 电视机n03530642 蜂巢n03532672 钩子n03534580 裙子n03535780 体操n03538406 马车n03544143 沙漏n03584254 音乐播放器n03584829 电熨斗n03590841 南瓜灯n03594734 牛仔裤n03594945 吉普车 checkn03595614 T恤n03598930 拼图n03599486 黄包车n03602883 操纵杆n03617480 和服n03623198 护具n03627232 绳结n03630383 医生n03633091 勺子n03637318 灯n03642806 笔记本电脑n03649909 割草机n03657121 镜头盖n03658185 小刀n03661043 图书馆n03662601 救生船n03666591 打火机n03670208 加长车 checkn03673027 轮船n03676483 口红n03680355 鞋子n03690938 护肤品n03691459 音响n03692522 放大镜n03697007 原木n03706229 指南针n03709823 包n03710193 邮箱n03710637 泳衣n03710721 泳衣n03717622 窨井盖n03720891 手摇铃n03721384 木琴n03724870 面具n03729826 火柴n03733131 绳子n03733281 迷宫n03733805 烧杯n03742115 冰箱n03743016 石柱n03759954 话筒n03761084 微波炉n03763968 军人n03764736 水壶n03769881 小客车 checkn03770439 短裙n03770679 面包车 checkn03773504 导弹n03775071 手套n03775546 碗n03776460 房车n03777568 老爷车n03777754 路由器n03781244 建筑n03782006 显示器n03785016 摩托车 checkn03786901 砚n03787032 学士帽n03788195 建筑n03788365 蚊帐n03791053 助动车 checkn03792782 自行车 checkn03792972 帐篷n03793489 键盘鼠标n03794056 捕鼠夹n03796401 货车 卡车 checkn03803284 狗嘴套n03804744 钉子n03814639 颈托n03814906 项链n03825788 奶瓶n03832673 笔记本电脑n03837869 建筑n03838899 黑管n03840681 埙n03841143 仪表盘n03843555 机油滤清器n03854065 管风琴n03857828 示波器n03866082 礼服n03868242 牛车n03868863 呼吸器n03871628 零食n03873416 划桨n03874293 水轮n03874599 锁n03876231 刷子n03877472 睡衣n03877845 建筑n03884397 乐器n03887697 纸巾n03888257 降落伞n03888605 体操n03891251 长椅n03891332 停车缴费器n03895866 火车 checkn03899768 院子n03902125 公用电话n03903868 柱子n03908618 文具袋n03908714 卷笔刀n03916031 香水n03920288 培养皿n03924679 打印机n03929660 吉他拨片n03929855 头盔n03930313 栅栏n03930630 轿车 checkn03933933 桥n03935335 储蓄罐n03937543 药丸n03938244 枕头n03942813 乒乓球n03944341 风车n03947888 帆船n03950228 茶壶n03954731 刨子n03956157 建筑n03958227 塑料袋n03961711 碗架n03967562 推土机n03970156 搋子n03976467 相机n03976657 杆子n03977966 警车 checkn03980874 披风n03982430 桌球n03983396 瓶子n03991062 盆栽n03992509 陶艺n03995372 钻机n03998194 毯子n04004767 打印机n04005630 监狱n04008634 导弹n04009552 投影仪n04019541 冰球n04023962 拳击n04026417 手提袋n04033901 羽毛笔n04033995 床 被子n04037443 赛车n04039381 网球n04040759 加热器n04041544 收音机n04044716 卫星接收器n04049303 酒桶n04065272 房车n04067472 鱼竿n04069434 相机n04070727 冰箱n04074963 遥控器n04081281 餐厅n04086273 手枪n04090263 狙击枪n04099969 摇椅n04111531 烤箱n04116512 橡皮n04118538 橄榄球n04118776 尺n04120489 运动鞋n04125021 保险箱n04127249 回形针n04131690 调料瓶n04133789 拖鞋n04136333 长裙n04141076 萨克斯n04141327 剑n04141975 秤n04146614 校车n04147183 帆船n04149813 计分板n04152593 显示器n04153751 螺丝n04154565 螺丝刀n04162706 安全带n04179913 缝纫机n04192698 盾牌n04200800 鞋店n04201297 榻榻米n04204238 购物篮n04204347 购物车n04208210 铲子n04209133 浴帽n04209239 浴帘n04228054 滑雪n04229816 面罩n04235860 睡袋n04238763 游标卡尺n04239074 移门n04243546 老虎机n04251144 游泳眼镜n04252077 滑雪车n04252225 铲雪车n04254120 洗手液n04254680 足球n04254777 袜子n04258138 太阳能板n04259630 帽子n04263257 碗n04264628 键盘n04265275 电热器n04266014 航天飞船n04270147 锅铲n04273569 快艇n04275548 蜘蛛网n04277352 毛线n04285008 运动型轿车 checkn04286575 探照灯n04296562 乐队n04310018 蒸汽机车n04311004 桥n04311174 鼓n04317175 听诊器n04325704 担架n04326547 石堆n04328186 秒表n04330267 火炉n04332243 滤网n04335435 公交车 checkn04336792 担架n04344873 沙发n04346328 皇宫n04347754 轮船n04350905 西装n04355338 日晷n04355933 墨镜n04356056 墨镜n04357314 防晒霜n04366367 桥n04367480 拖把n04370456 连帽衫n04371430 沙滩裤n04371774 秋千n04372370 开关n04376876 针筒n04380533 台灯n04389033 坦克n04392985 磁带播放器n04398044 茶壶n04399382 毛绒玩具n04404412 电视机n04409515 网球n04417672 草屋n04418357 幕布n04423845 指套n04428191 装甲车n04429376 皇位n04435653 瓦片n04442312 面包机n04443257 烟酒店n04447861 马桶n04456115 火炬n04458633 图腾n04461696 大卡车 checkn04462240 玩具店n04465501 拖拉机n04467665 大货车 checkn04476259 碟子 盘子n04479046 风衣n04482393 儿童自行车n04483307 船n04485082 三脚架n04486054 拱门n04487081 巴士 checkn04487394 长号n04493381 浴缸n04501370 闸机n04505470 打字机n04507155 伞n04509417 独轮车n04515003 钢琴n04517823 吸尘器n04522168 花瓶n04523525 拱廊n04525038 珊瑚绒n04525305 自动贩卖机n04532106 教皇袍n04532670 桥n04536866 小提琴n04540053 排球n04542943 煎饼锅n04548280 挂钟n04548362 钱夹n04550184 柜子n04552348 飞机n04553703 台盆n04554684 洗衣机n04557648 水瓶n04560804 水壶n04562935 煤气包n04579145 水壶n04579432 哨子n04584207 头发n04589890 窗户n04590129 百叶窗n04591157 领带n04591713 葡萄酒n04592741 飞机n04596742 炒锅n04597913 勺子n04599235 围巾n04604644 栅栏n04606251 沉船n04612504 帆船n04613696 蒙古包n06359193 网页n06596364 海报n06785654 填字游戏n06794110 交通标志n06874185 交通灯n07248320 书n07565083 菜单n07579787 菜n07583066 菜n07584110 菜n07590611 菜n07613480 蛋糕n07614500 冰激凌n07615774 棒冰n07684084 面包n07693725 甜甜圈n07695742 面包n07697313 汉堡n07697537 热狗n07711569 焗饭n07714571 蔬菜n07714990 西蓝花n07715103 花椰菜n07716358 蔬菜n07716906 金瓜n07717410 南瓜n07717556 南瓜n07718472 黄瓜n07718747 蔬菜n07720875 青椒 黄椒 红椒n07730033 花n07734744 蘑菇n07742313 苹果n07745940 草莓n07747607 橙子n07749582 柠檬n07753113 水果n07753275 菠萝n07753592 香蕉n07754684 榴莲n07760859 水果n07768694 石榴n07802026 草垛n07831146 意大利面n07836838 甜品n07860988 面团n07871810 肉酱n07873807 披萨n07875152 派n07880968 肉卷n07892512 红酒n07920052 咖啡n07930864 茶n07932039 饮料杯n09193705 雪山n09229709 泡泡n09246464 悬崖n09256479 珊瑚n09288635 温泉n09332890 风景n09399592 小岛n09421951 沙滩n09428293 海滩n09468604 瀑布 溪流n09472597 火山n09835506 棒球n10148035 婚礼n10565667 潜水n11879895 油菜花n11939491 菊花n12057211 植物n12144580 玉米n12267677 松果n12620546 植物n12768682 栗子n12985857 菌菇n12998815 菌菇n13037406 菌菇n13040303 菌菇n13044778 菌菇n13052670 菌菇n13054560 菌菇n13133613 玉米n15075141 卷筒纸","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"dataset","slug":"dataset","permalink":"http://shuiyujie.com/tags/dataset/"},{"name":"imagenet","slug":"imagenet","permalink":"http://shuiyujie.com/tags/imagenet/"}]},{"title":"深度学习优化篇","slug":"Deeplearning/深度学习优化篇","date":"2019-09-20T13:37:14.000Z","updated":"2019-09-30T14:49:36.657Z","comments":true,"path":"post/3f932fee.html","link":"","permalink":"http://shuiyujie.com/post/3f932fee.html","excerpt":"模型要想训得好，参数优化不可少。","text":"模型要想训得好，参数优化不可少。 激活函数一个复杂的神经网络，是有许多层的，其中最基本的单位便是神经元。一个线性神经元不断堆叠下去，永远都是线性关系。这就无法解决非线性问题，而不同的非线性映射函数的选择，就是激活函数的研究课题了。 为了解决非线性的问题，在感知机的基础上发展出了多层神经网络的概念。多层神经网络由输入层、多个中间层，以及输出层组成。 解决非线性问题的还可以使用非线性的激活函数，比如 sigmoid function。在 Logistic Regression 中，我们将一个线性函数输入到 sigmoid function 中，能够很好地解决二分类的问题，使用 Softmax 还能进一步解决多分类问题。 最常用激活函数主要有这样几种： sigmoid 和 tanh 激活函数。 ReLU 激活函数 ReLU 的一大堆变种(LReLU，PReLU，RReLU，ELU，SELU，GELU等等) 参数初始化2006年Hinton等人在science期刊上发表了论文“Reducing the dimensionality of data with neural networks”，揭开了新的训练深层神经网络算法的序幕。 利用无监督的RBM网络来进行预训练，进行图像的降维，取得比PCA更好的结果，通常这被认为是深度学习兴起的开篇。 这么看来，是因为好的初始化方法的出现，才有了深层神经网络工程化落地的可能性。 常用初始化的方法包括： 随机初始化 标准初始化 Xavier初始化 He初始化 归一化方法数据经过归一化和标准化后可以加快梯度下降的求解速度，这就是Batch Normalization等技术非常流行的原因，它使得可以使用更大的学习率更稳定地进行梯度传播，甚至增加网络的泛化能力。常见的归一化的方法有： Batch Normalization Layer Normalization Instance Normalization Group Normalization Weight Normalization 池化 平均池化 最大池化 随机池化 从数据中学习池化 正则化方法 L1/L2参数惩罚 Dropout与Drop Connect 提前终止准则 样本扩充 最优化方法 SGD 梯度下降法 Momentum 动量法 Nesterov 加速梯度下降法 Adagrad 算法 Adadelta/Rmsprop算法 Adam/Nadam/AdaMax算法 AMSgrad Adafoctor 牛顿法/拟牛顿法 共轭梯度法 参考【AI初识境】激活函数：从人工设计到自动搜索 【AI初识境】什么是深度学习成功的开始？参数初始化 【AI初识境】深度学习模型中的Normalization，你懂了多少？ 【AI初识境】被Hinton，DeepMind和斯坦福嫌弃的池化，到底是什么？","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"}]},{"title":"卷积神经网络基础","slug":"Deeplearning/卷积神经网络基础","date":"2019-09-20T11:44:59.000Z","updated":"2019-09-30T14:49:18.397Z","comments":true,"path":"post/dd983220.html","link":"","permalink":"http://shuiyujie.com/post/dd983220.html","excerpt":"聊一聊神经网络的前生今世。","text":"聊一聊神经网络的前生今世。 生物神经元到MP模型人工神经网络（Artificial Neural Network），简称神经网络（Neural Network， NN），是人类模拟生物神经网络的结构和功能提出的数学模型，广泛应用于计算机视觉等领域。 人工神经网络与生物神经网络有大量相似之处，例如两者最基础的单元都是神经元。 神经元又称神经细胞，是生物神经网络的基本组成。其外观和大小在神经系统中差异很 大，但都具有相同的结构体，胞体和树突、轴突。胞体又名为核周体，由内质网、微管、 游离核糖体、神经丝和核组成。轴突和树突是神经元的突起，在神经元间传递电信号。神 经元的功能是接受信号并对其做出反应、传导兴奋、处理并储存信息以及发生细胞之间的 联结等，由这些功能动物才能迅速对环境的变化做出整合性的反应。 神经元之间相互连接，当某一神经元处于“兴奋”状态时，其相连神经元的电位将发 生改变，若神经元电位改变量超过了一定的数值（也称为阈值），则被激活处于“兴奋状 态”，向下一级连接的神经元继续传递电位改变信息。信息从一个神经元以电传导的方式跨过细胞之间的联结(即突触)，传给另一个神经元，最终使肌肉收缩或腺体分泌。 神经元可以处理信息，也可以以某种目前还未知的方式存储信息。神经元通过突触的 连接使数目众多的神经元组成比其他系统复杂得多的神经系统。从神经元的结构特性和生物功能可以得出结论：神经元是一个多输入单输出的信息处理单元，并且对信息的处理是非线性的。 基于上述情形，1943 年 McCulloch 和 Pitts 提出了 MP 模型，一种基于阈值逻辑的算法的神经网络计算模型，由固定的结构和权重组成。 在 MP 模型中，某个神经元接受来自其余多个神经元的传递信号，多个输入与对应连接 权重相乘后输入该神经元进行求和，再与神经元预设的阈值进行比较，最后通过激活函数 产生神经元输出。每一个神经元均为多输入单输出的信息处理单元，具有空间整合特性和阈值特性。 MP 模型把神经元进行抽象为一个简单的数学模型，模拟了生物神经元形式，成功证明了神经元能够执行逻辑功能，开创了人工神经网络研究的时代。从此神经网络的研究演变 为两种不同的研究思路：一种是继续生物学原理方面的探究，着重关注大脑中信息传递与 处理的生物学过程；另一种则演变为计算机学科，即神经网络在人工智能里的实际应用。 后者的研究是模仿前者的原理机制。 感知机前面提到了：MP 模型是感知机的原型，MP 模型是一种理论，并没有在计算机上实现，而感知机可以视为 MP 模型在计算机上的实现。这一小节就来看一下感知机的理论，学习过程，实现，以及缺陷。 基本介绍感知机（Perceptron）是Frank Rosenblatt在1957年提出的概念，其结构与MP模型类似，一般被视为最简单的人工神经网络，也作为二元线性分类器被广泛使用。通常情况下指单层的人工神经网络，以区别于多层感知机（Multilayer Perceptron）。尽管感知机结构简单，但能够学习并解决较复杂问题。 假设我们有一个𝑛维输入的单层感知机，𝑥1 至 𝑥n为 n 维输入向量的各个分量，𝑤1 至𝑤n 为 各个输入分量连接到感知机的权量（或称权值），𝜃为阈值，𝑓为激活函数（又称为激励函 数或传递函数），y为标量输出。理想的激活函数 𝑓(∙) 通常为阶跃函数或者 sigmoid 函数。 感知机的输出是输入向量𝑋与权重向量𝑊求得内积后，经激活函数𝑓所得到的标量。 用数学公式来表示就是：$$output=\\begin{cases}0,ifwx+b\\leq 0\\1,ifwx+b &gt;0\\end{cases}$$权重 W 的初始值一般为随机设置的，往往达不到较好的拟合结果。那如何更改权重数值使标量输出 y 逼近实际值呢？ 感知机学习过程首先通过计算得到输出值，接下来将实际输出值和理论输出值做差，由此来调整每一个输出端上的权值。学习规则是用来计算新的权值矩阵 W 及新的偏差 B 的算法。 即我们针对给定的训练数据，我们假设权重 theta 为某一个值，利用已有的训练数据的输入与预设权重做加权和，将得到的数据与训练数据的输出做比较，当他们的值分别代入到损失函数中会得到一个值，这个值用于描述预测值与真实值之间的差距。我们希望差距越小越好，由于训练数据的是固定的，也就是输入与真实值固定，唯一能够改变的就是权重 theta，所以我们要做的就是调整 theta 来使损失函数取最小值。 也就是说，感知的学习过程即：在给定训练数据的情况下，通过调整权重 theta，使损失函数最小化的过程，其根本的目的是为了让预测值与真实值之间的差距更小，得到更接近与实践情况的模型。 感知机的缺陷单层感知器类似一个逻辑回归模型，可以做线性分类任务，但是不能做更复杂的任务。第二次AI浪潮中马文·明斯基在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确解决。作为人工智能领域的开创者之一，这一声明也直接或间接促使神经网络的研究陷入了近20年的停滞。 单层感知器结构简单，权重更新计算快速，能够实现逻辑计算中的 NOT、OR、AND 等简 单计算，但是对于稍微复杂的 NOR 异或问题就无法解决，其本质缺陷是不能处理线性不可分问题。而在此基础上提出的多层感知器就能解决此类问题。 原理上的缺陷BP神经网络仍然是有监督的传统机器学习方法，遵循着以下思路。 也就是说，不过是在最后面将SVM或者其他分类器换成神经网络，在大部分情况下其实没有什么优势，甚至增加了问题的复杂度。 提取的特征虽然是研究者们经过反复实验证明有效的特征（也就是说特征任然是人为提取的），但仍然会一定程度上丢失了图像中的结构信息，从而丢失了一些对旋转扭曲等的不变性。而且要求输入的大小是固定的。为了学习到如偏移等微小的变化，需要有足够多的参数和足够多丰富的样本，最终学习到的权重，很可能在不同的位置处还会有非常相似的权重。 结构上的缺陷传统神经网络层与层之间采用全连接，训练一个卷积神经网络的流程和训练一个全连接网络没有任何区别，区别仅仅在于层与层之间的连接方式不同。 隐藏层神经元数量越多的全连接网络包含连接权值参量越多，极大的增加了系统消耗和内存占用，并且需要更大的训练集来确定连接权值。 如果一个隐藏层特征图像大小为100x100，输入层的特征图像大小为100x100，这意味着学习这一层需要100x100x100x100=10^8的参数。如果以32位的浮点数进行存储，就需要4*108的字节的存储量，约等于400MB的参数量。仅仅这样的一个网络层，其模型参数量已经超过了AlexNet网络的参数量，而100x100的特征图像分辨率，已经低于很多任务能够成功解决的下限。除了计算过程中需要存储的海量的参数，还有海量的计算，这些都超过了当时硬件的能力，因此大大限制了网络的大小，尤其是对于一些大的图像输入。 其次，对于图像或者音频而言不具备平移，旋转，拉伸的不变性，输入到神经网络前必须经过预处理。以手写字符为例，对其进行归一化会导致字符的大小，倾斜程度及位置发生改变，再加上书写风格的差异，会导致图像特征变化。对于这些可能出现的问题，往往需要更多的神经元来增强网络的鲁棒性。 再者，全连接的网络忽略了输入的拓扑结构。在一幅图像中，相邻的像素相关性较高的可以归为一个区域，像素之间相关性较低的则可视为图片中的不同区域，利用这个特性进行局部特征的提取有巨大的优势。但如何充分利用这些局部信息呢？ 20 世纪 60 年代，Hubel 和 Wiesel 在研究猫脑皮层中负责处理局部敏感和方向选择的神经元时，发现了一种特别的网络结构显著降低了反馈神经网络的复杂性，随即提出了卷积神经网络（Convolutional Neural Networks，CNN）的概念。近期 CNN 已经成为众多科学领域的研究热点之一，该网络不需要对图像进行复杂的预处理，可以直接输入原始图像，因而在计算机视觉方面得到广泛的应用。本章节也将对卷积神经网络的基本概念做一个基本的介绍。 总结：传统的线性模型如感知机，无法解决非线性问题，因此需要探究新的解决方案来解决非线性问题。 多层感知机与 BP 算法多层感知机（Multi-Layer Perceptron）是由单层感知机推广而来，最主要的特点是有多个神经元层。一般将 MLP 的第一层称为输入层，中间的层为隐藏层，最后一层为输出 层。MLP 并没有规定隐藏层的数量，因此可以根据实际处理需求选择合适的隐藏层层数， 对于隐藏层和输出层中每层神经元的个数也没有限制。 MLP 神经网络结构模型如下，分别为单隐层前馈网络和多层前馈网络，分别如图 1.3、 图 1.4 所示。其中，输入层神经元仅接受外界信息并传递给隐藏层，隐藏层与输出层的神 经元对信号进行加工，包含功能神经元。 多层感知机的关键问题在于如何训练其中各层间的连接权值。其训练问题又大致地分为两类： 一类是将其它连接权值进行固定，只训练某两层间的连接权值，研究者们已从数学上证明了这种方法对所有非线性可分的样本集都是收敛的； 另一类即大家所熟知的 back propagation（BP）算法，通常使用 sigmoid 和 tanh 等连续函数模拟神经元对激励的响 应，使用反向传播对神经网络的连接权值进行训练。 虽然 BP 算法应用广泛，但同时存在优化函数容易陷入局部最优问题，在优化过程中偏 离真正的全局最优点，性能下降，并且“梯度消失”现象严重等问题亟待解决。这些问题 也限制了 BP 神经网络在计算机视觉等方面的应用。 卷积神经网络的核心基础概念卷积与权值共享卷积是 CNN 的核心，用卷积核作用于图像可以得到相应的图像特征。 在传统 BP 神经网络中，前后层之间的神经元是“全连接”的，即每个神经元都与前一层的所有神经元相连，而卷积中的神经元只与上一层中部分神经元相连。从仿生的角度来说，CNN 在处理图像矩阵问题时会更加高效，例如人的单个视觉神经元并不需要对全部图像进行感知，只需要对局部信息进行感知即可，距离较远，相关性比较弱的元素不在计算范围内。从计算的角度来说，卷积使得参数量与计算量大幅度降低。 接下来介绍下卷积的具体操作，操作如图 1.7 所示。 例如原始图像大小是 7×7，卷积核大小是 3×3。首先卷积核与原始图像左上角 3×3 对应位置的元素相乘求和，得到的数值作为结果矩阵第一行第一列的元素值，然后卷积核向右移动一个单位（即步长 stride 为 1），与原始图像前三行第 2、3、4 列所对应位置的元素分别相乘并求和，得到的数值作为结果矩阵第一行第二列的元素值，以此类推。 这就是一个卷积核矩阵在一个原始矩阵上从上往下、从左往右扫描，每次扫描都得到 一个结果，将所有结果组合到一起得到一个新的结果矩阵。 如果将大量图片作为训练集，则最终卷积核会训练成待提取的特征，例如识别飞机， 那么卷积核可以是机身或者飞机机翼的形状等。 卷积核与原始图像做卷积操作，符合卷积核特征的部分得到的结果也比较大，经过激活函数往下一层传播。不符合卷积特征的区域，获得的数值比较小，往下传播的程度也会受到限制。卷积操作后的结果可以较好地表征该区域符合特征的程度，这也是卷积后得到的矩阵被称为特征平面的原因。 从上述表示中我们可以引出权值共享的概念，即将从图像一个局部区域学习到的信息 应用到的其它区域。 图像的局部特征具有重复性（也称为位置无关性），即图像中存在某个基本特征图形可能出现在图片上的任意位置，于是为在数据的不同位置检测是否存在相同的模式可以通过在不同位置共享相同的权值实现。 用一个相同的卷积核对整幅图像进行一个卷积操作，相当于对图像做一个全图滤波， 选出图片上所有符合这个卷积核的特征。一个卷积核对应的特征比如是边缘，那么用该卷积核去对图像做全图滤波，即是将图像各个位置的边缘都选择出来（帮助实现不变性）。 不同的特征可以通过不同的卷积核实现。 感受野感受野（receptive field）是卷积神经网络的重要概念之一，当前流行的物体识别方法的架构大都围绕感受野的设计。 从直观上讲，感受野就是视觉感受区域的大小。在卷积神经网络中，用数学的语言就是感受野是 CNN 中的某一层输出结果的一个元素对应输入层的一个映射，即 feature map 上的一个点所对应的输入图上的区域，具体示例如图 1.8 所示。 如果一个神经元的大小是受到上层 N×N 的神经元的区域的影响，那么就可以说，该神 经元的感受野是 N×N，因为它反映了 N×N 区域的信息。在图 1.8 中，conv2 中的像素点 5，是由 conv1 的 2×2 的区域计算得来，而该 2×2 区域，又是由 raw image 中 5×5 的区 域计算而来，所以，该像素的感受野是 5×5。可以看出感受野越大，得到的全局信息越多。 接下来给出感受野相关参数的数学定义。一个特征的感受野可以通过区域的中心位置和特征大小进行描述。计算每一层的感受野信息，需要了解每一层的额外信息，包括：当前感受野的尺寸 r，相邻特征之间的距离 j，起始特征的中心坐标 start，其中特征的中心 坐标定义为其感受野的中心坐标（如上述固定大小 CNN 特征图所述）。假设卷积核 （kernel size）大小为 k，填充大小（padding size）为 p，步长大小（stride size）为 s，下标 in 表示输入，out 表示输出，其输出层的相关属性计算如下。 输出特征图在每个维度上的大小：$$n_{out}=\\left[ \\frac {n_{in}+2p-k}{5}\\right] +1$$ 相邻特征图相邻特征之间的距离：$$ j_{oui}=j_{in}s$$ 输出特征图的 receptive field size: $$ r_{out}=r_{in}+\\left( k-1\\right) _{in} $$ 池化有了感受野再来解释池化（pooling）也很简单，从 raw image 到 conv1， 再到 conv2，图像越来越小。每过一级就相当于一次降采样，这就是池化。池化可以通过步长不为 1 的卷积实现，也可以通过 pool 直接插值采样实现，本质上没有区别，只是权重不同。 通过卷积获得了特征之后，下一步则是用这些特征去做分类。理论上讲，人们可以把 所有解析出来的特征关联到一个分类器，例如 softmax 分类器，但计算量非常大，并且极 易出现过度拟合（over-fitting）。而池化层则可以对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。 池化作用于图像中不重合的区域（这与卷积操作不同），一般而言池化操作的每一池化窗口都是不重叠的，所以池化窗口的大小等于 stride。这里的移动步长 stride=2，采用一个大小为 2×2 的池化窗口，max pooling 是在每一个区域中寻找最大 值，最终在原特征图中提取主要特征得到右图。 最常见的池化操作为平均池化（mean pooling）和最大池化（max pooling）。平均池 化是计算图像区域所有元素的平均值作为该区域池化后的值，最大池化则是选图像区域中 元素的最大值作为该区域池化后的值。在 ICLR2013 上，Zeiler 提出了随机池化 （stochastic pooling），只需对 feature map 中的元素按照其概率值大小随机选择，元 素被选中的概率与其数值大小正相关，并非如同 max-pooling 那样直接选取最大值。这种 随机池化操作不但最大化的保证了取值的 Max，也部分确保不会所有元素都被选取 max 值，造成过度失真。 池化操作提取信息的过程中，如果选取区域均值（mean pooling），往往能保留整体数据的特征，较好的突出背景信息；如果选取区域最大值（max pooling），则能更好保留 纹理特征。但最理想的还是小波变换，不但可以在整体上更加细微，也能够保留更多的细节特征。池化操作本质是使特征图缩小，有可能影响网络的准确度，对此我们可以通过增加特征图的深度来弥补精度的缺失。 卷积神经网络的基本结构卷积神经网络依旧是层级网络，但层的功能和形式发生了变化，是传统神经网络的改 进。其主要包含数据输入层（Input layer），卷积计算层（Convolutional layer）， ReLU 激励层 （ReLU layer），池化层（Pooling layer）以及全连接层（Full connection layer）。 Layer Note 输入层 输入层是整个神经网络的输入，在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵。如上图所示，最左侧三维矩阵就可以代表一张图像。其中三维矩阵的长和宽代表了图像的大小，深度代表了图像的色彩通道(channel)。从输入层开始，卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵，直到最后的全连接层。数据输入层主要是对原始图像数据进行预处理，其中包括：去均值，把输入数据各个维度都中心化为 0；归一化，减少各维度数据因取值范围不同而带来的干扰。 卷积层 传统全连接层输入是整张图片的像素点，而卷积层中每个节点的输入是上一层神经网络的一小块，这个小块常用的大小有 3x3 或者 5x5。卷积层试图将神经网络中的每一个小块进行更加深入的分析从而得到抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵会变得更深，所以上图所示经过卷积层之后的节点矩阵深度会增加。卷积神经网络的重点，也与普通前馈神经网络的不同之处在于，卷积神经网络包含了 普通神经网络没有的特征处理器，它是由卷积层和池化层（也称为降采样层）构成的。卷积层中，一个神经元只与邻层部分（通常为方阵区域）的神经元连接，包含多干个特征平面(FeatureMap)，每个特征平面由多个神经元按矩阵形式排列组成，同一特征平面的神经元共享权值（卷积核）。共享权值（卷积核）大大减少了网络各层之间的连接，降低了过拟合的风险。一般卷积神经网络包含多个卷积层，一个卷积层可以有多个不同的卷积核。图像通过多个不同的卷积核进行处理提取出局部特征，每个卷积核映射出一个新的特征图，再将卷积输出结果进行非线性激活函数的处理产生输出。 池化层 池化层神经网络不会改变三维矩阵的深度，但她可以缩小矩阵的大小。池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片。通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到减少整个神经网络中参数的目的。接下来对激活函数处理的结果通过降采样，也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。池化用于压缩网络参数和数据大小，降低过拟合。如果输入为一幅图像，那么池化层的最主要作用就是压缩图像的同时保证该图像特征的不变性。一辆车的图像被缩小了一倍仍能认出这是一辆车，说明处理后的图像仍包含着原始图片里最重要的特征，图像压缩时 去掉的只是一些冗余信息，留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。池化操作的作用就是把冗余信息去除，将重要的信息保留，这样可以在一定程度上防止过拟合，方便网络的优化。 全连接层 在经过多轮卷积层和池化层的处理之后，在卷积神经网络的最后一般会由 1 到 2 个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在特征提取完成之后，任然需要使用全连接层来完成分类任务。全连接层，即两层之间所有神经元权重连接，通常全连接层在卷积神经网络尾部，跟传统的神经网络神经元的连接方式相同。 softmax层 Softmax 层主要用于分类问题，通过 Softmax 层，可以得到当前阳历属于不同种类的概率分布情况。 卷积神经网络的训练算法也同一般机器学习算法类似，先定义损失函数 Loss function，计算和实际结果的差值，找到最小化损失函数的参数值，利用随机梯度下降法进行权值调整。 训练卷积神经网络时，网络的参数往往需进行 fine-tuning，就是使用已用于其他目标、预训练模型的权重或者部分权重，作为初始值开始训练，这样可以很快收敛到一个较理想的状态。 卷积神经网络通过局部感受野(local receptive fields)，权值共享(shared weights)，下采样(sub-sampling)实现位移，缩放，和形变的不变性。主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。权值共享大大降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的 复杂过程。同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于全连接神经网络的一大优势。 参考[1] Hubel D H, Wiesel T N. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex[J]. The Journal of physiology, 1962, 160(1): 106-154. [2] Fukushima K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position[J]. Biological cybernetics, 1980, 36(4): 193-202. [3] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. 【AI初识境】从头理解神经网络-内行与外行的分水岭 Convolutional Neural Networks (CNNs / ConvNets) Tensorflow：实战Google深度学习框架","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"}]},{"title":"如何自建数据集","slug":"Deeplearning/如何自建数据集","date":"2019-09-17T11:28:57.000Z","updated":"2019-09-30T14:49:23.083Z","comments":true,"path":"post/b64b0362.html","link":"","permalink":"http://shuiyujie.com/post/b64b0362.html","excerpt":"(1) 学会使用爬虫爬取图像和视频,从视频中提取图片。 (2) 对获得的图片数据进行整理,包括重命名,格式统一,去重。","text":"(1) 学会使用爬虫爬取图像和视频,从视频中提取图片。 (2) 对获得的图片数据进行整理,包括重命名,格式统一,去重。 爬取图片有些任务没有直接对应的开源数据集,或者开源数据集中的数据比较少,这就需要我们通过搜索引擎自行爬取图片。 百度图片爬虫 Download images from Google, Bing, Baidu. 谷歌、百度、必应图片下载 Google, Naver multiprocess image web crawler (Selenium) 数据集整理爬取的如果是视频需要先转换成图片，如果是图片就要做好统一格式、数据清洗的工作。 视频转换成图片使用爬虫爬取数据,如果是视频可以使用 python getimagefromvideo.py &lt;video_path&gt; 将视频转换为图片 1234567891011121314151617181920#coding:utf8import cv2import dlibimport numpy as npimport sysimport osvideo_capture = cv2.VideoCapture(sys.argv[1])video_id = sys.argv[1].split('.')[0]os.mkdir(video_id)count = 0while True: is_sucessfully_read, im = video_capture.read() if is_sucessfully_read == False: break cv2.imwrite(os.path.join(video_id,str(count)+'.jpg'),im) print \"image shape=\",im.shape count = count + 1 print count 统一图片后缀格式统一后缀格式可以减少以后写数据 API 时的压力,也可以测试图片是不是可以正常的读取,及时防止未知问题的出现,这很重要。 使用 python reformat_image.py &lt;images_folder_path&gt; 将图片全部转换为 jpg 格式,这也是所有框架支持的格式。 12345678910111213141516171819202122232425import osimport sysimport cv2import numpy as npdef listfiles(rootDir): list_dirs = os.walk(rootDir) for root, dirs, files in list_dirs: for d in dirs: print os.path.join(root,d) for f in files: fileid = f.split('.')[0] filepath = os.path.join(root,f) try: src = cv2.imread(filepath,1) print \"src=\",filepath,src.shape os.remove(filepath) cv2.imwrite(os.path.join(root,fileid+\".jpg\"),src) except: os.remove(filepath) continuelistfiles(sys.argv[1]) 按格式重命名图片统一格式的命名有利于区分和整理数据 12mkdir tmp./rename_files_function.sh &lt;images_folder_path&gt; ./tmp/ &lt;label&gt; 12345678910111213141516171819202122232425262728293031323334353637383940i=0dir=$1resultdir=$2app=$3for file in $dir\"\"* do arr=$(echo $file | tr \"/\" \"\\n\") for x in $arr do filename=$x done brr=$(echo $filename | tr \".\" \"\\n\") brrs=( $brr ) fileid=$&#123;brrs[0]&#125; num=$&#123;#brrs[@]&#125; index=$(expr $num - 1) fileformat=$&#123;brrs[index]&#125; echo file=\"\"$file echo fileid=\"\"$fileid echo fileformat=\"\"$fileformat if [ $fileformat == jpeg -o $fileformat == png -o $fileformat == jpg -o $fileformat == bmp ] ; then #echo \"good\" i=$(expr $i + 1) resultfile=$resultdir\"\"$app\"\"$i\"\".$fileformat echo file=\"\"$file\"\",resultfile=\"\"$resultfile mv \"$file\" \"$resultfile\" else echo $file\"\"not good fidoneecho 执行删除\"\"$dir\"\"*#rm $dir\"\"*echo 执行mv\"\"$resultdir\"\"*mv $resultdir\"\"* $dir 去重如果你使用多个关键词或者使用不同的搜索引擎同样的关键词,或者从视频中提取图片,那么爬取回来的图片很可能有重复或者非常的相似,这样的样本应该被去除。 去除有很多种方法,比如直接比较两幅图像是不是完全相同,通过 hash 等相似度方法来进行相似度,这里我们提供一个方法,利用相似度来进行去重。 12345# sudo pip install python-Levenshteinconda install -c conda-forge python-levenshtein python remove_repeat.py &lt;image_path&gt;https://anaconda.org/conda-forge/python-levenshtein 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270#!/usr/bin/env python#coding:utf8import mathfrom PIL import Imageimport Levenshteinclass BWImageCompare(object): \"\"\"Compares two images (b/w).\"\"\" _pixel = 255 _colour = False def __init__(self, imga, imgb, maxsize=64): \"\"\"Save a copy of the image objects.\"\"\" sizea, sizeb = imga.size, imgb.size newx = min(sizea[0], sizeb[0], maxsize) newy = min(sizea[1], sizeb[1], maxsize) # Rescale to a common size: imga = imga.resize((newx, newy), Image.BICUBIC) imgb = imgb.resize((newx, newy), Image.BICUBIC) if not self._colour: # Store the images in B/W Int format imga = imga.convert('I') imgb = imgb.convert('I') self._imga = imga self._imgb = imgb # Store the common image size self.x, self.y = newx, newy def _img_int(self, img): \"\"\"Convert an image to a list of pixels.\"\"\" x, y = img.size for i in xrange(x): for j in xrange(y): yield img.getpixel((i, j)) @property def imga_int(self): \"\"\"Return a tuple representing the first image.\"\"\" if not hasattr(self, '_imga_int'): self._imga_int = tuple(self._img_int(self._imga)) return self._imga_int @property def imgb_int(self): \"\"\"Return a tuple representing the second image.\"\"\" if not hasattr(self, '_imgb_int'): self._imgb_int = tuple(self._img_int(self._imgb)) return self._imgb_int @property def mse(self): \"\"\"Return the mean square error between the two images.\"\"\" if not hasattr(self, '_mse'): tmp = sum((a-b)**2 for a, b in zip(self.imga_int, self.imgb_int)) self._mse = float(tmp) / self.x / self.y return self._mse @property def psnr(self): \"\"\"Calculate the peak signal-to-noise ratio.\"\"\" if not hasattr(self, '_psnr'): self._psnr = 20 * math.log(self._pixel / math.sqrt(self.mse), 10) return self._psnr @property def nrmsd(self): \"\"\"Calculate the normalized root mean square deviation.\"\"\" if not hasattr(self, '_nrmsd'): self._nrmsd = math.sqrt(self.mse) / self._pixel return self._nrmsd @property def levenshtein(self): \"\"\"Calculate the Levenshtein distance.\"\"\" if not hasattr(self, '_lv'): stra = ''.join((chr(x) for x in self.imga_int)) strb = ''.join((chr(x) for x in self.imgb_int)) lv = Levenshtein.distance(stra, strb) self._lv = float(lv) / self.x / self.y return self._lvclass ImageCompare(BWImageCompare): \"\"\"Compares two images (colour).\"\"\" _pixel = 255 ** 3 _colour = True def _img_int(self, img): \"\"\"Convert an image to a list of pixels.\"\"\" x, y = img.size for i in xrange(x): for j in xrange(y): pixel = img.getpixel((i, j)) yield pixel[0] | (pixel[1]&lt;&lt;8) | (pixel[2]&lt;&lt;16) @property def levenshtein(self): \"\"\"Calculate the Levenshtein distance.\"\"\" if not hasattr(self, '_lv'): stra_r = ''.join((chr(x&gt;&gt;16) for x in self.imga_int)) strb_r = ''.join((chr(x&gt;&gt;16) for x in self.imgb_int)) lv_r = Levenshtein.distance(stra_r, strb_r) stra_g = ''.join((chr((x&gt;&gt;8)&amp;0xff) for x in self.imga_int)) strb_g = ''.join((chr((x&gt;&gt;8)&amp;0xff) for x in self.imgb_int)) lv_g = Levenshtein.distance(stra_g, strb_g) stra_b = ''.join((chr(x&amp;0xff) for x in self.imga_int)) strb_b = ''.join((chr(x&amp;0xff) for x in self.imgb_int)) lv_b = Levenshtein.distance(stra_b, strb_b) self._lv = (lv_r + lv_g + lv_b) / 3. / self.x / self.y return self._lvclass FuzzyImageCompare(object): \"\"\"Compares two images based on the previous comparison values.\"\"\" def __init__(self, imga, imgb, lb=1, tol=15): \"\"\"Store the images in the instance.\"\"\" self._imga, self._imgb, self._lb, self._tol = imga, imgb, lb, tol def compare(self): \"\"\"Run all the comparisons.\"\"\" if hasattr(self, '_compare'): return self._compare lb, i = self._lb, 2 diffs = &#123; 'levenshtein': [], 'nrmsd': [], 'psnr': [], &#125; stop = &#123; 'levenshtein': False, 'nrmsd': False, 'psnr': False, &#125; while not all(stop.values()): cmp = ImageCompare(self._imga, self._imgb, i) diff = diffs['levenshtein'] if len(diff) &gt;= lb+2 and \\ abs(diff[-1] - diff[-lb-1]) &lt;= abs(diff[-lb-1] - diff[-lb-2]): stop['levenshtein'] = True else: diff.append(cmp.levenshtein) diff = diffs['nrmsd'] if len(diff) &gt;= lb+2 and \\ abs(diff[-1] - diff[-lb-1]) &lt;= abs(diff[-lb-1] - diff[-lb-2]): stop['nrmsd'] = True else: diff.append(cmp.nrmsd) diff = diffs['psnr'] if len(diff) &gt;= lb+2 and \\ abs(diff[-1] - diff[-lb-1]) &lt;= abs(diff[-lb-1] - diff[-lb-2]): stop['psnr'] = True else: try: diff.append(cmp.psnr) except ZeroDivisionError: diff.append(-1) # to indicate that the images are identical i *= 2 self._compare = &#123; 'levenshtein': 100 - diffs['levenshtein'][-1] * 100, 'nrmsd': 100 - diffs['nrmsd'][-1] * 100, 'psnr': diffs['psnr'][-1] == -1 and 100.0 or diffs['psnr'][-1], &#125; return self._compare def similarity(self): \"\"\"Try to calculate the image similarity.\"\"\" cmp = self.compare() lnrmsd = (cmp['levenshtein'] + cmp['nrmsd']) / 2 return lnrmsd return min(lnrmsd * cmp['psnr'] / self._tol, 100.0) # TODO: fix psnr!if __name__ == '__main__': import sys import os srcimages = os.listdir(sys.argv[1]) srcimages.sort() tot = len(srcimages) tot = (tot ** 2 - tot) / 2 print 'Comparing %d images:' % tot images = &#123;&#125; ###向后删除图片 similarity_thresh = 0.5 ##相似度阈值，超过即判断为相同图片 i = 0 while(i &lt; len(srcimages)-1): print \"i=\", i,\"num of srcimages\",len(srcimages) imga = Image.open(os.path.join(sys.argv[1],srcimages[i])) imgb = Image.open(os.path.join(sys.argv[1],srcimages[i+1])) cmp = FuzzyImageCompare(imga, imgb) sim = cmp.similarity() / 100 print \"image \",os.path.join(sys.argv[1],srcimages[i]),\" and image\",os.path.join(sys.argv[1],srcimages[i+1]),\" sim=\",sim if sim &gt; similarity_thresh: print \"delete \",os.path.join(sys.argv[1],srcimages[i+1]) os.remove(os.path.join(sys.argv[1],srcimages[i+1])) srcimages.pop(i+1) else: i = i+1 ''' results, i = &#123;&#125;, 1 for namea, imga in images.items(): for nameb, imgb in images.items(): if namea == nameb or (nameb, namea) in results: continue print ' * %2d / %2d:' % (i, tot), print namea, nameb, '...', cmp = FuzzyImageCompare(imga, imgb) sim = cmp.similarity() results[(namea, nameb)] = sim print '%.2f %%' % sim i += 1 res = max(results.values()) imgs = [k for k, v in results.iteritems() if v == res][0] print 'Most similar images: %s %s (%.2f %%)' % (imgs[0], imgs[1], res) ''' 在此之后还需要自己手动筛选图片，工作量其实也不小，不过经过去重还是可以减少不少工作量的。 数据集标注爬取的图片需要自己标注，可以使用下面这些标注工具。 https://github.com/tzutalin/labelImg LabelImg is a graphical image annotation tool and label object bounding boxes in images https://youtu.be/p0nR2YsCY_U https://github.com/wkentaro/labelme Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation). https://github.com/Microsoft/VoTT Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos. 数据集划分一般会按照 8:1:1 将数据集划分为训练集、验证集、测试集。这个要根据自己的情况编写 shell 脚本，下面是我用 darknet 训练 yolov3 模型时划分数据的脚本。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/bin/shif [ $# != 1 ];then echo \"Usage: $0 &lt;full path&gt;\" exit -1fipath=$1for sub_dir in `ls $path` do # 获取子文件夹的全路径 sub_dir_path=$path/$sub_dir if [ -d $sub_dir_path ] then # 将子目录下所有文件移动到父目录中 `mv $sub_dir_path/* $path` # 删除子目录 `rm -rf $sub_dir_path` fi # 给所有文件添加前缀 done`rm tmp.txt`# 将文件夹下指定类型的文件写到文件中# ***** 问题：最后会有个空行 *****# 图片文件存在对应的 txt 文件,则将图片路径追加到 tmp.txt 文件中for image in `find $path | grep -E 'jpg|png|JPEG|JPG|PNG'`do txt=$&#123;image%.*&#125;\".txt\" if [ -f $txt ] then echo $&#123;image&#125; `echo $&#123;image&#125; &gt;&gt; tmp.txt` fi done# 将路径 8:1:1 放到 train.txt,val.txt,test.txt# 1. 计算 tmp,txt 文件行数# 2. 计算得出分配到各个文件的行号# 3. 将对应行数的内容写到对应文件夹中line=`cat tmp.txt | wc -l`line1=$(($line/10*8))line2=$(($line/10*8+line/10+1))`sed -n 1,$&#123;line1&#125;p tmp.txt &gt;&gt; train.txt``sed -n $(($&#123;line1&#125;+1)),$&#123;line2&#125;p tmp.txt &gt;&gt; val.txt``sed -n $(($&#123;line2&#125;+1)),$(($&#123;line&#125;-1))p tmp.txt &gt;&gt; test.txt` yolov3 的标注格式如下所示 19 0.732955 0.591102 0.270317 0.193503 统计标签的时候可以使用 1awk '&#123;print $1&#125;' *.txt | sort -g | uniq -c 以上就是自己建立一个数据集的流程：爬取图片-&gt;整理图片-&gt;标注图片-&gt;训练。","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"dataset","slug":"dataset","permalink":"http://shuiyujie.com/tags/dataset/"}]},{"title":"【Java并发编程】线程本地存储模式ThreadLocal","slug":"Java/Java 并发编程/【Java并发编程】线程本地存储模式ThreadLocal","date":"2019-09-16T14:29:45.000Z","updated":"2019-09-18T13:37:45.027Z","comments":true,"path":"post/790a1fa3.html","link":"","permalink":"http://shuiyujie.com/post/790a1fa3.html","excerpt":"我们知道多个线程同时读写同一共享变量会导致并发问题。 一种解决方案是使用 Immutability 模式，如果共享变量在初始化之后就不会改变，只能读取，那么无论多少个线程同时读这个共享变量都不会出现并发问题。比如说 Java 中的 Long、Integer、Short、Byte 等基本数据类型的包装类的实现。 另一种解决方案是突破共享变量，没有共享变量就不会有并发问题。那么如何避免共享呢？思路其实很简单，就是每个线程拥有自己的变量，彼此不共享，就不会有共享问题。 具体来说有两种方法：线程封闭和线程本地存储(ThreadLocal)。","text":"我们知道多个线程同时读写同一共享变量会导致并发问题。 一种解决方案是使用 Immutability 模式，如果共享变量在初始化之后就不会改变，只能读取，那么无论多少个线程同时读这个共享变量都不会出现并发问题。比如说 Java 中的 Long、Integer、Short、Byte 等基本数据类型的包装类的实现。 另一种解决方案是突破共享变量，没有共享变量就不会有并发问题。那么如何避免共享呢？思路其实很简单，就是每个线程拥有自己的变量，彼此不共享，就不会有共享问题。 具体来说有两种方法：线程封闭和线程本地存储(ThreadLocal)。 线程封闭方法里的局部变量，因为不会和其他线程共享，所以没有并发问题，叫做线程封闭，比较官方的解释是：仅在单线程内访问数据。由于不存在共享，所以即便不同步也不会有并发问题，性能杠杠的。 如上图所示，JVM 中每一个线程都会有一个 Java 虚拟机栈。Java 程序每调用一个方法都会入栈，每执行完一个方法都会出栈，且每一个方法都有一个栈帧。栈帧中存储了参数、局部变量和返回地址。由此可见，方法的局部变量是不可能和其它线程共享的。 局部变量可以避免线程共享，此外还有什么方法能避免线程共享么？有，那就是 Java 语言提供的线程本地存储(ThreadLocal)。 ThreadLockSimpleDateFormat 不是线程安全的，如果想在并发场景下使用它可以将其设置为一个方法的局部变量，这样就会创建许多对象占用内存。或者使用 ThreadLock，不同线程调用 SafeDateFormat 会返回不同的 SimpleDateFormat 对象，但是由于在线程之间不共享，就和局部变量一样是安全的。 1234567891011static class SafeDateFormat &#123; // 定义 ThreadLocal 变量 static final ThreadLocal&lt;DateFormat&gt; tl=ThreadLocal.withInitial( ()-&gt; new SimpleDateFormat( \"yyyy-MM-dd HH:mm:ss\")); static DateFormat get()&#123; return tl.get(); &#125;&#125; 接下来再看一下 ThreadLock 的代码实现。 1234567891011121314151617181920212223242526272829303132class Thread &#123; // 内部持有 ThreadLocalMap ThreadLocal.ThreadLocalMap threadLocals;&#125;class ThreadLocal&lt;T&gt;&#123; public T get() &#123; // 首先获取线程持有的 //ThreadLocalMap ThreadLocalMap map = Thread.currentThread() .threadLocals; // 在 ThreadLocalMap 中 // 查找变量 Entry e = map.getEntry(this); return e.value; &#125; static class ThreadLocalMap&#123; // 内部是数组而不是 Map Entry[] table; // 根据 ThreadLocal 查找 Entry Entry getEntry(ThreadLocal key)&#123; // 省略查找逻辑 &#125; //Entry 定义 static class Entry extends WeakReference&lt;ThreadLocal&gt;&#123; Object value; &#125; &#125;&#125; 在 Java 的实现方案中，ThreadLocal 仅仅是一个工具类，内部并不持有和线程有关的数据，所有和线程有关的数据都存储在 Thread 中，比如 Thread 内部持有当前线程的 ThreadLocalMap。 这样设计有一个好处就是能够避免内存泄露。 SimpleDateFormat 的线程安全问题与 ThreadLocal 手撕面试题ThreadLocal！！！","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"},{"name":"设计模式","slug":"设计模式","permalink":"http://shuiyujie.com/tags/设计模式/"}]},{"title":"JVM-模拟YGC，查看GC日志","slug":"Java/JVM/JVM-模拟YGC，查看GC日志","date":"2019-09-14T03:06:20.000Z","updated":"2019-09-28T03:34:26.713Z","comments":true,"path":"post/5dbc4dfe.html","link":"","permalink":"http://shuiyujie.com/post/5dbc4dfe.html","excerpt":"我们知道每次创建新的对象都会保存到新生代中。新生代会使用垃圾回收器比如说 ParNew 垃圾回收器，将新生代进一步分成 Eden 区和两个 Survivor 区。当新生代满了的时候就会触发 Young GC。 本文会实战 Young GC 的场景，并且带大家查看 GC 日志。","text":"我们知道每次创建新的对象都会保存到新生代中。新生代会使用垃圾回收器比如说 ParNew 垃圾回收器，将新生代进一步分成 Eden 区和两个 Survivor 区。当新生代满了的时候就会触发 Young GC。 本文会实战 Young GC 的场景，并且带大家查看 GC 日志。 模拟 Young GC 场景首先我们将 JVM 的参数按如下所示设置： 1-XX:NewSize=5242880 -XX:MaxNewSize=5242880 -XX:InitialHeapSize=10485760 -XX:MaxHeapSize=10485760 -XX:SurvivorRatio=8 -XX:PretenureSizeThreshold=10485760 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log 以上参数都是 JDK1.8 版本的参数，其他版本略有不同，但也没差多少： 参数 含义 -XX:InitialHeapSize-XX:MaxHeapSize 初始堆大小和最大堆大小 -XX:NewSize-XX:MaxNewSize 初始新生代大小和最大新生代大小 -XX:PretenureSizeThreshold=1048576 指定了大对象阈值是10MB -XX:+PrintGCDetils 打印详细的gc日志 -XX:+PrintGCTimeStamps 打印出来每次GC发生的时间 -Xloggc:gc.log 设置将gc日志写入一个磁盘文件","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"JVM-如何设置JVM的参数","slug":"Java/JVM/JVM-如何设置JVM的参数","date":"2019-09-13T07:58:49.000Z","updated":"2019-09-13T09:00:56.110Z","comments":true,"path":"post/1410ce3d.html","link":"","permalink":"http://shuiyujie.com/post/1410ce3d.html","excerpt":"新生代、老生代、永久代的概念 JVM内存相关核心参数图解 如何在启动系统的时候设置 JVM 参数","text":"新生代、老生代、永久代的概念 JVM内存相关核心参数图解 如何在启动系统的时候设置 JVM 参数 新生代、老生代、永久代的概念JVM-JVM中内存区域划分这一章中讲过，每执行一个方法，该方法都会有一个栈帧进入 Java 虚拟机栈，栈帧中保存着方法中的局部变量、返回值地址等信息。 栈帧中保存的局部变量如果是一个对象，它就会指向 Java 堆中某个具体的对象实例，如下图所示： 新生代：刚创建的对象都会在新生代，变量将会指向这个对象实例。如果没有任何变量指向这个对象，该对象就可能被回收。 老年代：新生代内存满了之后就会触发 Minor GC ，垃圾回收器会回收新生代中没有人引用的对象实例。如果一个对象实例经过10多次回收都没有被回收掉，就算它年龄有 10 多岁了，将会进入到老年代。 永久代：可以理解成方法区中的类和类信息 JVM内存相关核心参数图解在JVM内存分配中的核心参数，如下所示。 -Xms：Java堆内存的大小 -Xmx：Java堆内存的最大大小 -Xmn：Java堆内存中的新生代大小，扣除新生代剩下的就是老年代的内存大小了 -XX:PermSize：永久代大小 -XX:MaxPermSize：永久代最大大小 -Xss：每个线程的栈内存大小 启动时设置 JVM 参数IDEA 可以在 Run-edit configration.. 中设置参数，如下所示： 如果在线上部署系统，可以使用java -jar方式启动，如：java -Xms512M -Xmx512M -Xmn256M -Xss1M -XX:PermSize=128M -XX:MaxPermSize=128M -jar App.jar","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"JVM-JVM中内存区域划分","slug":"Java/JVM/JVM-JVM中内存区域划分","date":"2019-09-12T08:07:13.000Z","updated":"2019-09-12T13:47:45.282Z","comments":true,"path":"post/3860d841.html","link":"","permalink":"http://shuiyujie.com/post/3860d841.html","excerpt":"在JVM-Java中的类加载机制一文中，我讲述了 Java 类加载的过程，它包含加载、验证、准备、解析、初始化、使用、卸载这几个步骤。我们在准备阶段会将类加载到内存中，为类变量分配内存并赋予初值；在初始化阶段则会正式执行初始化代码。 那么在准备阶段和初始化阶段，JVM 具体是怎么为类分配内存的呢？","text":"在JVM-Java中的类加载机制一文中，我讲述了 Java 类加载的过程，它包含加载、验证、准备、解析、初始化、使用、卸载这几个步骤。我们在准备阶段会将类加载到内存中，为类变量分配内存并赋予初值；在初始化阶段则会正式执行初始化代码。 那么在准备阶段和初始化阶段，JVM 具体是怎么为类分配内存的呢？ 什么是 JVM 中的内存区域划分当 JVM 运行我们的代码时，它会使用多块不同的内存，不同的内存存放不同的数据，再配合代码的执行流程，这样就能把程序运行起来了。 概括来讲 JVM 中的内存空间可以分成三大块： 存放加载的类。保存 JVM 加载的类。 运行方法使用。保存程序时我们要执行一个个方法，即方法中的局部变量。 创建的对象。保存运行程序时创建的对象。 JVM 中的内存区域划分JVM 的内存划分可以分成三大块：存放加载的类，存放创建的对象，运行方法时使用。如果进一步细分，可以分成这样几个主要区域：方法区、程序计数器、Java虚拟机栈和 Java堆。 如果存在 main() 函数，JVM 会从 main() 函数开始完成整个类加载过程，所以上面这段代码会从 main() 开始加载。JVM 进行会启动，先加载 Kafka.java 到内存中，然后有一个 main 进程开始执行 main() 方法： 方法区：会加载 Kafka 和 ReplicaManager 这两个类 字节码执行引擎：class 文件是一个个字节码，JVM 将会使用字节码执行引擎执行代码指令。 程序计数器：用程序计数器来标记代码指令执行到哪一行。 多线程：JVM是支持多线程的，可能开启多个线程来执行代码指令，每一个线程都有自己的程序计数器 Java虚拟机栈：保存方法内的局部变量等数据，线程每执行一个方法，就会对这个方法调用创建对应的一个栈帧 Java 堆内存：new 出来的实例对象将会保存在 Java 堆内存中，栈帧中的局部变量将会指向 Java 堆中的实例对象。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"JVM-Java中的类加载机制","slug":"Java/JVM/JVM-Java中的类加载机制","date":"2019-09-11T23:20:04.000Z","updated":"2019-09-12T02:15:28.579Z","comments":true,"path":"post/5e4eb5a4.html","link":"","permalink":"http://shuiyujie.com/post/5e4eb5a4.html","excerpt":"在正式了解 Java 类加载机制的细节之前，我们有必要了解一下 JVM 整体的运行原理，对 JVM 运行机制的整体脉络进行一次梳理。 首先我们会有一系列以 .java 结尾的源文件。要把这些源文件发布到线上，我们需要将其打成 jar 包，或者打成 war 包。这个打包的过程就是将 .java 文件编译成 .class 文件。接着就可以使用 Tomcat 这样的容器，或者 java 命令来运行一个 jar 包中的文件。 这里有一个问题，编译好的 .class 字节码是怎么运行起来的呢？","text":"在正式了解 Java 类加载机制的细节之前，我们有必要了解一下 JVM 整体的运行原理，对 JVM 运行机制的整体脉络进行一次梳理。 首先我们会有一系列以 .java 结尾的源文件。要把这些源文件发布到线上，我们需要将其打成 jar 包，或者打成 war 包。这个打包的过程就是将 .java 文件编译成 .class 文件。接着就可以使用 Tomcat 这样的容器，或者 java 命令来运行一个 jar 包中的文件。 这里有一个问题，编译好的 .class 字节码是怎么运行起来的呢？ JVM 在什么情况下会加载一个类？JVM 会通过类加载器来加载一个类，然后将类保存到内存中，并执行代码。一个类从加载到使用，一般会经历下面这样一个过程： 那么什么时候 JVM 会使用类加载器去加载一个类呢？答案很简单，就是在使用这个类的时候。 举例来说，如果有下面这样一段代码 12345public class Application &#123; public static void main()&#123; MyConfig config = new MyConfig(); &#125;&#125; Application 类中有一个 main() 函数，main() 作为程序的入口一定会在 JVM 进行启动之后被加载到内存中。之后开始执行 main() 中的方法，遇到别的类就将这个类加载到内存中，情况如图所示： 验证、准备和解析的过程 验证阶段：验证 .class 文件是否符合 Java 虚拟机规范。这点比较容易理解，就像我们使用数据之前要先校验一下。 准备阶段：给加载到的类分配内存空间，比如说 MyConfig 类中有类变量，就要给它分配内存空间，给一个初始值。 解析阶段：将符号引用替换为直接引用 验证、准备和解析可以合称为准备阶段。在准备阶段，JVM 为加载进来的类分配了内存空间，为类变量分配了内存空间，并给类变量赋了初值。接下来的初始化阶段，就会正式执行类初始化的代码了。 核心阶段：初始化前面说了，在准备阶段，JVM 会给类分配内存，会给类变量分配内存并赋予初值。在初始化阶段，则会正式执行类初始化代码。 什么叫正式执行初始化代码呢？来看下面这样一段代码 准备阶段会为类变量 flushInterval 分配内存空间，并且将其初值赋为 0。在初始化阶段，flushInterval 执行赋值语句，获取一个配置参数。 此外，初始化阶段还会执行静态代码块，本例中完成数据的加载工作。 什么时候会初始化一个类呢？ new 一个实例化的对象，将会出发类加载到初始化的全过程，将这个类准备好，再实例化一个对象处理 包含 main() 的主类，将会立即初始化 初始化一个类的时候，如果其父类还未初始化，就必须先初始化他的父类","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"JVM-双亲委派模型","slug":"Java/JVM/JVM-双亲委派模型","date":"2019-09-11T15:02:35.000Z","updated":"2019-09-11T15:50:58.791Z","comments":true,"path":"post/5b2f68e0.html","link":"","permalink":"http://shuiyujie.com/post/5b2f68e0.html","excerpt":"本文介绍 Java 中的类加载器，以及双亲委派机制。 Java 中的类加载器包括：启动类加载器，扩展类加载器，应用程序类加载器，自定义类加载器。","text":"本文介绍 Java 中的类加载器，以及双亲委派机制。 Java 中的类加载器包括：启动类加载器，扩展类加载器，应用程序类加载器，自定义类加载器。 启动类加载器启动类加载器(Bootstrap ClassLoader)，主要负责加载Java目录下的核心类，即 Java 安装目录下的 lib 目录，其中就包含 Java 最核心的一类类库。 JVM一旦启动会首先依托启动类加载器，去加载${JAVA_HOME}/lib中的核心类库。 扩展类加载器扩展类加载器(Extension ClassLoader)，它将会加载 ${JAVA_HOME}/lib/ext 目录中的类。 应用程序类加载器应该程序类加载器Application ClassLoader，负责去加载“ClassPath”环境变量所指定的路径中的类。 可以将其理解为我们写好的 Java 代码，应用程序类加载器负责将我们写的Java代码加载到内存中。 自定义类加载器自定义类加载器，可以根据自己的需求加载类。 双亲委派机制 JVM类加载器具有如上所示的亲子层级结构，启动加载器在第一层，扩展加载器在第二层，应用类加载器在第三层，自定义类加载器在最后一层。 基于该亲子层级结构，就有一个双亲委派机制，具体来说是怎么样的呢？ 具体来说，现在有一个Manager类，它是我们自己写的Java代码，需要使用应用程序类加载器来加载。 此时，应该程序类加载器会先问它的上一级扩展类加载器，你能加载到这个类么？然后扩展类加载器继续问它的上一级启动类加载器，你能加载到这个类么？ 启动类加载器在${JAVA_HOME}/lib下找不到Manage这个类，就告诉扩展类加载器，我加载不到，你自己找去。扩展类加载器在 ${JAVA_HOME}/lib/ext 下也没有找到这个类，叫告诉应用程序类加载器自己去加载这个类。 终于，应该用程序类加载器在自己负责的范围内，比如说我们打成的 jar 包中，找到了这个类，就将其加载到内存中。 总的来说，双亲委派模型就是：先找父亲去加载，父亲加载不到再由儿子来加载。这样的话，可以避免多层级的加载器结构重复加载某些类。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://shuiyujie.com/categories/Java/JVM/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"jvm","slug":"jvm","permalink":"http://shuiyujie.com/tags/jvm/"}]},{"title":"FFMPEG使用说明","slug":"图像处理/FFMPEG使用说明","date":"2019-09-11T14:35:24.000Z","updated":"2019-09-11T23:41:11.884Z","comments":true,"path":"post/20fed56f.html","link":"","permalink":"http://shuiyujie.com/post/20fed56f.html","excerpt":"通过Java调用FFMpeg命令的方式来对音视频进行处理（获取信息、截图等等）。 https://github.com/tonydeng/fmj","text":"通过Java调用FFMpeg命令的方式来对音视频进行处理（获取信息、截图等等）。 https://github.com/tonydeng/fmj 截图命令截取一张352x240尺寸大小，格式为jpg的图片1ffmpeg -i input_file -y -f image2 -t 0.001 -s 352x240 output.jpg 把视频的前30帧转换成一个Animated Gif1ffmpeg -i input_file -vframes 30 -y -f gif output.gif 在视频的第8.01秒出截取230x240的缩略图1ffmpeg -i input_file -y -f mjpeg -ss 8 -t 0.001 -s 320x240 output.jpg 每隔一秒截一张图1ffmpeg -i out.mp4 -f image2 -vf fps=fps=1 out%d.png 每隔20秒截一张图1ffmpeg -i out.mp4 -f image2 -vf fps=fps=1/20 out%d.png 多张截图合并到一个文件里（2x3）每隔一千帧(秒数=1000/fps25)即40s截一张图1ffmpeg -i out.mp4 -frames 3 -vf &quot;select=not(mod(n\\,1000)),scale=320:240,tile=2x3&quot; out.png 从视频中生成GIF图片1ffmpeg -i out.mp4 -t 10 -pix_fmt rgb24 out.gif 转换视频为图片（每帧一张图）1ffmpeg -i out.mp4 out%4d.png 图片转换为视频1ffmpeg -f image2 -i out%4d.png -r 25 video.mp4 切分视频并生成M3U8文件1ffmpeg -i input.mp4 -c:v libx264 -c:a aac -strict -2 -f hls -hls_time 20 -hls_list_size 0 -hls_wrap 0 output.m3u8 相关参数说明： 12345678-i 输入视频文件-c:v 输出视频格式-c:a 输出音频格式-strict-f hls 输出视频为HTTP Live Stream（M3U8）-hls_time 设置每片的长度，默认为2，单位为秒-hls_list_size 设置播放列表保存的最多条目，设置为0会保存所有信息，默认为5-hls_wrap 设置多少片之后开始覆盖，如果设置为0则不会覆盖，默认值为0。这个选项能够避免在磁盘上存储过多的片，而且能够限制写入磁盘的最多片的数量。 注意，播放列表的sequence number对每个segment来说都必须是唯一的，而且它不能和片的文件名（当使用wrap选项时，文件名可能会重复使用）混淆。 分离视频音频流12345# 分离视频流ffmpeg -i input_file -vcodec copy -an output_file_video# 分离音频流ffmpeg -i input_file -acodec copy -vn output_file_audio 视频解复用12ffmpeg -i test.mp4 -vcoder copy -an -f m4v test.264ffmpeg -i test.avi -vcoder copy -an -f m4v test.264 视频转码12345678# 转码为码流原始文件ffmpeg -i test.mp4 -vcoder h264 -s 352*278 -an -f m4v test.264# 转码为码流原始文件ffmpeg -i test.mp4 -vcoder h264 -bf 0 -g 25 -s 352-278 -an -f m4v test.264# 转码为封装文件 -bf B帧数目控制, -g 关键帧间隔控制, -s 分辨率控制ffmpeg -i test.avi -vcoder mpeg4 -vtag xvid -qsame test_xvid.avi 视频封装1ffmpeg -i video_file -i audio_file -vcoder copy -acodec copy output_file 视频剪切12345# 视频截图ffmpeg -i test.avi -r 1 -f image2 image.jpeg# 剪切视频 -r 提取图像频率， -ss 开始时间， -t 持续时间ffmpeg -i input.avi -ss 0:1:30 -t 0:0:20 -vcoder copy -acoder copy output.avi 视频录制1ffmpeg -i rtsp://hostname/test -vcoder copy out.avi YUV序列播放1ffplay -f rawvideo -video_size 1920x1080 input.yuv YUV序列转AVI1ffmpeg -s w*h -pix_fmt yuv420p -i input.yuv -vcoder mpeg4 output.avi 常用参数说明主要参数123-i 设定输入流-f 设定输出格式-ss 开始时间 视频参数12345-b 设定视频流量，默认是200Kbit/s-s 设定画面的宽和高-aspect 设定画面的比例-vn 不处理视频-vcoder 设定视频的编码器，未设定时则使用与输入流相同的编解码器 音频参数1234-ar 设定采样率-ac 设定声音的Channel数-acodec 设定沈阳的Channel数-an 不处理音频 使用ffmpeg合并MP4文件12345ffmpeg -i \"Apache Sqoop Tutorial Part 1.mp4\" -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate1.tsffmpeg -i \"Apache Sqoop Tutorial Part 2.mp4\" -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate2.tsffmpeg -i \"Apache Sqoop Tutorial Part 3.mp4\" -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate3.tsffmpeg -i \"Apache Sqoop Tutorial Part 4.mp4\" -c copy -bsf:v h264_mp4toannexb -f mpegts intermediate4.tsffmpeg -i \"concat:intermediate1.ts|intermediate2.ts|intermediate3.ts|intermediate4.ts\" -c copy -bsf:a aac_adtstoasc \"Apache Sqoop Tutorial.mp4\" 使用ffmpeg转换flv到mp41ffmpeg -i out.flv -vcodec copy -acodec copy out.mp4 视频添加水印水印局中1ffmpeg -i out.mp4 -i sxyx2008@163.com.gif -filter_complex overlay=\"(main_w/2)-(overlay_w/2):(main_h/2)-(overlay_h)/2\" output.mp4 参数解释 -i out.mp4(视频源) -i sxyx2008@163.com.gif(水印图片) overlay 水印的位置 output.mp4 输出文件 视频翻转和旋转翻转水平翻转语法: -vf hflip1ffplay -i out.mp4 -vf hflip 垂直翻转语法：-vf vflip1ffplay -i out.mp4 -vf vflip 旋转语法：transpose={0,1,2,3} 0:逆时针旋转90°然后垂直翻转 1:顺时针旋转90° 2:逆时针旋转90° 3:顺时针旋转90°然后水平翻转 将视频顺时针旋转90度1ffplay -i out.mp4 -vf transpose=1 将视频水平翻转(左右翻转)1ffplay -i out.mp4 -vf hflip 顺时针旋转90度并水平翻转1ffplay -i out.mp4 -vf transpose=1,hflip 添加字幕有的时候你需要给视频加一个字幕(subtitle)，使用ffmpeg也可以做。一般我们见到的字幕以srt字幕为主，在ffmpeg里需要首先将srt字幕转化为ass字幕，然后就可以集成到视频中了(不是单独的字幕流，而是直接改写视频流)。 12ffmpeg -i my_subtitle.srt my_subtitle.assffmpeg -i inputfile.mp4 -vf ass=my_subtitle.ass outputfile.mp4 但是值得注意的是： my_subtitle.srt需要使用UTF8编码，老外不会注意到这一点，但是中文这是必须要考虑的； 将字幕直接写入视频流需要将每个字符渲染到画面上，因此有一个字体的问题，在ass文件中会指定一个缺省字体，例如Arial，但是我们首先需要让ffmpeg能找到字体文件，不然文字的渲染就无从谈起了。ffmpeg使用了fontconfig来设置字体配置。你需要首先设置一下FONTCONFIG_PATH或者FONTCONFIG_FILE环境变量，不然fontconfig是无法找到配置文件的，这一点请参看这篇文章，如果你设置的是FONTCONFIG_PATH，那把配置文件保存为%FONTCONFIG_PATH%/font.conf即可，然后你可以在font.conf文件中配置字体文件的路径之类的。 Windows下为fontconfig设置如下的环境变量 1234FC_CONFIG_DIR=C:\\ffmpegFONTCONFIG_FILE=font.confFONTCONFIG_PATH=C:\\ffmpegPATH=C:\\ffmpeg\\bin;%PATH% 下面是一个简单的Windows版font.conf文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?xml version=\"1.0\"?&gt;&lt;fontconfig&gt;&lt;dir&gt;C:\\WINDOWS\\Fonts&lt;/dir&gt;&lt;match target=\"pattern\"&gt; &lt;test qual=\"any\" name=\"family\"&gt;&lt;string&gt;mono&lt;/string&gt;&lt;/test&gt; &lt;edit name=\"family\" mode=\"assign\"&gt;&lt;string&gt;monospace&lt;/string&gt;&lt;/edit&gt;&lt;/match&gt;&lt;match target=\"pattern\"&gt; &lt;test qual=\"all\" name=\"family\" mode=\"not_eq\"&gt;&lt;string&gt;sans-serif&lt;/string&gt;&lt;/test&gt; &lt;test qual=\"all\" name=\"family\" mode=\"not_eq\"&gt;&lt;string&gt;serif&lt;/string&gt;&lt;/test&gt; &lt;test qual=\"all\" name=\"family\" mode=\"not_eq\"&gt;&lt;string&gt;monospace&lt;/string&gt;&lt;/test&gt; &lt;edit name=\"family\" mode=\"append_last\"&gt;&lt;string&gt;sans-serif&lt;/string&gt;&lt;/edit&gt;&lt;/match&gt;&lt;alias&gt; &lt;family&gt;Times&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Times New Roman&lt;/family&gt;&lt;/prefer&gt; &lt;default&gt;&lt;family&gt;serif&lt;/family&gt;&lt;/default&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;Helvetica&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Arial&lt;/family&gt;&lt;/prefer&gt; &lt;default&gt;&lt;family&gt;sans&lt;/family&gt;&lt;/default&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;Courier&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Courier New&lt;/family&gt;&lt;/prefer&gt; &lt;default&gt;&lt;family&gt;monospace&lt;/family&gt;&lt;/default&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;serif&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Times New Roman&lt;/family&gt;&lt;/prefer&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;sans&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Arial&lt;/family&gt;&lt;/prefer&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;monospace&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Andale Mono&lt;/family&gt;&lt;/prefer&gt;&lt;/alias&gt;&lt;match target=\"pattern\"&gt; &lt;test name=\"family\" mode=\"eq\"&gt; &lt;string&gt;Courier New&lt;/string&gt; &lt;/test&gt; &lt;edit name=\"family\" mode=\"prepend\"&gt; &lt;string&gt;monospace&lt;/string&gt; &lt;/edit&gt;&lt;/match&gt;&lt;match target=\"pattern\"&gt; &lt;test name=\"family\" mode=\"eq\"&gt; &lt;string&gt;Courier&lt;/string&gt; &lt;/test&gt; &lt;edit name=\"family\" mode=\"prepend\"&gt; &lt;string&gt;monospace&lt;/string&gt; &lt;/edit&gt;&lt;/match&gt;&lt;/fontconfig&gt; 下面这个是Linux系统下改版过来的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE fontconfig SYSTEM \"fonts.dtd\"&gt;&lt;!-- /etc/fonts/fonts.conf file to configure system font access --&gt;&lt;fontconfig&gt;&lt;!-- Find fonts in these directories--&gt;&lt;dir&gt;C:/Windows/Fonts&lt;/dir&gt;&lt;!--&lt;dir&gt;/usr/X11R6/lib/X11/fonts&lt;/dir&gt;--&gt;&lt;!-- Accept deprecated 'mono' alias, replacing it with 'monospace'--&gt;&lt;match target=\"pattern\"&gt; &lt;test qual=\"any\" name=\"family\"&gt;&lt;string&gt;mono&lt;/string&gt;&lt;/test&gt; &lt;edit name=\"family\" mode=\"assign\"&gt;&lt;string&gt;monospace&lt;/string&gt;&lt;/edit&gt;&lt;/match&gt;&lt;!-- Load per-user customization file, but don't complain if it doesn't exist--&gt;&lt;include ignore_missing=\"yes\" prefix=\"xdg\"&gt;fontconfig/fonts.conf&lt;/include&gt;&lt;!-- Load local customization files, but don't complain if there aren't any--&gt;&lt;include ignore_missing=\"yes\"&gt;conf.d&lt;/include&gt;&lt;include ignore_missing=\"yes\"&gt;local.conf&lt;/include&gt;&lt;!-- Alias well known font names to available TrueType fonts. These substitute TrueType faces for similar Type1 faces to improve screen appearance.--&gt;&lt;alias&gt; &lt;family&gt;Times&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Times New Roman&lt;/family&gt;&lt;/prefer&gt; &lt;default&gt;&lt;family&gt;serif&lt;/family&gt;&lt;/default&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;Helvetica&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Arial&lt;/family&gt;&lt;/prefer&gt; &lt;default&gt;&lt;family&gt;sans&lt;/family&gt;&lt;/default&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;Courier&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Courier New&lt;/family&gt;&lt;/prefer&gt; &lt;default&gt;&lt;family&gt;monospace&lt;/family&gt;&lt;/default&gt;&lt;/alias&gt;&lt;!-- Provide required aliases for standard names Do these after the users configuration file so that any aliases there are used preferentially--&gt;&lt;alias&gt; &lt;family&gt;serif&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Times New Roman&lt;/family&gt;&lt;/prefer&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;sans&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Arial&lt;/family&gt;&lt;/prefer&gt;&lt;/alias&gt;&lt;alias&gt; &lt;family&gt;monospace&lt;/family&gt; &lt;prefer&gt;&lt;family&gt;Andale Mono&lt;/family&gt;&lt;/prefer&gt;&lt;/alias&gt;&lt;/fontconfig&gt; 参考： http://blog.raphaelzhang.com/2013/04/video-streaming-and-ffmpeg-transcoding/ 嵌入字幕在一个MP4文件里面添加字幕，不是把 .srt 字幕文件集成到 MP4 文件里，而是在播放器里选择字幕，这种集成字幕比较简单，速度也相当快 1ffmpeg -i input.mp4 -i subtitles.srt -c:s mov_text -c:v copy -c:a copy output.mp4 希望字幕直接显示出来，其实也不难 12ffmpeg -i subtitle.srt subtitle.assffmpeg -i input.mp4 -vf ass=subtitle.ass output.mp4 参考： http://blog.neten.de/posts/2013/10/06/use-ffmpeg-to-burn-subtitles-into-the-video/","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://shuiyujie.com/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://shuiyujie.com/tags/图像处理/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://shuiyujie.com/tags/ffmpeg/"},{"name":"音频处理","slug":"音频处理","permalink":"http://shuiyujie.com/tags/音频处理/"}]},{"title":"如何解决文件分享问题","slug":"善用佳软/如何解决文件分享问题","date":"2019-09-10T15:30:04.000Z","updated":"2019-09-23T11:19:40.943Z","comments":true,"path":"post/8896ebad.html","link":"","permalink":"http://shuiyujie.com/post/8896ebad.html","excerpt":"互联网时代每个人都能成为内容创作者，当你需要分享时，尽管各大社交平台都会自带分享服务，但是这些分享服务往往都是有壁垒的，比如需要注册才能使用、只能在该平台内分享、无法私密分享、无法匿名分享、没有提供文件直链、下载速度受限等等。 因此，今天分享的文章要解决的就是文件分享难题：文件直链和文件传输。 为内容创作者解决文件分享的难题","text":"互联网时代每个人都能成为内容创作者，当你需要分享时，尽管各大社交平台都会自带分享服务，但是这些分享服务往往都是有壁垒的，比如需要注册才能使用、只能在该平台内分享、无法私密分享、无法匿名分享、没有提供文件直链、下载速度受限等等。 因此，今天分享的文章要解决的就是文件分享难题：文件直链和文件传输。 为内容创作者解决文件分享的难题 二维码分享 草料二维码 支持多种内容的二维码生成工具，提供多种丰富的美化二维码。 QArt Coder 生成二维码的同时，还可以将你上传的图片处理成黑白像素风格的二维码。（V） 第九工厂 艺术二维码生成，需要注册和付费，但一些免费的的魔板也很不错。 芝麻二维码 可以让多个二维码合并的网站。 短地址分享 新浪短网址 支持包括新浪，百度，腾讯，Bitly等10家服务并永久有效，有 小书签可以一键生成。 百度网址 百度旗下的短网址服务。 cmcc.in 具有数据统计功能，需要运营统计时的可以考虑。 淘宝宝短网址 只适用于淘宝链接。 suo.im 批量短网址，支持一次生成300+短链接。 Bitly 国外老牌短网址服务应用，支持数据统计功能。 U.NU 国外短网址服务，支持小书签一键生成。 ffff.im 优雅的短网址服务，同时无需注册就支持数据统计。 代码分享 Ubuntu Pastebin 无需注册，应该是最方便的选择，支持定期失效。 GithubGist 需注册，知名代码托管社区Github提供的服务。(V) Pastebin 无需注册+注册，登录后拥有更多功能，支持浏览器拓展。 Zeropaste 无需注册，支持私密分享，支持MD代码预览。 文本分享 note.ms 无需注册，简洁，自动生成链接，也可以自定义链接，链接里的内容会实时更新。 PrivNote 无需注册，笔记将在阅读后自动销毁。 Vuash 无需注册，开源项目，笔记将在阅读后自动销毁。 faxZero 无需注册，在线发传真，美国加拿大免费，其他国家需要收费。 Eddtor 无需注册，支持富文本内容分享，登录后有更多功能。 谷歌文档 需注册，第一好用的在线文档编辑应用，但大陆地区无法正常访问。 腾讯文档 需注册，腾讯推出的文档分享应用，支持 QQ、微信一键登录。 石墨文档 需注册，在线协作第一选择，替代谷歌文档的选择，附件支持直接分享。 Treation 需注册，功能与石墨文档类似，但专注于在线表格，附件支持直接分享。 图片分享（图床） 新浪图床 需注册，服务稳定速度超快，支持三种尺寸，支持拓展。（Firefox，Chrome） 免登录新浪图床 无需注册，无需登录也可以使用新浪图床。 SM.SM 无需注册，服务稳定速度快，每次最多10张，每张最大5Mb。 A.Photo 无需注册，速度快，和SM.SM一样是个人网站，注册后可保存上传记录。 Imgse 无需注册，速度快，提供图片直链，每张最大10MB，注册后可保存上传记录。 牛图网 无需注册，速度快，成立于2010年，服务稳定。 Vim-cn 无需注册，非常直白的上传界面，速度不错，海外服务器多。 Imgur 无需注册，国外老牌图片分享社区，图片支持外链，但国内无法上传。 Image.frl 无需注册，荷兰老牌图片分享网站，多用于博客和论坛，分享页支持评论。 PhotoBucket 需注册，国外老牌图片分享网站，多见于论坛图片分享，国内速度比较慢。 多媒体分享（直链） 音乐解析器 无需注册，输入云音乐，QQ音乐，虾米音乐的链接即可获取mp3外链。MP3直链测试 170MV 无需注册，输入音悦台视频即可解析获得mp4外链。MP4直链测试 Streamable 无需注册，最大支持2G，10分钟的视频上传，速度快，支持编辑功能。MP4直链测试 Coka.la 无需注册，最大支持512M任何格式文件的上传，速度一般，快进会小卡，永久保存。 （TXT直链测试，MP3直链测试，MP4直链测试） dmca.gripe 无需注册，最大支持5G任何格式文件的上传，根据文件大小需要等1-2分钟不等，速度一般但足够用了。 （TXT直链测试，MP3直链测试，MP4直链测试） 石墨文档 需注册，任何格式文件最大支持100M，文档删除后附件外接仍然有效，七牛云服务器速度极快。 （TXT直链测试，MP3直链测试，MP4直链测试） Treation 需注册，免费版任何格式文件最大支持10M，文档删除后附件外接仍然有效，速度极快。 （TXT直链测试，MP3直链测试，MP4直链测试） 七牛云 需注册，上传内容没有限制，速度极快，免费用户每月10G流量，一般使用也是足够的。 （TXT直链测试，MP3直链测试，MP4直链测试） 无限图床 无需注册，无任何限制，支持所有文件格式，区块链应用，文件存储到去中心化网络中。 （TXT直链测试，MP3直链测试，MP4直链测试） 文件分享 奶牛快传 无需注册，临时，支持最大2GB的文件，最多同时上传6个文件，保存时间为7天。 Firefox Send 无需注册，临时，火狐浏览器出品，支持按时间或者下载次数后自动删除文件。 WeTransfer 无需注册，临时，最大支持2GB文件，保存周期14天，链接会发送到指定的邮箱中。 SendAnywhere 无需注册，临时，密钥、链接、邮件三种方式分享方式，最大支持4GB文件。 Ge.tt 无需注册，临时，分享的文件链接会保存 30 天时间。 Volafile 无需注册，临时，建立一个专属的分享房间，左边聊天右边分享各类文件。 reep.io 无需注册，永久，P2P 传输技术，不会在服务器上存储任何文件。 drop.me 无需注册，永久，没有大小限制，没有文件格式限制，但国内速度一般。 Simple Savr 无需注册，临时，只能在同一局域网内使用，可以在当你没有在电脑上登陆QQ微信时使用。 超大文件分享（个人存储） 百度云 需注册，国内使用率最高的云盘，海量资源共享，提高下载速度可以 看这里。 微云 需注册，下载速度快，上传速度稍慢，如果要分享给的朋友不用百度云，可以用这个。 蓝奏云 需注册，国内网盘提供商，不限制下载速度，免费用户可以上传100M的文件。 Google Drive 需注册，谷粉的第一选择。(V) One Drive 需注册，OnedDrive，Google Drive，Dropbox 都可以用直链生成工具来获取直接。(V) Dropbox 需注册，网盘的领路人，有很多围绕Dropbox而产生的神奇用法，比如ifttt等等。(V)","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://shuiyujie.com/tags/网站/"}]},{"title":"手机接码平台，马甲邮箱","slug":"善用佳软/手机接码平台，马甲邮箱","date":"2019-09-10T15:20:04.000Z","updated":"2019-09-23T11:19:40.942Z","comments":true,"path":"post/6b53840a.html","link":"","permalink":"http://shuiyujie.com/post/6b53840a.html","excerpt":"介绍接码平台和马甲邮箱，保护隐私从源头做起。","text":"介绍接码平台和马甲邮箱，保护隐私从源头做起。 接码平台 易码 接码平台，收费换手机号，便宜 阿里小号 一般用它来接收验证码，需要的时候打开 App，即用即开，关闭后也不会打扰用户。目前价格是一年 20 元（优惠中，很方便） SMSReceiveFree 可以收到來自世界各地的簡訊和來電，提供來自美國、英國和加 拿大的虛擬門號。SMS訊息會在24小時内删除。每個月會更換一批新的電話號碼。 Receive-SMS-Online 提供來自美國、西班牙、法國、義大利、荷蘭、波蘭、荷蘭、德國、羅馬尼亞、英國等的虛擬號碼。 Getfreesmsnumber 提供來自英國、美國、比利時、加拿大、瑞典、德國、澳洲、捷克和其他國家的虛擬號碼。 Receive-a-sms 提供來自丹麥、印尼、美國、匈牙利、智利、西班牙、葡萄牙、英國、法國、俄羅斯、澳洲、波蘭、加拿大、荷蘭、奥地利、德國、瑞典、韓國、马来西亚、捷克等國的虛擬號碼。 Receive SMS Online for FREE and with NO Registration 提供巴西、美國、法國、英國、德國、加拿大、香港，澳 洲、西班牙、以色列等國的虛擬號碼。 马甲邮箱 十分钟邮箱 十分钟邮箱适用于那些没有多大再次访问价值的网站。邮箱十分钟内有效（部分支持延迟），临时收个验证码很方便。 ThrowAwayMail 售价： 免费优点： 清晰明了的界面、优秀的易用性缺点： 功能单一、无法回复邮件适合人群： 需求简单、即用即走的用户 ThrowAwayMail 是四款服务里界面最简洁清爽的，页面布局也更易用合理。只需点击蓝色提示并通过机器人验证，系统便会自动分配一个马甲邮箱地址给你，限时 48 小时。你可以申请延时 48 小时，因此一个邮箱地址共可以使用 96 小时。 Dropmail 售价： 免费（提供自愿捐赠链接）特点： 功能全面、无时间限制、整合了聊天工具推送服务缺点： 界面设计过于陈旧、难以查看和管理大量邮件、无法回复邮件适合人群： 有移动端查看邮件需求的用户Dropmail 的设计语言稍显老旧，没有多余的颜色和装饰，基本上由文字和按钮组成，但功能却一个不少。而最令人欣喜的是，提供多达 23 种语言的 Dropmail 官方支持中文。 Guerrilla Mail 售价： 免费（提供自愿捐赠链接）特点： 可以发送邮件（包括附件）、有 Android 客户端、集成密码管理工具缺点： 邮件接收有延迟、Android App 已经很久未更新适合人群： 有长期使用马甲邮箱、需要发信功能的用户Guerrilla 在英文中有「游击队」之意，而 Guerrilla Mail 正如其名，通过给用户提供可丢弃的临时邮箱地址，让广告邮箱商和黑客找不着北。 TrashMail 售价： 基础版免费；高级版 21.99 美元/年特点： 统一管理转发地址、API 支持、直接回复缺点： 免费版限制过多适合人群： 有统一管理马甲邮箱的需求或希望集成该功能进自己应用的用户 巧用 gmail我们以 abc123@gmail.com 举例，后文中的 @ 使用符号 # 代替。 法则一：@ 前面的部分，可以在任意位置加个 . 对于 Gmail 而言有没有这个点都是一样的。 例如 abc123 # gmail.com，可以改为以下任何一个： abc.123 # gmail.com abc…123 # gmail.com .abc1.2.3. # gmail.com …abc…123… # gmail.com 法则二：用户名和 @ 之间，可以用 + 插入任何字串，而且可以加插任意多个 + 例如abc123 # gmail.com，可以改为以下任何一个： abc123+def456 # gmail.com abc123+a+b+c+1+2+3 # gmail.com abc123+Ax+By+C # gmail.com 真正的粉丝，还可以用这个 ： abc123+1s # gmail.com abc123+2s # gmail.com abc123+3s # gmail.com … 用这个方法，可以让自己在不同网站的注册名称不同，例如 abc123+Apple # gmail.com abc123+Facebook # gmail.comabc 123+Twitter # gmail.com 甚至可以作为临时邮箱地址，例如 abc123+temp+1 # gmail.com abc123+temp+2 # gmail.com 这样子，只要开始收到垃圾邮件，看看收件人就可以知道是那个网站出卖了你的个人资料了。不过需要注意，不是所有服务也支持带有 + 的邮箱地址。 法则三：上面两个法则可以任意搭配。例如，abc123 # gmail.com 可以改为以下任何一个： abc…123+Ma.ki+Ni.co # gmail.com abc123.+.Nico.Nico.Ni. # gmail.com abc.1.2.3+Yosoro.+.Zura # gmail.com 这样，就可以创造多元化无限创意的小号了。","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://shuiyujie.com/tags/网站/"}]},{"title":"计算机视觉学习路线","slug":"Deeplearning/计算机视觉学习路线","date":"2019-09-10T14:50:10.000Z","updated":"2019-09-28T13:51:21.477Z","comments":true,"path":"post/9027adc3.html","link":"","permalink":"http://shuiyujie.com/post/9027adc3.html","excerpt":"深度学习算法工程师的基本要求 熟练掌握python和c++编程，至少熟悉 Caffe 和 Tensorflow/Pytorch 两种框架。 熟练玩转深度学习各类模型架构使用和设计。 熟练玩转数据的整理和使用，必须深刻理解数据在深度学习任务中的地位。","text":"深度学习算法工程师的基本要求 熟练掌握python和c++编程，至少熟悉 Caffe 和 Tensorflow/Pytorch 两种框架。 熟练玩转深度学习各类模型架构使用和设计。 熟练玩转数据的整理和使用，必须深刻理解数据在深度学习任务中的地位。 编程语言C/C++ C/C++面试基础知识总结 Google 开源项目风格指南 (中文版) Python Python - 100天从新手到大师 List of Data Science Cheatsheets to rule the world 一些有趣且鲜为人知的 Python 特性. Python 爬虫相关 Linux Linux命令大全搜索工具，内容包含Linux命令手册、详解、学习、搜集 shell &amp; git &amp; vim OpenCV 《OpenCV3编程入门》书本配套源码 LearnOpencv 基于OpenCV4.0 C++/Python SDK的案例代码演示程序与效果图像 opencv 理论知识相关论文 计算机视觉入门论文：computer version AutoML 论文合集：Awesome-AutoML-Papers Most popular metrics used to evaluate object detection algorithms. virgilio 传统的图像降噪算法和深度学习图像降噪算法重要论文，以及Benchmark的总结：GitHub - flyywh/Image-Denoising-State-of-the-art 深度学习 CS231 李飞飞 已授权个人翻译笔记 深度学习500问 《神经网络与深度学习》 Neural Network and Deep Learning 《机器学习》（西瓜书）公式推导解析 吴恩达老师的机器学习课程个人笔记 深度学习入门教程&amp;&amp;优秀文章&amp;&amp;Deep Learning Tutorial 机器学习&amp;深度学习网站资源汇总（Machine Learning Resources） 深度学习工程模板 深度学习模型大合集：Deep Learning Models 深度学习综述：Awesome Deep Learning 《迁移学习简明手册》 机器学习 统计学习方法 《统计学习方法》的代码实现 《统计学习方法》知识点总结 Numpy手写主流机器学习模型 100-Days-Of-ML-Code 100-Days-Of-ML-Code中文版 机器学习算法python实现 Homemade Machine Learning 机器学习综述：Awesome Machine Learning 开源框架TensorFlow TensorFlow 2.0 Tutorials TensorFlow examples TensorFlow Tutorial and Examples for Beginners (support TF v1 &amp; v2) TensorFlow Course An Open Source Machine Learning Framework for Everyone Pytorch Awesome-Pytorch-list Pytorch模型训练实用教程 Caffe BVLC/caffe Darknet darknet深度学习框架源码分析 AlexeyAB/darknet","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"学习资料","slug":"学习资料","permalink":"http://shuiyujie.com/tags/学习资料/"},{"name":"machinelearning","slug":"machinelearning","permalink":"http://shuiyujie.com/tags/machinelearning/"}]},{"title":"3分钟找书指南","slug":"善用佳软/3分钟找书指南","date":"2019-09-10T14:30:04.000Z","updated":"2019-10-18T13:41:57.727Z","comments":true,"path":"post/c5829ae7.html","link":"","permalink":"http://shuiyujie.com/post/c5829ae7.html","excerpt":"3分钟找书指南v1.0","text":"3分钟找书指南v1.0 Jiumo Search 鸠摩搜书 - 文档搜索引擎 http://mebook.cc/ 万千合集站 ePUBee电子书库，最大的电子书库，在线电子书管理 书伴-Kindle Lore Free-去中心化知识共享社区 http://www.iamtxt.com/ Vol.moe-漫画网站 SoBooks - kindle 超星发现 OpenStax|免费大学教科书数据库) 中国国家图书馆 周读 熊猫搜书 亿年书海 - 电子书 wiki 网 中國哲學書電子化計劃 书格 古籍館-中國最大的古籍圖書館","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://shuiyujie.com/tags/网站/"}]},{"title":"【Java并发编程】Lock&Condition","slug":"Java/Java 并发编程/【Java并发编程】Lock-Condition","date":"2019-09-09T14:18:02.000Z","updated":"2019-09-11T14:59:39.489Z","comments":true,"path":"post/3e6f77c3.html","link":"","permalink":"http://shuiyujie.com/post/3e6f77c3.html","excerpt":"在并发编程中有两个核心问题:一个是互斥,即同一时刻只允许一个线程访问共享资源;另一个是同步,即线程之间如何通信和协作。解决这两个问题的方法前人已经替我们总结出来理论模型,分别是管程模型和信号量模型. Java中实现管程有两种方式 一是使用synchronized关键字给代码块添加隐式锁实现互斥,同时使用notify()和notifyAll()实现同步 二是使用 Java SDK 并发包下的 Lock 和 Condition 两个接口,来实现管程. Lock 的特性包括:能够响应中断、支持超时和非阻塞地获取锁 Condition 实现了管程模型里面的条件变量。 注：Java 参考了 MESA 模型，语言内置的管程（synchronized）对 MESA 模型进行了精简。MESA 模型中，条件变量可以有多个，Java 语言内置的管程里只有一个条件变量。Java 并发包下的 Lock&amp;Condition则则支持多个条件变量。","text":"在并发编程中有两个核心问题:一个是互斥,即同一时刻只允许一个线程访问共享资源;另一个是同步,即线程之间如何通信和协作。解决这两个问题的方法前人已经替我们总结出来理论模型,分别是管程模型和信号量模型. Java中实现管程有两种方式 一是使用synchronized关键字给代码块添加隐式锁实现互斥,同时使用notify()和notifyAll()实现同步 二是使用 Java SDK 并发包下的 Lock 和 Condition 两个接口,来实现管程. Lock 的特性包括:能够响应中断、支持超时和非阻塞地获取锁 Condition 实现了管程模型里面的条件变量。 注：Java 参考了 MESA 模型，语言内置的管程（synchronized）对 MESA 模型进行了精简。MESA 模型中，条件变量可以有多个，Java 语言内置的管程里只有一个条件变量。Java 并发包下的 Lock&amp;Condition则则支持多个条件变量。 为什么要用 lock&amp;condition 再次实现管程Java 已经通过synchronized实现了管程,那么为什么 jdk1.5 还要在并发包中用 lock&amp;condition 来实现管程? 这两种实现方式存在什么区别么? 它们各自有什么样的应用场景? 在之前的死锁问题一文中提到了死锁的产生必须同时具备 4 个条件,我们只需要破坏其中任意一个条件都能够预防死锁的发生.其中有一个条件是不可抢占,也就是线程 T1 持有某一个资源,其他线程不能强行抢占它持有的资源. 当我们使用synchronized加锁时,当它持有锁A,但是无法持有到锁 B, 时线程就会直接进入阻塞状态,此时一旦发生死锁就没有办法唤醒这个线程了.Lock&amp;Condition的出现就是为了做到破坏不可抢占条件. 具体来说,怎么才能破坏不可抢占条件呢?有这样 3 个思路: 能够响应中断.当线程进入阻塞状态之后,我们能够发送一个中断信号给这个阻塞的线程.该线程能够响应中断信号,就有机会释放持有的锁 A,就能破坏不可抢占的条件. 超时机制.线程在一段时间内无法获取到锁,它不会进入阻塞状态,而是返回错误,那么这个线程也有机会释放持有的锁. 非阻塞地获取锁.如果尝试获取锁失败，并不进入阻塞状态，而是直接返回. 这三种方案可以弥补 synchronized 实现的管程无法破坏不可抢占条件的问题,具体来说 Lock 接口实现这里这样 3 个方法: 123456// 支持中断的 APIvoid lockInterruptibly() throws InterruptedException;// 支持超时的 APIboolean tryLock(long time, TimeUnit unit) throws InterruptedException;// 支持非阻塞获取锁的 APIboolean tryLock(); 用两个条件变量实现阻塞队列 Java 语言内置的管程里只有一个条件变量，而 Lock&amp;Condition 实现的管程是支持多个条件变量的，这是二者的一个重要区别。 在很多并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易。例如，实现一个阻塞队列，就需要两个条件变量。 一个阻塞队列，需要两个条件变量，一个是队列不空（空队列不允许出队），另一个是队列不满（队列已满不允许入队）。 123456789101112131415161718192021222324252627282930313233343536373839public class BlockedQueue&lt;T&gt; &#123; final Lock lock = new ReentrantLock(); // 条件变量,队列不满 final Condition notFull = lock.newCondition(); // 条件变量.队列不空 final Condition notEmpty = lock.newCondition(); // 入队 void enq(T x) &#123; lock.lock(); try &#123; while(队列已满)&#123; // 等待队列不满 notFull.await(); &#125; // 省略入队操作 // 入队后,通知可出队 notEmpty.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125; // 出队 void deq()&#123; lock.lock(); try&#123; while(队列已空)&#123; // 等待队列不空 notEmpty.await(); &#125; // 省略出队操作 // 出队后,通知可入队 notFull.signal(); &#125;finally &#123; lock.unlock(); &#125; &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"},{"name":"管程","slug":"管程","permalink":"http://shuiyujie.com/tags/管程/"},{"name":"并发工具类","slug":"并发工具类","permalink":"http://shuiyujie.com/tags/并发工具类/"}]},{"title":"【Java并发编程】Java线程的生命周期","slug":"Java/Java 并发编程/【Java并发编程】Java线程的生命周期","date":"2019-09-08T02:47:53.000Z","updated":"2019-09-11T14:59:35.977Z","comments":true,"path":"post/541ca933.html","link":"","permalink":"http://shuiyujie.com/post/541ca933.html","excerpt":"在 Java 领域，实现并发程序的主要手段是多线程。线程是操作系统的概念，虽然不同语言对线程操作进行了不同的封装，但是万变不离其综。 通用的线程模型可以用「五态模型」来描述，它们分别是：初始状态，可运行状态，运行状态，休眠状态和终止状态。 这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。 除了简化合并，这五种状态也有可能被细化，比如，Java 语言里就细化了休眠状态。","text":"在 Java 领域，实现并发程序的主要手段是多线程。线程是操作系统的概念，虽然不同语言对线程操作进行了不同的封装，但是万变不离其综。 通用的线程模型可以用「五态模型」来描述，它们分别是：初始状态，可运行状态，运行状态，休眠状态和终止状态。 这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。 除了简化合并，这五种状态也有可能被细化，比如，Java 语言里就细化了休眠状态。 Java 线程的生命周期Java 语言中的线程有 6 种状态 NEW (初始状态) RUNNABLE (可运行/运行状态) BLOCKED (阻塞状态) WATTING (无限期等待) TIMED_WAITING (有时限等待) TERMINATED (终止状态) 其中 BLOCKED、WATTING 和 TIMED_WAITING 在操作系统层面同属于前面提高的休眠状态。简化的 Java 的生命周期图，如下所示： 那么些状态是如何产生，如何进行转换的呢？ 从 NEW 到 RUNNABLENEW 状态指的是在编程语言层面创建了线程，但是操作系统层面还未创建线程。Java 中刚创建出来的 Thread 对象就是 NEW 状态，创建 Thread 对象的方法有两种： 继承 Thread 对象，重写 run() 方法 实现 RUNNABLE 接口，重写 run() 方法 此时通过MyThread myThread = new MyThread()或者Thread thread = new Thread(new Runner())创建出来的线程就处在 NEW 状态。 如果想要将线程转换到 RUNNABLE 状态，只需要使用start()方法，即myThread.start() 或者 thread.start()。 从 RUNNABLE 到 BLOCKEDBLOCK 表示阻塞状态，在 Java 中只有一种情况会触发 RUNNABLE 到 BLOCKED 的转换，那就是线程等待 synchronized 锁。synchronized 修饰的代码块同一时刻只允许一个线程执行，其他线程就只能等待。当等待的线程拿到 synchronized 隐式锁的时，就会从 BLOCKED 切换到 RUNNABLE。 这里的 BLOCKED 和操作系统层面的阻塞有所区别。在操作系统层面，当线程调用阻塞时 API，比如说进行 IO 操作，那么该线程就会处于休眠状态。但是在 JVM 层面 Java 程序的状态并不会发生改变，任然会保持为了 RUNNABLE 状态。JVM 层面并不关系操作系统调度相关的状态。在 JVM 看来，等待 CPU 的使用权，与等待 I/O 没有区别，都是在等待某个资源，所以都归入到 RUNNABLE 状态。 从 RUNNBALE 到 WAITING总体来说，有三种场景会触发这种转换。 第一种场景，获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法。 第二种场景，调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完， 而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。 第三种场景，调用 LockSupport.park() 方法。其中的 LockSupport 对象，也许你有点陌生，其实 Java 并发包中的锁，都是基于它实现的。调用 LockSupport.park() 方法，当前线程会阻塞， 线程的状态会从 RUNNABLE 转换到 WAITING。调用 LockSupport.unpark(Thread thread) 可 唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE。 从 RUNNBALE 到 TIMED_WATING有五种场景会触发这种转换： 调用带超时参数的 Thread.sleep(long millis) 方法； 获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法； 调用带超时参数的 Thread.join(long millis) 方法； 调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法； 调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。 从 RUNNBALE 到 TERMINATED线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访 问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法。 interrupt() 会通过异常或者主动监测的方式通知线程，线程有机会执行后续操作，也可以无视这个通知。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"}]},{"title":"【Java并发编程】死锁","slug":"Java/Java 并发编程/【Java并发编程】死锁","date":"2019-09-07T14:57:54.000Z","updated":"2019-09-11T14:59:21.975Z","comments":true,"path":"post/3d624a82.html","link":"","permalink":"http://shuiyujie.com/post/3d624a82.html","excerpt":"我们来想象一个场景。现在你是清朝末年账房里的一个先生，张三要给李四转4两银子，你从柜台取过张三的账本，发现李四的账本不在了，于是你想先留着张三的账本，等会儿再去柜台看看李四的账本别人用完没。 没成想，李四想着给张三转5两银子，另一个账房先生和你同时取了李四的账本。你俩想法一样，他留着李四的账本，等着一会儿去拿张三的账本。 现在的局面正是俩人互相「死等」对方的账本，在代码的世界，这种局面就叫做「死锁」。死锁用书面点的方法来说就是：一组互相竞争的线程互相等待，导致「永久」阻塞的现象。","text":"我们来想象一个场景。现在你是清朝末年账房里的一个先生，张三要给李四转4两银子，你从柜台取过张三的账本，发现李四的账本不在了，于是你想先留着张三的账本，等会儿再去柜台看看李四的账本别人用完没。 没成想，李四想着给张三转5两银子，另一个账房先生和你同时取了李四的账本。你俩想法一样，他留着李四的账本，等着一会儿去拿张三的账本。 现在的局面正是俩人互相「死等」对方的账本，在代码的世界，这种局面就叫做「死锁」。死锁用书面点的方法来说就是：一组互相竞争的线程互相等待，导致「永久」阻塞的现象。 死锁的局面我们现在设法用代买来描述「死锁」的局面。 12345678910111213141516171819public class Account &#123; private int balance; // 转账 void transfer(Account target, int amt)&#123; // 锁定转出账户 synchronized (this)&#123; // 锁定转入账户 synchronized (target)&#123; if(this.balance &gt; amt)&#123; this.balance -= amt; target.balance += amt; &#125; &#125; &#125; &#125;&#125; 我们现在有一个账户类 Account，transfer()是一个转账的方法，它将当前对象的钱转到目标账户中。为了保证transfer()能够并发执行而不出现问题，我们给采用细粒度锁，即对资源this和target分别加上了锁，情况如下图所示： 当线程 T1 指向由账户 A 向账户 B 转账的操作，线程 T2 指向由账户 B 向账户 A 转账的操作。线程 T1 和 T2 同时执行到synchronized (this)，分别锁定了账户 A 和账户 B。当它们继续向下执行到synchronized (target)时会发现它们互相无法取得自己需要的锁，因为锁已经被对方持有，这就造成了「死锁的局面」。 死锁产生的 4 中条件及解决办法并发程序一旦死锁，我们一般只能重启程序。所以没有很好的办法直接解决死锁，我们能做的是规避死锁，也就是避免死锁产生的条件，有个叫 Coffman 的牛人已经总结过了，只有以下这四个条件都发生时才会出现死锁： 产生条件 解决办法 1. 互斥，共享资源 X 和 Y 只能被一个线程占用 用锁就是为了互斥，所以不会破坏这个条件 2. 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X； 一次性申请所有资源 3. 不可抢占，其他线程不能强行抢占线程 T1 占有的资源； 线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源 4. 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源 按序申请资源 具体来说一下解决方案： 一次申请所有资源。用一个 List 保存所有被占用的资源，并用apply()和free()方法来维护这个 List。每次调用transfer()时，循环等待所有资源都不在 List 中，并一次申请所有资源。 主动释放资源。synchronized采用阻塞等待的方式申请资源，如果想实现主动释放资源可以使用java.util.concurrent 这个包 下面提供的Lock 按序申请。Accout 类中维护一个变量叫 id，每个对象都有自己的 id 序号。每次申请资源都比较两个对象的 id 大小，每次申请资源时，按照先大后小，或者先小后大的固定顺序。这样就保证先申请资源 A，再申请资源 B，不会出现先申请资源 B 再申请资源 A 的情况发生。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"}]},{"title":"【Java并发编程】Java内存模型","slug":"Java/Java 并发编程/【Java并发编程】Java内存模型","date":"2019-09-07T10:28:41.000Z","updated":"2019-09-11T14:59:32.601Z","comments":true,"path":"post/f7a60bf6.html","link":"","permalink":"http://shuiyujie.com/post/f7a60bf6.html","excerpt":"我们知道 CPU 与内存、IO 设备之间的读写速度存在巨大差距，短板理论也告诉我们计算机的性能取决于其性能最差的部分。 为了使内存和 IO 设备能够充分利用 CPU 的性能，同时也要兼顾设备的成本，操作系统和编译器为了做了许多优化，主要有 3 个方面： CPU 增加缓存来平衡与内存之间速度的差异 操作系统使用进程与线程，通过分时复用 CPU 来平衡 CPU 和 IO 设备速度的差异 编译器通过优化指令执行次序，来合理利用 CPU 的缓存 当然，万事万物有利有弊，正式由于这些优化手段，也引入了并发编程的核心问题，即： 缓存引起的可见性问题 线程切换引起的原子性问题 编译优化引起的有序性问题 以上就是并发编程的 3 大核心问题。对于Java并发编程这个话题，我们主要研究的就是怎么解决这 3 个问题。 解决的思路说来也不难，就是按需禁用缓存和编译优化。在我们需要的时候，程序员指定程序禁用缓存和编译优化就可以提升程序的性能。 这就是我们本文的重点Java内存模型。 Java 内存模型规范了 JVM 实现按需禁用缓存和编译优化的方法。其主要包括volatile、synchronized 和 final 三个关键字，以及 6 项 Happens-Before 规则。","text":"我们知道 CPU 与内存、IO 设备之间的读写速度存在巨大差距，短板理论也告诉我们计算机的性能取决于其性能最差的部分。 为了使内存和 IO 设备能够充分利用 CPU 的性能，同时也要兼顾设备的成本，操作系统和编译器为了做了许多优化，主要有 3 个方面： CPU 增加缓存来平衡与内存之间速度的差异 操作系统使用进程与线程，通过分时复用 CPU 来平衡 CPU 和 IO 设备速度的差异 编译器通过优化指令执行次序，来合理利用 CPU 的缓存 当然，万事万物有利有弊，正式由于这些优化手段，也引入了并发编程的核心问题，即： 缓存引起的可见性问题 线程切换引起的原子性问题 编译优化引起的有序性问题 以上就是并发编程的 3 大核心问题。对于Java并发编程这个话题，我们主要研究的就是怎么解决这 3 个问题。 解决的思路说来也不难，就是按需禁用缓存和编译优化。在我们需要的时候，程序员指定程序禁用缓存和编译优化就可以提升程序的性能。 这就是我们本文的重点Java内存模型。 Java 内存模型规范了 JVM 实现按需禁用缓存和编译优化的方法。其主要包括volatile、synchronized 和 final 三个关键字，以及 6 项 Happens-Before 规则。 Happens-BeforeHappens-Before 是一种规则，它要求 JVM 按照这种规则来禁用内存和编译优化。简单来说 Happens-Before 规定了前一个操作结果对后一个操作是可见的。 举例来说现在有事件 A 和事件 B，且 A happens-before B，那就意味着事件 A 对于事件 B 是可见的，无论事件 A 和事件 B 发生在同一个线程还是不同线程，即使它们分别在两个不同的线程上发生，happens-before都会保证事件 A 对于事件 B 是可见的。具体包括以下 6 条规则： 程序的顺序性规则 volatile 变量规则 传递性规则 管程中锁的规则 线程的 start() 规则 线程的 join() 规则 程序的顺序性规则1234567891011121314public class VolatileExample &#123; int x = 0; volatile boolean v = false; public void write()&#123; x = 42; v = true; &#125; public void read()&#123; if(v)&#123; System.out.println(x); &#125; &#125;&#125; 程序的顺序性规则即，前面的操作 happens-before 后面的操作，比如 x=10 对于 v=true 是可见的。 volatile 规则volatile 在告诉编译器，对于这个变量的读写，不能使用 CPU 缓存，而是直接从内存执行读写操作。 volatile 规则规定，对于一个 volatile 变量的写操作，happens-before 于对这个变量的读操作，也就是上面这段代码的write()对于read()是可见的。 这条可以结合下面这条传递性规则来理解。 传递性规则如果 A happens-before B，B happens-before A，则 A happens-before C。这条也比较符合直觉，结合 volatile 规则来看一下。 从图中可以看出： 根据规则1，x=42 happens-before v=true 根据规则2，写变量 happens-before 读变量 再结合传递性规则，可得：x=42 happens-before 读变量v=true，这就意味着线程 B 可以看到x==42。 而在 JDK.5 之前，x 的值可能是 0，也可能是 42。因为共享变量 x，可能会缓存在 CPU 中。JDK1.5 对volatile 语义的增强，确保了线程 B 读取到的 x 肯定是 42。 管程中的锁的规则这条规则是指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 123456synchronized(this) &#123; // 此处自动加锁 // x 是共享变量，初始值 = 10 if(this.x &lt; 12) &#123; this.x = 12; &#125;&#125;// 此处自动加锁 synchronized 是 Java 对于管程的实现，简单来说当代码进入同步块就会自动加锁，同步块执行完成就会释放锁。 将其与规则4结合是这样的：线程 A 执行代码块将 x 的值从 10 修改为 12；之后线程 B 进入代码块能够看到线程 A 对于 x 的写操作。 线程的 start() 规则主线程 A 启动子线程 B 之后，子线程 B 能够看到主线程在启动子线程 B 前的操作。具体来说，主线程调用子线程的start()方法前的操作，子线程都能看到。 线程的 join() 规则这条关于线程等待，在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"}]},{"title":"【Java并发编程】Java 中如何创建线程？","slug":"Java/Java 并发编程/【Java并发编程】Java-中如何创建线程？","date":"2019-09-05T14:21:53.000Z","updated":"2019-09-11T14:59:29.157Z","comments":true,"path":"post/714434fb.html","link":"","permalink":"http://shuiyujie.com/post/714434fb.html","excerpt":"继承Thread实现线程创建 实现Runnable接口 实现Callable接口，结合 FutureTask使用 利用该线程池ExecutorService、Callable、Future来实现","text":"继承Thread实现线程创建 实现Runnable接口 实现Callable接口，结合 FutureTask使用 利用该线程池ExecutorService、Callable、Future来实现 继承 Thread 类123456789101112131415161718192021222324252627282930313233343536public class AddThread extends Thread &#123; private int start, end; private int sum = 0; public AddThread(String name, int start, int end) &#123; super(name); this.start = start; this.end = end; &#125; public void run() &#123; System.out.println(\"Thread-\" + getName() + \" 开始执行!\"); for (int i = start; i &lt;= end; i ++) &#123; sum += i; &#125; System.out.println(\"Thread-\" + getName() + \" 执行完毕! sum=\" + sum); &#125; public static void main(String[] args) throws InterruptedException &#123; int start = 0, mid = 5, end = 10; AddThread thread1 = new AddThread(\"线程1\", start, mid); AddThread thread2 = new AddThread(\"线程2\", mid + 1, end); thread1.start(); thread2.start(); // 确保两个线程执行完毕 thread1.join(); thread2.join(); int sum = thread1.sum + thread2.sum; System.out.println(\"sum: \" + sum); &#125;&#125; 输出结果 12345Thread-线程1 开始执行!Thread-线程1 执行完毕! sum=15Thread-线程2 开始执行!Thread-线程2 执行完毕! sum=40sum: 55 实现Runnable接口12345678910111213141516171819202122232425262728293031323334public class AddRun implements Runnable &#123; private int start, end; private int sum = 0; public AddRun(int start, int end) &#123; this.start = start; this.end = end; &#125; public void run() &#123; System.out.println(Thread.currentThread().getName() + \" 开始执行!\"); for(int i = start; i &lt;= end; i++) &#123; sum += i; &#125; System.out.println(Thread.currentThread().getName() + \" 执行完毕! sum=\" + sum); &#125; public static void main(String[] args) throws InterruptedException &#123; int start = 0, mid = 5, end = 10; AddRun run1 = new AddRun(start, mid); AddRun run2 = new AddRun(mid + 1, end); Thread thread1 = new Thread(run1, \"线程1\"); Thread thread2 = new Thread(run2, \"线程2\"); thread1.start(); thread2.start(); thread1.join(); thread2.join(); int sum = run1.sum + run2.sum; System.out.println(\"sum: \" + sum); &#125;&#125; 实现Callable接口，结合FutureTask创建线程12345678910111213141516171819202122232425262728293031323334353637383940public class AddCall implements Callable&lt;Integer&gt; &#123; private int start, end; private int sum = 0; public AddCall(int start, int end) &#123; this.start = start; this.end = end; &#125; public Integer call() throws Exception &#123; int sum = 0; System.out.println(Thread.currentThread().getName() + \" 开始执行!\"); for (int i = start; i &lt;= end; i++) &#123; sum += i; &#125; System.out.println(Thread.currentThread().getName() + \" 执行完毕! sum=\" + sum); return sum; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; int start = 0, mid = 5, end = 10; FutureTask&lt;Integer&gt; future1 = new FutureTask&lt;Integer&gt;(new AddCall(start, mid)); FutureTask&lt;Integer&gt; future2 = new FutureTask&lt;Integer&gt;(new AddCall(mid + 1, end)); Thread thread1 = new Thread(future1, \"线程1\"); Thread thread2 = new Thread(future2, \"线程2\"); thread1.start(); thread2.start(); int sum1 = future1.get(); int sum2 = future2.get(); System.out.println(\"sum = \" + (sum1 + sum2)); &#125;&#125; 线程池方式创建12345678910111213141516171819202122232425262728293031public class AddPool implements Callable&lt;Integer&gt; &#123; private int start, end; public AddPool(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override public Integer call() throws Exception &#123; int sum = 0; System.out.println(Thread.currentThread().getName() + \" 开始执行!\"); for (int i = start; i &lt;= end; i++) &#123; sum += i; &#125; System.out.println(Thread.currentThread().getName() + \" 执行完毕! sum=\" + sum); return sum; &#125; public static void main(String[] arg) throws ExecutionException, InterruptedException &#123; int start=0, mid=500, end=1000; ExecutorService executorService = Executors.newFixedThreadPool(2); Future&lt;Integer&gt; future1 = executorService.submit(new AddPool(start, mid)); Future&lt;Integer&gt; future2 = executorService.submit(new AddPool(mid+1, end)); int sum = future1.get() + future2.get(); System.out.println(\"sum: \" + sum); &#125;&#125; 参考资料Java并发学习之四种线程创建方式的实现与对比","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"}]},{"title":"【TensorFlow2.0】实现ResNet","slug":"Deeplearning/【TensorFlow2-0】实现ResNet","date":"2019-09-05T14:05:06.000Z","updated":"2019-09-30T14:48:45.234Z","comments":true,"path":"post/3a297e7b.html","link":"","permalink":"http://shuiyujie.com/post/3a297e7b.html","excerpt":"深度残差网络（Deep residual network, ResNet）的提出是CNN图像史上的一件里程碑事件，ResNet取得了5项第一，并又一次刷新了CNN模型在ImageNet上的历史。 本文介绍如何用 TensorFlow2.0 来实现 ResNet18，并用其训练 cifar100 的分类模型。","text":"深度残差网络（Deep residual network, ResNet）的提出是CNN图像史上的一件里程碑事件，ResNet取得了5项第一，并又一次刷新了CNN模型在ImageNet上的历史。 本文介绍如何用 TensorFlow2.0 来实现 ResNet18，并用其训练 cifar100 的分类模型。 ResNet 的实现 ResNet 的基本结构如上图所示，为了解决网络层次过深而导致的退化问题（Degradation problem）。ResNet 设计了一种短路连接（shortcut connection）来解决这个问题。 在编程实现的角度来看，首先我们要建一个 BasicBlock ，它包括两个 layer，以及一个 shortcut connection。ResNet 则是由多个 BasicBlock 堆叠而成。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layers, Sequentialclass BasicBlock(layers.Layer): def __init__(self, filter_num, stride=1): super(BasicBlock, self).__init__() self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same') self.bn1 = layers.BatchNormalization() self.relu = layers.Activation('relu') self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same') self.bn2 = layers.BatchNormalization() if stride != 1: self.downsample = Sequential() self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride)) else: self.downsample = lambda x:x def call(self, inputs, training=None): # [b, h, w, c] out = self.conv1(inputs) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) identity = self.downsample(inputs) output = layers.add([out, identity]) output = tf.nn.relu(output) return outputclass ResNet(keras.Model): def __init__(self, layer_dims, num_classes=100): # [2, 2, 2, 2] super(ResNet, self).__init__() self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)), layers.BatchNormalization(), layers.Activation('relu'), layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same') ]) self.layer1 = self.build_resblock(64, layer_dims[0]) self.layer2 = self.build_resblock(128, layer_dims[1], stride=2) self.layer3 = self.build_resblock(256, layer_dims[2], stride=2) self.layer4 = self.build_resblock(512, layer_dims[3], stride=2) # output: [b, 512, h, w], self.avgpool = layers.GlobalAveragePooling2D() self.fc = layers.Dense(num_classes) def call(self, inputs, training=None): x = self.stem(inputs) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) # [b, c] x = self.avgpool(x) # [b, 100] x = self.fc(x) return x def build_resblock(self, filter_num, blocks, stride=1): res_blocks = Sequential() # may down sample res_blocks.add(BasicBlock(filter_num, stride)) for _ in range(1, blocks): res_blocks.add(BasicBlock(filter_num, stride=1)) return res_blocksdef resnet18(): return ResNet([2, 2, 2, 2])def resnet34(): return ResNet([3, 4, 6, 3]) 使用 ResNetResNet 的使用和一般的模型使用没有什么区别，如果不清楚可以看这篇文章：【TensorFlow2.0】手撕前向传播算法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import tensorflow as tffrom tensorflow.keras import layers, optimizers, datasets, Sequentialimport osfrom resnet1 import resnet18os.environ['TF_CPP_MIN_LOG_LEVEL']='2'tf.random.set_seed(2345)def preprocess(x, y): # [-1~1] x = tf.cast(x, dtype=tf.float32) / 255. - 0.5 y = tf.cast(y, dtype=tf.int32) return x,y(x,y), (x_test, y_test) = datasets.cifar100.load_data()y = tf.squeeze(y, axis=1)y_test = tf.squeeze(y_test, axis=1)print(x.shape, y.shape, x_test.shape, y_test.shape)train_db = tf.data.Dataset.from_tensor_slices((x,y))train_db = train_db.shuffle(1000).map(preprocess).batch(512)test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))test_db = test_db.map(preprocess).batch(512)sample = next(iter(train_db))print('sample:', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))def main(): # [b, 32, 32, 3] =&gt; [b, 1, 1, 512] model = resnet18() model.build(input_shape=(None, 32, 32, 3)) model.summary() optimizer = optimizers.Adam(lr=1e-3) for epoch in range(500): for step, (x,y) in enumerate(train_db): with tf.GradientTape() as tape: # [b, 32, 32, 3] =&gt; [b, 100] logits = model(x) # [b] =&gt; [b, 100] y_onehot = tf.one_hot(y, depth=100) # compute loss loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True) loss = tf.reduce_mean(loss) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) if step %50 == 0: print(epoch, step, 'loss:', float(loss)) total_num = 0 total_correct = 0 for x,y in test_db: logits = model(x) prob = tf.nn.softmax(logits, axis=1) pred = tf.argmax(prob, axis=1) pred = tf.cast(pred, dtype=tf.int32) correct = tf.cast(tf.equal(pred, y), dtype=tf.int32) correct = tf.reduce_sum(correct) total_num += x.shape[0] total_correct += int(correct) acc = total_correct / total_num print(epoch, 'acc:', acc)if __name__ == '__main__': main()","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://shuiyujie.com/tags/tensorflow/"}]},{"title":"【TensorFlow2.0】手撕前向传播算法","slug":"Deeplearning/【TensorFlow2.0】手撕前向传播算法","date":"2019-09-05T13:55:06.000Z","updated":"2019-09-30T14:48:01.607Z","comments":true,"path":"post/c6ed1613.html","link":"","permalink":"http://shuiyujie.com/post/c6ed1613.html","excerpt":"本文介绍如何使用 TensorFlow2.0 实现前向传播,先介绍用 TensorFlow 普通的 API 来实现前向传播,将会介绍:如何加载数据集,如何完成参数初始化和构建前向传播网络,如何计算 accuracy. 由于神经网络的训练流程大同小异,就可以使用 tf.keras 封装的 API 来简化模型训练和测试的流程.本文第二部分将会介绍如何使用 tf.keras 来定义神经网络以及优化器,如何用tf.keras.metrics来计算 accuracy 和 loss.","text":"本文介绍如何使用 TensorFlow2.0 实现前向传播,先介绍用 TensorFlow 普通的 API 来实现前向传播,将会介绍:如何加载数据集,如何完成参数初始化和构建前向传播网络,如何计算 accuracy. 由于神经网络的训练流程大同小异,就可以使用 tf.keras 封装的 API 来简化模型训练和测试的流程.本文第二部分将会介绍如何使用 tf.keras 来定义神经网络以及优化器,如何用tf.keras.metrics来计算 accuracy 和 loss. 安装 TensorFlow2.0本文的开发环境是 Ubuntu16.04 + CUDA10 + Anaconda + TensorFlow2.0，目前 conda 不支持安装 TensorFlow2.0 的包，所以需要用 pip 来安装 TensorFlow2.0。 1pip install tensorflow-gpu==2.0.0-rc0 numpy matplotlib pandas -i https://pypi.tuna.tsinghua.edu.cn/simple 截止到现在，TensorFlow 发布了最新的 rc 版本，与以后的正式发行版差别不大了。 完成一次前向传播 如何加载数据集 第1步: 使用 keras.datasets 加载 mnist 数据集,将会返回两组数据 第2步: 将 Numpy 类型的数据转换为 tensor 第3步: 将 tensor 转换为 datasets 123456789101112131415161718192021222324252627282930313233343536373839# 需要引入的包import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import datasetsimport os# 设置 TensorFlow 的日志级别,避免输出过多提示信息os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'# 第1步: 使用 keras.datasets 加载 mnist 数据集,将会返回两组数据# x: [60000,28,28], x_test:[10000,28,28]# y: [60000], y_test:[10000](x, y), (x_text, y_text) = datasets.mnist.load_data()# 第2步: 将 Numpy 类型的数据转换为 tensor# keras.datasets 加载到的是 Numpy 类型的数据,将其转换为 tensor# convert to tensor# x:[0~255] =&gt; [0~1]x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.y = tf.convert_to_tensor(y, dtype=tf.int32)x_text = tf.convert_to_tensor(x_text, dtype=tf.float32) / 255.y_text = tf.convert_to_tensor(y_text, dtype=tf.int32)# 查看数据分布情况,shape,dype,min,maxprint(x.shape, y.shape, x.dtype, y.dtype)print(tf.reduce_min(x), tf.reduce_max(y))print(tf.reduce_min(y), tf.reduce_max(y))# 第3步: 将 tensor 转换为 datasets# tensorflow 推荐使用 tf.data.Dataset 来加载数据集# tensor =&gt; datasets# 128个为一个batch返回datasetstrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)test_db = tf.data.Dataset.from_tensor_slices((x_text,y_text)).batch(128)train_iter = iter(train_db)sample = next(train_iter)print(\"batch:\", sample[0].shape, sample[1].shape) 前向传播参数初始化前面创建的 train_db 是 shape = [128, 28, 28],表示 128 张 28x28 的灰度图像.在输入的时候我将其展开成 shape = [128, 28*28],接着用全连接层降维到 shape = 10 的 tensor,对应 mnist 共 10 类的标签. 12345678# [b, 784] =&gt; [b, 256] =&gt; [b, 128] =&gt; [b, 10]# w:[dim_in, dim_out]; b:[dim_out]w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))b1 = tf.Variable(tf.zeros([256]))w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))b2 = tf.Variable(tf.zeros([128]))w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))b3 = tf.Variable(tf.zeros([10])) 前向传播 &amp; 自动求导1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 学习率lr = 1e-3# train_db 将会被计算 10 次for epoch in range(10): # iterate db for 10 # 遍历 train_db for step, (x, y) in enumerate(train_db, 1): # x:[128,28,28] # y:[128] # 将28x28的张量展开成784 # x:[128,28,28] =&gt; [128,784] x = tf.reshape(x, [-1,28*28]) # 使用 tf.GradientTape() 自动求导 with tf.GradientTape() as tape: # 构建前向传播的网络 # x:[b,28*28] # h1 = x@w1+b1 # [b,784]@[784,256]+[256] =&gt; [b,256] + [256] =&gt; [] h1 = tf.nn.relu(x@w1 + b1) h2 = tf.nn.relu(h1@w2 + b2) out = h2@w3 + b3 # y 使用 one_hot 编码,与神经网络的输出对应 # y: [b] =&gt; [b, 10] y_onehot = tf.one_hot(y, depth=10) # 计算均方误差 mse = mean(sum(y-out)^2) # loss:[b,10] loss = tf.square(y_onehot-out) # 求误差的请平均值 # loss:[b,10] =&gt; scalar loss = tf.reduce_mean(loss) # 借助于 tensorflow 自动求导 grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3]) # 根据梯度更新参数 # w1 = w1 - lr * w1_grad w1.assign_sub(lr * grads[0]) b1.assign_sub(lr * grads[1]) w2.assign_sub(lr * grads[2]) b2.assign_sub(lr * grads[3]) w3.assign_sub(lr * grads[4]) b3.assign_sub(lr * grads[5]) # 每迭代100次输出一次loss if step % 100 == 0: print(epoch+1, (epoch+1)*step, 'loss:', float(loss)) 计算准确率 accuracy我们需要通过模型的准确率来评估模型,模型评估需要在 test_db 上进行. 12345678910111213141516171819202122232425262728293031323334total_number = 0 total_correct = 0 # 遍历 test_db for step, (x, y) in enumerate(test_db): # [b, 28, 28] =&gt; [b, 28*28] x = tf.reshape(x,[-1,28*28]) # 利用最新的参数完成一次前向传播 # [b, 784] =&gt; [b, 256] =&gt; [b, 128] =&gt; [b, 10] h1 = tf.nn.relu(x@w1 + b1) h2 = tf.nn.relu(h1@w2 + b2) out = h2@w3 + b3 # 使用 softmax() 输出每个分类的概率值 # [b, 10] ~ R # [b, 10] ~ [0,1] prob = tf.nn.softmax(out, axis=1) # 概率最大的值就是模型的预测值 # [b, 10] =&gt; [b] preb = tf.argmax(prob, axis=1) preb = tf.cast(preb, dtype=tf.int32) # 预测值与真实值比较 # [b] int32 # print(y.dtype, preb.dtype) correct = tf.cast(tf.equal(y, preb), dtype=tf.int32) correct = tf.reduce_sum(correct) total_correct += int(correct) total_number += x.shape[0] acc = total_correct / total_number print(\"accuracy:\", acc) 前向传播过程优化 构建数据集时添加数据预处理Tensorflow 鼓励使用 tf.data.Dataset加载数据集,它给我封装了许多处理数据集的方法,比如train_db = train_db.map(preprocess).shuffle(60000).batch(128) 在preprocess(x,y)中进行数据预处理,利用map(preprocess)调用预处理函数,再用shuffle()随机成对打乱数据集,最后用batch()将数据集按照 128 一份来分隔. 12345678910111213141516def preprocess(x, y): # 数据预处理,归一化,类型转换 x = tf.cast(x, dtype=tf.float32) / 255. y = tf.cast(y, dtype=tf.int32) return x,y# x: [60000,28,28], x_test:[10000,28,28]# y: [60000], y_test:[10000](x, y), (x_text, y_text) = datasets.mnist.load_data()# tensor =&gt; datasetstrain_db = tf.data.Dataset.from_tensor_slices((x,y))train_db = train_db.map(preprocess).shuffle(60000).batch(128)test_db = tf.data.Dataset.from_tensor_slices((x_text,y_text))test_db = test_db.map(preprocess).shuffle(10000).batch(128) 使用 tf.keras 来构建模型TensorFlow2.0 降低了使用者的门槛,利用 tf.keras 可以直接使用 Keras 的一系列 API,这样就可以更加方便地定义模型,比如说前面定义参数 [w,b] 的过程就可以简化成下面这样 12345678910111213# 用 Sequential 构建 3 层的全连接network = Sequential([layers.Dense(256, activation='relu'), layers.Dense(128, activation='relu'), layers.Dense(10) ])# build 并制定 inputnetwork.build(input_shape=[None, 28*28])# 查看模型参数network.summary()# 指定优化器optimizer = optimizers.Adam(lr=0.01) 使用 metrics 自动计算 loss 和 accuracy12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# Step1.Build a meteracc_meter = metrics.Accuracy()loss_meter = metrics.Mean()for epoch in range(10): # iterate db for 10 for step, (x, y) in enumerate(train_db, 1): x = tf.reshape(x, [-1,28*28]) with tf.GradientTape() as tape: # [b,784] =&gt; [b,10] out = network(x) y_onehot = tf.one_hot(y, depth=10) loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True)) # Step2.Update data loss_meter.update_state(loss) # compute gradients # grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3]) grads = tape.gradient(loss, network.trainable_variables) optimizer.apply_gradients(zip(grads, network.trainable_variables)) if step % 100 == 0: # Step3.Get Average data print(step, 'loss:', loss_meter.result().numpy()) # Clear buffer loss_meter.reset_states() total_number = 0 total_correct = 0 for step, (x, y) in enumerate(test_db): acc_meter.reset_states() # [b, 28, 28] =&gt; [b, 28*28] x = tf.reshape(x,[-1,28*28]) # [b, 784] =&gt; [b, 256] =&gt; [b, 128] =&gt; [b, 10] # h1 = tf.nn.relu(x@w1 + b1) # h2 = tf.nn.relu(h1@w2 + b2) # out = h2@w3 + b3 # 输出调用 network 就可以获得 out = network(x) # [b, 10] ~ R # [b, 10] ~ [0,1] prob = tf.nn.softmax(out, axis=1) # [b, 10] =&gt; [b] pred = tf.argmax(prob, axis=1) pred = tf.cast(pred, dtype=tf.int32) # [b] int32 # print(y.dtype, preb.dtype) # correct = tf.cast(tf.equal(y, pred), dtype=tf.int32) # correct = tf.reduce_sum(correct) # total_correct += int(correct) # total_number += x.shape[0] # metrics 能快速计算 acc 和 loss acc_meter.update_state(y, pred) # acc = total_correct / total_number print(step, 'Evaluate Acc:', acc_meter.result().numpy()) tf.keras 版12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import datasets, layers, optimizers, Sequential, metricsimport osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'def preprocess(x, y): \"\"\" x is a simple image, not a batch \"\"\" x = tf.cast(x, dtype=tf.float32) / 255. x = tf.reshape(x, [28*28]) y = tf.cast(y, dtype=tf.int32) y = tf.one_hot(y, depth=10) return x,y# x: [60000,28,28], x_test:[10000,28,28]# y: [60000], y_test:[10000](x, y), (x_text, y_text) = datasets.mnist.load_data()# tensor =&gt; datasetstrain_db = tf.data.Dataset.from_tensor_slices((x,y))train_db = train_db.map(preprocess).shuffle(60000).batch(128)test_db = tf.data.Dataset.from_tensor_slices((x_text,y_text))test_db = test_db.map(preprocess).shuffle(10000).batch(128)# 使用 tf.Sequential 定义上面的这些参数network = Sequential([layers.Dense(256, activation='relu'), layers.Dense(128, activation='relu'), layers.Dense(10) ])network.build(input_shape=(None, 28*28))network.summary()network.compile(optimizer=optimizers.Adam(lr=0.01), loss=tf.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])network.fit(train_db, epochs=10, validation_data=test_db, validation_freq=2)network.evaluate(test_db)sample = next(iter(ds_val))x = sample[0]y = sample[1] # one-hotpred = network.predict(x) # [b, 10]# convert back to numbery = tf.argmax(y, axis=1)pred = tf.argmax(pred, axis=1)print(pred)print(y)","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://shuiyujie.com/tags/tensorflow/"}]},{"title":"【TensorFlow2.0】TensorFlow的安装","slug":"Deeplearning/【TensorFlow2-0】TensorFlow的安装","date":"2019-09-05T13:50:06.000Z","updated":"2019-09-22T14:46:42.639Z","comments":true,"path":"post/7b2629fd.html","link":"","permalink":"http://shuiyujie.com/post/7b2629fd.html","excerpt":"本文的环境是 Ubuntu16.04 + CUDA10 + Anaconda + TensorFlow2.0，现在的 conda 不支持安装 TensorFlow2.0 的包，所以需要用 pip 来安装 TensorFlow2.0。 1pip install tensorflow-gpu==2.0.0-rc0 numpy matplotlib pandas -i https://pypi.tuna.tsinghua.edu.cn/simple 截止到现在，TensorFlow 发布了最新的 rc 版本，与以后的正式发行版差别不大了。","text":"本文的环境是 Ubuntu16.04 + CUDA10 + Anaconda + TensorFlow2.0，现在的 conda 不支持安装 TensorFlow2.0 的包，所以需要用 pip 来安装 TensorFlow2.0。 1pip install tensorflow-gpu==2.0.0-rc0 numpy matplotlib pandas -i https://pypi.tuna.tsinghua.edu.cn/simple 截止到现在，TensorFlow 发布了最新的 rc 版本，与以后的正式发行版差别不大了。 以下是 TensorFlow1.0 的安装方法：","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://shuiyujie.com/tags/tensorflow/"}]},{"title":"【TensorFlow2.0】TensorFlow简介","slug":"Deeplearning/【TensorFlow2.0】TensorFlow简介","date":"2019-09-05T13:45:06.000Z","updated":"2019-09-30T14:48:39.259Z","comments":true,"path":"post/b5bae26d.html","link":"","permalink":"http://shuiyujie.com/post/b5bae26d.html","excerpt":"","text":"TensorFlow 里程碑 2015.9发布0.1版本 2017.2发布1.0版本 2019春发布2.0版本 TensorFlow vs PytorchTensorFlow1.0 上手困难，经常被诟病，都说 Pytorch。现在 TensorFlow2.0 出了，就容易上手多了。 学习建议就是 忘掉 TensorFlow1.x PyTorch和TensorFlow选择一个主修 两者都要掌握 Keras逐渐淡出 TF+Keras PyTorch+Caffe2 TensorFlow 的优点GPU 加速用 CPU 和 GPU 分别测试一下运算速度 12345678910111213141516171819202122232425262728import tensorflow as tfimport timeitwith tf.device('/cpu:0'): cpu_a = tf.random.normal([10000, 1000]) cpu_b = tf.random.normal([1000, 2000]) print(cpu_a.device, cpu_b.device)with tf.device('/gpu:0'): gpu_a = tf.random.normal([10000, 1000]) gpu_b = tf.random.normal([1000, 2000]) print(gpu_a.device, gpu_b.device)def cpu_run(): with tf.device('/cpu:0'): c = tf.matmul(cpu_a, cpu_b) return cdef gpu_run(): with tf.device('/gpu:0'): c = tf.matmul(gpu_a, gpu_b) return c# warm upcpu_time = timeit.timeit(cpu_run, number=10)gpu_time = timeit.timeit(gpu_run, number=10)print('warmup:', cpu_time, gpu_time)cpu_time = timeit.timeit(cpu_run, number=10)gpu_time = timeit.timeit(gpu_run, number=10)print('run time:', cpu_time, gpu_time) 自动求导12345678910import tensorflow as tfx = tf.constant(1.)a = tf.constant(2.)b = tf.constant(3.)c = tf.constant(4.)with tf.GradientTape() as tape: tape.watch([a, b, c]) y = a**2 * x + b * x + c[dy_da, dy_db, dy_dc] = tape.gradient(y, [a, b, c])print(dy_da, dy_db, dy_dc) 神经网络 API TensorFlow 社区TensorFlow 社区地址 TensorFlow Github 地址 TensorFlow 生态 - TFX TFX - 基于 TensorFlow 的端到端机器学习平台 TFX Github 地址 TensorFlow 生态 - Kubeflow Kubeflow 表示 K8s + Data flow 一个典型的 AI 工作流程是怎么样的？可以划分成 8 个步骤： 从客户获得产品需求 设计我们的产品 数据处理 训练模型 数据/模型可视化 将模型做成服务 模型验证：蓝绿测试、灰度测试 商业上取得成功 KubeFlow Github 地址 kubernetes Githube 地址","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://shuiyujie.com/tags/tensorflow/"}]},{"title":"chrome食用指南","slug":"善用佳软/chrome食用指南","date":"2019-08-31T00:58:49.000Z","updated":"2019-09-10T14:30:57.892Z","comments":true,"path":"post/1e186084.html","link":"","permalink":"http://shuiyujie.com/post/1e186084.html","excerpt":"Google Chrome 浏览器（后面简称 Chrome）是全球市场占有率最高的浏览器，全球范围内有超过 10 亿人在使用 Chrome。","text":"Google Chrome 浏览器（后面简称 Chrome）是全球市场占有率最高的浏览器，全球范围内有超过 10 亿人在使用 Chrome。 1、登录 Google 账号。登录 Google 账号之后可以同步书签，历史记录等。同步书签是非常便利的一个功能，这样即使换了一台电脑也能快速找到自己收藏的实用网站。 2、自定义搜索引擎。在设置(S)中可以选择默认的搜索引擎，建议换成 Google。在地址栏中输入想要搜索的内容，可以直接用 Google 搜索。 3、设置为默认浏览器。必须设置为默认浏览器呀！ Chrome 中各类好用的插件让它如虎添翼，介绍几个我最常用的插件。 印象笔记「裁藏」「裁藏」需要配合印象笔记来使用。它的作用是将网页中你感兴趣的部分保存到印象笔记中。 广告屏蔽插件 AdGuard屏蔽广告弹窗，侧边栏的小广告，保持浏览网页时的专注度。 脚本管理插 TampermonkeyTampermonkey 可以管理各类脚本，而脚本可以辅助我们做更多事情。比如，百度云盘高速下载、豆瓣搜索电影的时候显示资源链接等。 可以搜索油猴脚本了解更多内容。 Explain and Send Screenshots可以滚动将整个网页的内容生成一张长图。 防追踪插件 GHOSTERY打开网页之后会有许多后台程序开始追踪你的个人信息，ghostery会告诉你哪些人在追踪你的信息，并帮助你屏蔽他们。","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://shuiyujie.com/tags/网站/"},{"name":"工具","slug":"工具","permalink":"http://shuiyujie.com/tags/工具/"}]},{"title":"快，学会shell","slug":"Linux/快，学会shell","date":"2019-07-21T14:22:05.000Z","updated":"2019-08-31T04:54:22.336Z","comments":true,"path":"post/92782b0b.html","link":"","permalink":"http://shuiyujie.com/post/92782b0b.html","excerpt":"题图：https://pixabay.com/illustrations/hacker-cyber-crime-internet-2300772/ 本文分成入门篇和基础篇。基础篇包括变量、字符串处理、数学运算三部分。基础篇包括流控制、函数和函数库三部分。主要是基于例子进行讲解，其中有 4 个复杂一点的脚本，看懂了也就入门了。","text":"题图：https://pixabay.com/illustrations/hacker-cyber-crime-internet-2300772/ 本文分成入门篇和基础篇。基础篇包括变量、字符串处理、数学运算三部分。基础篇包括流控制、函数和函数库三部分。主要是基于例子进行讲解，其中有 4 个复杂一点的脚本，看懂了也就入门了。 我们先来聊一聊 shell 和 shell script 的概念。计算机的运行离不开硬件，我们通过操作系统（OS，Operating System）操作硬件，而我们所说的 linux 严格来说是操作系统（OS）的核心部分——内核（Kernel）。我们无法直接操作 kernel，需要借助于 kernel 外的一层壳 shell 才能与 kernel 进行交互。如果把操作系统(OS)看做是一家公司，shell 就是前台，kernel 就是董事会。当我们访问公司的时候，先和前台(shell)打个招呼，前台通知董事会(kernel)，董事会来控制公司(OS)。 俗话说“铁打的营盘流水的兵”，就是公司人来人往，都不会影响公司的运转。对于操作系统也一样，我们可以替换操作系统的前台(shell)，甚至董事会(kernel)。如果你想知道你的系统中用到的是什么 shell 可以访问 /etc/shells 文件。,我的电脑上就有下面几种 shell 1234567# /etc/shells: valid login shells/bin/sh/bin/dash/bin/bash/bin/rbash/bin/zsh/usr/bin/zsh 入门篇12345#!/bin/bashfor ((i=0; i&lt;10; i++));do echo $&#123;i&#125;done 直接来看一个例子吧。创建一个名为 shell001.sh 的文件，写上上面几行代码： 第 1 行，指定 shell 的解释器。shell 脚本就和 python 或者 jsp 需要用解释器来解析，第 1 行就是用于指定解释器，也就是之前提到的 /etc/shells 下面列出来的。 第 2 行，循环语句，共循环 10 次，会在后面流控制章节讲解，加上 do 和 done 是 for 循环中的语法规则，for、do、done 是 shell script 中的关键字 第 4 行，打印 i 这个变量 那么怎么运行这个脚本呢？既然是运行我们既要给它赋予可执行权限chmod +x shell001.sh，接着用 ./shell001.sh 执行这个脚本。脚本运行起来将会在终端输出 0 到 9 这几个数字。 变量前面没有解释第 4 行echo ${i}。echo 是一个简单的 linux 命令，它会将输入从到标准输出(stdout)上，然后在终端中显示出来，这里显示的就是${i}这个变量的值。 在 shell 中定义变量的规则如下： 变量和等号之间不能有空格 变量名称由字母、数字和下划线组成 变量名称的第一个字符必须是字母或者下划线 变量名称中不允许空格和标点 比如说一个变量为name=&quot;shuiyj&quot;，那么使用变量就要加上$符号，打印这个变量就使用echo ${name}。此外变量除了显示地赋值，还可以使用语句给变量赋值: 12# 获取该文件夹下后缀为 jpg 结尾的列表for image in `ls *.jpg` 变量匹配我们会定义和使用一个变量了，接下来我会介绍几个使用的处理变量的方法。 现在一张图片的名字叫做 cat.jpg，我想要获取文件的名称，即 cat。当然这有很多的中方法，这里介绍一种实用的方法——变量匹配。 语法 说明 ${变量名#匹配规则} 从变量开头进行规则匹配，将符合最短的数据删除 ${变量名##匹配规则} 从变量开头进行规则匹配，将符合最长的数据删除 ${变量名%匹配规则} 从变量结尾进行规则匹配，将符合最短的数据删除 ${变量名%%匹配规则} 从变量结尾进行规则匹配，将符合最长的数据删除 ${变量名/旧字符串/新字符串} 变量中符合规则的第一个旧字符串将会被旧字符串代替 ${变量名//旧字符串/新字符串} 变量中符合规则的所有旧字符串将会被旧字符串代替 回到最开始的需求，就可以使用%来实现 123cat=\"cat.jpg\"echo $&#123;cat&#125; # cat.jpgecho $&#123;cat%.*&#125; # cat 首先定义一个变量 cat 并为其赋值，接着用$获取 cat 变量的值并打印出来，最后使用变量替换截取字符串。 变量匹配在 shell 中会被高频使用，要记住这些规则。 特殊变量的含义shell 中有一些特殊的变量，它们有很多实用的功能，比如说校验输入的参数，允许追加更多参数，判断上一条命令是否执行成功等。 变量 含义 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有参数。被双引号(&quot; &quot;)包含时，与 $*稍有不同，下面将会讲到。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。 比如说我们要对 shell 中的输入参数进行校验，可以添加这样一段 123456#!/bin/bashif [ $# != 2 ];then echo \"Usage: $0 &lt;change ID&gt; &lt;target ID&gt;\" exit -1fi... 其中$#表示输入的参数数量，我们通过条件判断在程序的输入参数不为 2 的时候将会进行提示，并退出程序。其中$0一般是可执行文件的名称。 字符串变量的话题就先讲到这里，接下来讲 shell 中处理字符串的一些注意事项和技巧。无论学习哪一门编程语言，字符串的处理都是一个绕不开的话题，并且在 shell 编程中用的最多的就是字符串。 单引号和双引号的区别字符串可以用单引号，也可以用双引号，还可以不用引号，我们要注意它们之间的区别。 123456# 单引号str='this is a string'# 双引号your_name='qinjx'str=\"Hello, I know your are \\\"$your_name\\\"! \\n\" 单引号 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的 单引号字串中不能出现单引号（对单引号使用转义符后也不行） 双引号 双引号里可以有变量 双引号里可以出现转义字符 想要了解更多它们之间的区别可以看这篇文章：shell十三问之4：””(双引号)与’’(单引号)差在哪？。 拼接字符串shell 中的字符串拼接只要直接连在一起就可以。 12345first_name=\"yujie\"last_name=\"shui\"greeting=\"hello,$&#123;first_name&#125; $&#123;last_name&#125;\"echo $&#123;greeting&#125; 计算字符串的长度shell 中有两种方法可以计算字符串长度。 123str=\"abcde\"echo $&#123;#str&#125; # 5expr length \"$&#123;str&#125;\" # 5 获取子串在字符串中的索引1expr index $&#123;string&#125; $&#123;substring&#125; 抽取子串 语法 说明 ${string:position} 从string中的position开始 ${string:position} 从position开始，并指定长度为length ${string:-position} 从右边开始匹配 ${string:(postion)} 从左边开始匹配 expr substr $string $position $length 从position开始，匹配长度为length 命令替换命令替换指的是 shell执行命令并将命令替换部分替换为执行该命令后的结果（先执行该命令，然后用结果代换到命令行中），共有有两种实现命令替换的方式： 1234# 方法一`command`# 方法二$(command) 还记得之前讲变量的时候提到变量既可以直接获取，也可以从语句获取么？ 12# 获取该文件夹下后缀为 jpg 结尾的列表for image in `ls *.jpg` 可以看到 Image 获取到的是ls *.jpg的返回值，也就是一个文件的列表。这里就用到命令替换的符号即两个反引号。 再比如获取系统的所有用户并输出 123456index=1 for user in `cat /etc/passwd | cut -d \":\" -f 1` do echo \"This is $&#123;index&#125; user: $&#123;user&#125;\" index = $(($index + 1)) done cat /etc/passwd | cut -d &quot;:&quot; -f 1将会截取用户名，由于使用了命令替换，其执行结果会返回给 user 变量，此时的 user 就是一个包含用户名称的列表。 最后再举一个使用$()的例子，比如获取系统时间计算今年或明年 12echo \"This is $(date +%Y) year\" echo \"This is $(($(date +%Y) + 1)) year\" 数学运算shell 中就两种变量，字符串和数字，数字又要按照整型和浮点型分开进行处理，处理它们的函数是不同的。整型运算需要使用expr $num1 operator $num2或者$(($num1 operator $num2))，浮点型运算则需要使用bc。 任然是通过案例的方式进行说明，假设现在有这样一个需求：提示用户输入一个正整数 num，然后计算 1+2+3+…+num 的值，并且必须对 num 是否为正整数做判断，不符合应该允许再次输入。 123456789101112131415161718#!/bin/bash#while truedo read -p \"please input a positive number: \" num expr $num + 1 &amp;&gt; /dev/null if [ $? -eq 0 ];then if [ `expr $num \\&gt; 0` -eq 1 ];then for((i=1;i&lt;=$num;i++)) do sum=`expr $sum + $i` done echo \"1+2+3+....+$num = $sum\" exit fi fi echo \"error,input enlegal\"done 这个脚本优点复杂但是不用着急，我们先关注于数学运算expr $num + 1 这一部分，其中关于if判断的部分会在下一节讲解。 expr $num + 1 意思就是做一次整数运算，将 num 和 1 相加。做这个操作的目的是判断 num 是不是一个整数，因为 expr 只能应用在整数运算上，所以执行expr $num + 1之后，如果 num 是整数退出状态就是正常的$? = 0，否则 $? ≠ 0 ，并且我们并不需要返回结果，可以将结果重定向到/dev/null中，即expr $num + 1 &amp;&gt; /dev/null。 注：在特殊变量的含义这一节可以了解$?的含义。退出状态指的是命令执行完毕之后像操作系统返回的值，成功则为 0。 expr 支持整数运算，不支持浮点数运算，要做浮点数运算那就要用到 bc。bc 是 bash 内建的运算器，支持浮点数运算，使用方法如下所示： 1234echo \"23.3+30\" | bc53.3echo \"scale=4;23.3/3.2\" | bc7.2812 基础篇在前面的入门篇，我们了解了变量、字符串和数学运算，接下来我将会介绍 shell 中流程控制的语法规则，以及 shell 中如何使用函数以及函数库。当我们掌握以上这些内容，shell 就可以算是入门了，那么就一起开始吧。 流控制流控制就是用判断语句，循环语句来控制程序执行的逻辑，就从我们在上一节数学运算中的那个脚本讲起吧，它既包含了if又包含了while循环，是一个很好的例子。 IF 控制语句123456789101112131415161718#!/bin/bash#while truedo read -p \"please input a positive number: \" num expr $num + 1 &amp;&gt; /dev/null if [ $? -eq 0 ];then if [ `expr $num \\&gt; 0` -eq 1 ];then for((i=1;i&lt;=$num;i++)) do sum=`expr $sum + $i` done echo \"1+2+3+....+$num = $sum\" exit fi fi echo \"error,input enlegal\"done 上一节中的脚本中expr $num + 1 &amp;&gt; /dev/null是关于数学运算的部分，紧跟着的if就是一个控制语句，我们抛开无关部分，开看一下关于if的骨架 12345expr $num + 1 &amp;&gt; /dev/nullif [ $? -eq 0 ];then ...fi 这里首选要进一步解释退出状态的含义。之前已经说了，退出状态指的是命令（包括脚本和函数）在执行完毕之后，向操作系统返回的值。这个值是一个 0~255 的整数，用来表示命令执行成功还是失败，其中 0 代表命令执行成功。参数$?则用于保存这个返回值。 由此可以看出if在这里做的就是判断expr $num + 1 &amp;&gt; /dev/null是否执行成功。 此外，我们也可以使用if...elif..else的形式，如下： 12345678if condition1then command1elif condition2 command2else commandNfi 此外，在实际开发过程中还经常会对文件状态进行判断，比如说判断这是不是一个文件夹、是不是一个文本文件等；或者会对字符串进行判断，比如说字符串是否为空，字符串长度是否符合要求；还会对数值进行比较操作，就像例子中提到到值是不是为 0 等。 判断表达式12345if test #表达式为真if test ! #表达式为假test 表达式1 –a 表达式2 #两个表达式都为真test 表达式1 –o 表达式2 #两个表达式有一个为真test 表达式1 ! 表达式2 #条件求反 文件表达式12345678910111213141516171819202122test File1 –ef File2 #两个文件是否为同一个文件，可用于硬连接。主要判断两个文件是否指向同一个inode。test File1 –nt File2 #判断文件1是否比文件2新test File1 –ot File2 #判断文件1比是否文件2旧test –b file #文件是否块设备文件test –c File #文件并且是字符设备文件test –d File #文件并且是目录test –e File #文件是否存在 （常用）test –f File #文件是否为正规文件 （常用）test –g File #文件是否是设置了组idtest –G File #文件属于的有效组IDtest –h File #文件是否是一个符号链接（同-L）test –k File #文件是否设置了Sticky bit位test –b File #文件存在并且是块设备文件test –L File #文件是否是一个符号链接（同-h）test –o File #文件的属于有效用户IDtest –p File #文件是一个命名管道test –r File #文件是否可读test –s File #文件是否是非空白文件test –t FD #文件描述符是在一个终端打开的test –u File #文件存在并且设置了它的set-user-id位test –w File #文件是否存在并可写test –x File #文件属否存在并可执行 字符串表达式12345678test string #string不为空test –n 字符串 #字符串的长度非零test –z 字符串 #字符串的长度是否为零test 字符串1＝字符串2 #字符串是否相等，若相等返回truetest 字符串1＝=字符串2 #字符串是否相等，若相等返回truetest 字符串1!＝字符串2 #字符串是否不等，若不等反悔falsetest 字符串1&gt;字符串2 # 在排序时，string1 在 string2 之后test 字符串1&lt;字符串2 # 在排序时，string1 在 string2 之前 整数表达式123456test 整数1 -eq 整数2 #整数相等test 整数1 -ge 整数2 #整数1大于等于整数2test 整数1 -gt 整数2 #整数1大于整数2test 整数1 -le 整数2 #整数1小于等于整数2test 整数1 -lt 整数2 #整数1小于整数2test 整数1 -ne 整数2 #整数1不等于整数2 以上表达式摘自test - shell环境中测试条件表达式工具，test 是测试条件表达式的工具，test 后面部分的内容可以用于if条件判断中。 WHILE 和 UNTIL 循环再接之前的脚本来讲解 while 的用法 123456789101112131415161718#!/bin/bash#while truedo read -p \"please input a positive number: \" num expr $num + 1 &amp;&gt; /dev/null if [ $? -eq 0 ];then if [ `expr $num \\&gt; 0` -eq 1 ];then for((i=1;i&lt;=$num;i++)) do sum=`expr $sum + $i` done echo \"1+2+3+....+$num = $sum\" exit fi fi echo \"error,input enlegal\"done 前面也说过，这个脚本的目的是接收一个 num，如果输入的 num 不是一个整数就一直让用户输入，直到输入的 num 是一个整数为止。这里就用到了 while 循环，并且将循环条件设置为 true，也就是一个永久的循环。循环不能终止，按照逻辑我们要在用户输入整数并完成计算之后推出程序,所以这里通过exit来退出程序，这里也可以使用break来跳出循环。我们还可以配合使用continue表示继续执行，这里没有举例说明。 while命令退出状态不为0时终止循环，而until命令则刚好相反。除此之外，until命令与while命令很相似。until循环会在接收到为0的退出状态时终止。在while-count脚本中，循环会一直重复到count变量小于等于5。使用until改写脚本也可以达到相同的效果。 12345678910#!/bin/bash# until-count: display a series of numberscount=1until [ $count -gt 5 ]; do echo $count count=$((count + 1))doneecho \"Finished.\" 将测试表达式改写为count –gt 5 until就可以在合适的时刻终止循环。选择使用while还是until，通常取决于哪种循环能够允许程序员写出最明了的测试表达式。 case 分支前面讲了使用if做条件判断，在其他语言比如 C++ 或者 Java 等中都存在switch..case..这样的语句，shell 也提供了case这个多选项符合命令，它的命令格式是这样的： 123case word in [pattern [| pattern]...) commands ;;]...esac 在这里我想举一个做算数运算的例子，这和例子与下一节讲解的函数相关，其中用到了case，但是即使不了解函数怎么使用，也不会对理解case的运用造成影响。 1234567891011121314151617181920#!/bin/bash#function calcu&#123; case $2 in +) echo \"`expr $1 + $3`\" ;; -) echo \"`expr $1 - $3`\" ;; \\*) echo \"`expr $1 \\* $3`\" ;; /) echo \"`expr $1 / $3`\" ;; esac&#125;calcu $1 $2 $3 这个脚本希望做的是一次算数运算，根据操作符是+ - * /来进行运算。 for 循环现在到了流控制的最后一节了，for 循环其实在文章一开始我们就见过了，在文章最开始我举了两个例子： 1234567891011# 获取该文件夹下后缀为 jpg 结尾的列表for image in `ls *.jpg`do ....done# 输出 0-9 共 10 个数字for ((i=0; i&lt;10; i++));do echo $&#123;i&#125;done 第一种是传统的形式，和 python 中的 for 循环很像，我们可以像这样for i in A B C D;do echo $i; done使用 for 循环，可以将循环的内容就当成 python 中的一个列表，也可以像for i in {A..E};do echo $i; done这样创建字符列表。 第二种方式就是 C 语言的形式了for ((i=0; i&lt;10; i++))，比较常规也没什么值得讲的。 函数在上一节中，我们介绍了 shell 中的流控制的语法：if, while, until, case和 for。再之前我们讲了变量、字符串和数学运算。到这一节就可以讲一下函数这个话题了。 了解了上面这些内容理论上已经能够写出任何的程序的，不过写程序的过程中会有许多类似的代码，如果全部重新写一遍程序就会显得冗长，所以一般的做法就是将可以复用的代码抽取出来形成函数。shell 自然也支持函数的使用，接下里就看看在 shell 中怎么定义和使用函数。 首先看 shell 中函数是怎么定义的。shell 中的函数有两种定义格式，使用任意一种都可以。 1234567name()&#123; command1 command2 ... commandn&#125; 1234567function name&#123; command1 command2 ... commandn&#125; 我比较习惯用第二种形式，你可以选择任何一种方式，不过我接下来的例子是按照第二种定义方式。 我们知道了函数定义的架构，但是如果你用过其他编程语言会发现它没有参数列表，也没有返回值，这在一开始也让我觉得很困惑，但是我们在变量那一节学过特殊变量的含义，其中有一个变量是$0表示函数的名称，shell 中的变量是通过命令行键入，再用$1 $2 $3读取的。这就和 C++ 或者 Java 中的主函数读取参数一个道理，char** argv和String[] args就是由命令行键入的参数列表。 下面就用一个具体函数的例子进行说明，这个例子在 流控制——case 这一节也讲过，但是没有讲完： 12345678910111213141516171819202122#!/bin/bash#function calcu&#123; case $2 in +) echo \"`expr $1 + $3`\" ;; -) echo \"`expr $1 - $3`\" ;; \\*) echo \"`expr $1 \\* $3`\" ;; /) echo \"`expr $1 / $3`\" ;; esac&#125;calcu $1 $2 $3echo \"\"calcu $1 $2 $3\"\" 首先这个脚本的文件名为 calcu，其中定义了一个函数 calcu，采用的是第二种函数定义方式，在函数体中我们利用case做了一个多条件判断。 在脚本的结尾我们调用了 calcu 这个函数，并将输入了三个参数，紧接着为了给大家看到三个参数分别是什么我选择将其打印出来。调用函数的过程是这样的： 123ubuntu@VM-0-2-ubuntu:/tmp$ ./calcu.sh 5 + 38calcu 5 + 3 可以看到在这里$1 = 5, $2 = +, $3 = 3，这就是 shell 中传递参数的方式。 再看 shell 中返回值这个问题，shell 有两种返回值的方式，一种是使用return，一种是使用echo。 使用return 使用 return 返回值，只能返回 1-255 的整数 函数使用 return 返回值。通常只能用来供其他地方调用获取状态，因此通常仅返回0或1；0表示成功，1表示失败 使用echo 使用 echo 可以返回任何字符串结果 通常用于返回数据，比如一个字符串值或者列表值 echo的内容作为返回值可以看一下上面这个例子，这里再举一个return来返回值的例子。 12345678910111213#!/bin/bash#this_pid=$$function is_nginx_running&#123; ps -ef | grep nginx | grep -v grep | grep -v $this_pid &amp;&gt; /dev/null if [ $? -eq 0 ];then return else return 1 fi&#125;is_nginx_running &amp;&amp; echo \"Nginx is running\" || echo \"Nginx is stoped\" 和前面说过的退出状态一样，return 也是 0 表示函数执行成功，其他表示执行失败。这里举的例子是通过查看 Nginx 的进程来来确认 Nginx 是否运行。 首先是用$$来接收这个 shell 脚本的 Pid，因为脚本名字中带有 Nginx 就需要将其利用 Pid 过滤掉。还记得之前讲的的退出状态么，如果 ps 命令找到了 nginx 进程退出状态就会是 0，表示成功。最后就通过return来返回 nginx 是否正常运行。 之后可以用后台挂起的形式运行这个脚本，将其作为 nginx 的守护进程：nohup sh nginx.sh &amp;，使用tail -f nohup.out来查看监听结果。 局部变量和全局变量讲到函数就还有一个作用域的问题需要讨论，shell 中的局部变量、全局变量和一般编程语言没什么区别。 12345678910#!/bin/bash#var1=\"Hello world\"function test&#123; local var2=87&#125;testecho $var1echo $var2 这里给到一个脚本自行体会一下即可。 函数库基础篇的最后一部分就用来介绍一下 shell 中的函数库。函数库是每一门编程语言中非常重要的一部分，比如说 C++ 中的标准库，Java 中的 JDK，正式这些优秀的库存在才让编程变得更加高效。 shell 也是可以封装自己的库的，比如我现在下一个你要加加减乘除封装成一个函数库，作为之前 calcu 脚本的升级版，该函数库实现以下几个函数： 加法函数 add 减法函数 reduce 乘法函数 multiple 除法函数 divide 打印系统运行情况的函数 sys_load，该函数显示内存运行情况， 1234567891011121314151617181920212223242526272829function add&#123; echo \"`expr $1 + $2`\"&#125;function reduce&#123; echo \"`expr $1 - $2`\"&#125;function multiple&#123; echo \"`expr $1 \\* $2`\"&#125;function divide&#123; echo \"`expr $1 / $2`\"&#125;function sys_load&#123; echo \"Memory Info\" echo free -m echo echo \"Disk Usage\" echo df -h echo&#125; 我们将上面文件封装成一个函数库 lib/base_function。 经常使用的重复代码封装成函数文件 一般不直接执行，而是由其他脚本调用 接着用一个 shell 脚本 calculate.sh 调用函数库中的函数 1234567#!/bin/bash#. /root/lesson/3.5/lib/base_functionadd 12 23reduce 90 30multiple 12 12divide 12 2 编写函数库文件的建议 库文件名的后缀是任意的，但一般使用 .lib 库文件通常没有可执行选项 库文件无序和脚本在同级目录，只需在脚本中引用时指定 第一行一般使用 #/bin/echo, 输出警告信息，避免用户执行 参考资料《Linux 命令行大全》 Shell脚本编程30分钟入门 Awesome Shell","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"http://shuiyujie.com/tags/shell/"}]},{"title":"Linux进程","slug":"Linux/Linux进程","date":"2019-06-29T15:39:03.000Z","updated":"2019-06-30T12:02:23.951Z","comments":true,"path":"post/87698413.html","link":"","permalink":"http://shuiyujie.com/post/87698413.html","excerpt":"ps：显示当前所有进程的运行情况。 top：实时显示当前所有任务的资源占用情况。 jobs：列出所有活动作业的状态信息。 bg：设置在后台中运行作业。 fg：设置在前台中运行作业。 kill：发送信号给某个进程。 killall：杀死指定名字的进程。 shutdown：关机或者重启系统。","text":"ps：显示当前所有进程的运行情况。 top：实时显示当前所有任务的资源占用情况。 jobs：列出所有活动作业的状态信息。 bg：设置在后台中运行作业。 fg：设置在前台中运行作业。 kill：发送信号给某个进程。 killall：杀死指定名字的进程。 shutdown：关机或者重启系统。 使用ps命令查看进程信息","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]},{"title":"Linux目录结构","slug":"Linux/Linux目录结构","date":"2019-06-28T14:43:03.000Z","updated":"2019-09-19T14:59:23.705Z","comments":true,"path":"post/5e2eb880.html","link":"","permalink":"http://shuiyujie.com/post/5e2eb880.html","excerpt":"","text":"/bin - 基本命令的二进制文件。 /boot - 引导加载程序的静态文件。 /dev - 设备文件。 /etc - 配置文件。 /home - 普通用户主目录的父目录。 /lib - 共享库文件。 /lib64 - 共享64位库文件。 /lost+found - 存放未链接文件。 /media - 自动识别设备的挂载目录。 /mnt - 临时挂载文件系统的挂载点。 /opt - 可选插件软件包安装位置。 /proc - 内核和进程信息。 /root - 超级管理员用户主目录。 /run - 存放系统运行时需要的东西。 /sbin - 超级用户的二进制文件。 /sys - 设备的伪文件系统。 /tmp - 临时文件夹。 /usr - 用户应用目录。 /var - 变量数据目录。 123456789101112131415161718192021222324252627282930313233343536373839404142434445\"/\" : 根目录Linux文件系统的入口.也是最高级，最重要的的目录.除衍生出其它目录，还和系统的开机，还原，系统修复有的，一般要求不要把任务应用程序直接放在根目录下，如果满了，可能就登录不了了（需要注意是否日志输出在根目录下）\"/bin\":基本系统所需要的命令,主要是单用户模式下，还能执行的命令。主要有cat，mv,mkdir,cp,bash ,ls\"/boot\":内核和加载内核所需要的文件.grub系统引导管理器也在这个目录下,存在/boot/grub/\"/dev\":在linux系统下，任何设备及接口设备，都是以文件的形式存在，设备文件存储目录.像终端.磁盘等.比较重要的有/dev/null （crontab经常把不想输出输到这里）;/dev/zero;/dev/tty;/dev/lp*;/dev/hd*;/dev/sd*\"/etc\":系统的主要配置文件都放在这个目录下，一般来说，这个普通人是看不到这些文件的;这里最好也不要放可执行文件。常见的目录有如下：/etc/inittab ;/etc/init.d/;/etc/modprobe.conf ;/etc/X11/ ;/etc/fstab/ ;/etc/sysconfig/ ;/etc/init.d/开机启动脚本放在这里;/etc/xinetd.d/ super daemon启动脚本放在这里/etc/sysconfig/network/scripts/网管配置放在下面下\"/home\":普通用户的目录默认存储目录.主文件夹有两种代号：~ 代表这个用户的主目录~dmtsai:代表 dmtsai的主文件夹\"/lib\":库文件和内核模块存放目录.主要是开机用到，以及在/bin/;/sbin目录下命 令会调用的库函数。/lib/modules会放内核相关的模块。\"/lib64\":和/lib类似，主要是64的库函数/usr 是存放可以分享与不可以动的内容，不是user的缩写，而是UNIX Software Resource 的缩写，就是UNIX操作系统的软件资源主要子目录有：/usr/X**/ X Windows数据存在于此/usr/bin/ 绝大部分用户可用命令在此/usr/include/ C/C++等的头文件与包含文件在些。如果以源码的*.tar.gz安装软件时，可能会引用下面的文件/usr/lib/ 包含应用程序函数库及目标文件，以及不被一般用户惯用的执行文件或脚 本/usr/lib64/ 与/usr/lib/相似，会对64位的/usr/local/ 本机自己安装的的软件，建议安装到这里，下面也有bin,etc,include,lib子目录，这些子目录功能大家可以想到/usr/sbin/ 非系统正常运行所需要的命令，最常见的就是网络服务器软件的daemon/usr/share/ 放共享文件的地方，基本是文本文件，可读的。子目录有/usr/share/man 在线帮助文件/usr/share/doc 软件杂项的说明文件/usr/share/zoneinfo 软件杂项的说明文件/usr/src 软件源码放在这里\"/media\":即插即用设备的挂载点自动存放在这个目录下.像U盘,cdrom/dvd自动挂载后,就会在这个目录下.常见有/media/floppy,/media/cdrom\"/mnt\":临时文件系统的挂载点目录.以前和/media一样，但有专门/media后，专门做临时挂载\"/opt\":第三方软件的存放目录.什么叫第三方呢？像KDE，就是第三方的，只是集成到linux里，当然你可以放自己的东西到下面。\"/root\":Linux超级权限用户root的跟目录.单独存放，也方便在进入单用户模式的操作。\"/sbin\":基本的系统维护命令,只能由超级用户使用.这些命令为开机、修复、还原系统过程所需要的。常见的命令有fdisk,fsck,ifconfig,init,mkfs\"/srv\":存放一些服务器启动之后需要提取的数据.主要存放网络服务后的一些数据，如www,ftp\"/tmp\":临时文件目录,这个目录任何人可以访问，有可能需要定期清理。\"/usr\":存放用户使用系统命令和应用程序等信息.像命令.帮助文件等.\"/var\":存放经常变动的数据,像日志.邮件等.这个目录下的数据，需要定期清理，最好写个脚本，放在crontab里。常见的子目录：/var/cache/ 执行中的缓存文件/var/lib/ 软件本身执行的过程中，用到的数据。比如Mysql数据一般放在/var/lib/mysql/;而rpm数据则放在 /var/lib/rpm//var/lock/ 资源被锁时，放在此。有些资源只能一个程序使用，比如刻录机。/var/log/ 系统日志存在地，有可能需要定时清理/var/mail/ 系统邮件，看情况需要定时清理/var/run/ 某些程序或服务启动后，PID放在下现/var/spool/放置队列数据， 看情况需要定时清理/lost+found 是ext2/ext3文件系统才产生的，目的是当文件系统产生错误里，将一些丢失的片段防在这个目录下。/proc 是一个虚拟文件系统。放置内存中的数据，当有一个进程启动时，就有一个文件夹。比较重要的/proc/meminfo,/proc/cpuinfo可以通过这两文件查看内存和CPU情况，当然还有 /proc/dma, /proc/interrupts, /proc/ioports,/proc/net/*等/sys 和/proc相似，也是虚拟文件系统，主要记录内核相关，比如内核模块，内核检测的硬件信息。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]},{"title":"备忘-Mac使用小技巧","slug":"善用佳软/备忘-Mac使用小技巧","date":"2019-06-22T16:39:34.000Z","updated":"2019-09-22T23:39:42.426Z","comments":true,"path":"post/ccb5aaa3.html","link":"","permalink":"http://shuiyujie.com/post/ccb5aaa3.html","excerpt":"备忘：一些使用 mac 的小 tips。","text":"备忘：一些使用 mac 的小 tips。 如何设置允许任何来源软件安装？“安全性与隐私” 勾选任何来源，如果没有该选项，在终端输入如下命令： 1sudo spctl --master-disable 如何设置开机启动项目 连接不上 Wifi，其他设备可以正常使用123456以下方法都是打苹果客服的热线得来的，顺序为推荐顺序1.打开设置-网络-位置，更改为自动，或者顺便新建一个新的位置2.打开设置-网络-高级，DNS，设置为114.114.114.114和8.8.8.83.打开finder-前往-电脑-Macintosh HD-资源库-Perferences，删除这个文件com.apple.wifi.message-tracer4.关机，立即同时按下control+commmand+R+P, 听到三声响声后松手","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"mac","slug":"mac","permalink":"http://shuiyujie.com/tags/mac/"}]},{"title":"【Caffe】Caffe实战手写数字分类","slug":"Deeplearning/【Caffe】Caffe实战手写数字分类","date":"2019-06-17T10:21:53.000Z","updated":"2019-09-30T14:46:52.361Z","comments":true,"path":"post/96cd1245.html","link":"","permalink":"http://shuiyujie.com/post/96cd1245.html","excerpt":"Caffe是优秀的深度学习开源框架，并具有良好的开源生态，我们可以在Model zoo中找到许多模型的实现。本文以 Caffe 提供的 Training LeNet on MNIST with Caffe 为例，介绍 Caffe 使用流程，并且在原文基础上增加可视化 accuracy 和 loss 的内容。 本文的目标是能够了解 Caffe 基本的使用流程。","text":"Caffe是优秀的深度学习开源框架，并具有良好的开源生态，我们可以在Model zoo中找到许多模型的实现。本文以 Caffe 提供的 Training LeNet on MNIST with Caffe 为例，介绍 Caffe 使用流程，并且在原文基础上增加可视化 accuracy 和 loss 的内容。 本文的目标是能够了解 Caffe 基本的使用流程。 准备数据在安装和编译 Caffe 之后进入根目录，执行以下命令，将会下载 MNIST 数据集。 123cd $CAFFE_ROOT./data/mnist/get_mnist.sh #下载 mnist 数据集./examples/mnist/create_mnist.sh # 将下载下来的数据集格式调整为 caffe 能够解析的格式 数据集包括 mnist_train_lmdb 和 mnist_test_lmdb 两部分。 定义 NetCaffe 提供的示例中使用 lenet_train_test.prototxt 定义 LeNet 的网络结构。LetNet 网络结构为卷积层 -&gt; 池化层 -&gt; 卷积层 -&gt; 池化层 -&gt; 全连接层 -&gt; 全连接层 -&gt; 全连接层。Caffe 用 layer 来表示表示网络，网络被定义在以.prototxt结尾的文件中，因为 Caffe 使用了 Google Protobuf 作为数据传输的格式。 在 lenet_train_test.prototxt 中我们可看到name: &quot;LeNet&quot;表示网络结构的名称为 LeNet，接下来是 123456789101112131415layer &#123; name: \"mnist\" # 名称 type: \"Data\" # 输入层 transform_param &#123; scale: 0.00390625 # 归一化，范围为 0-1 &#125; data_param &#123; source: \"mnist_train_lmdb\" # 指定要读取的数据 backend: LMDB batch_size: 64 &#125; top: \"data\" top: \"label\"&#125;... 我们可以通过 http://ethereon.github.io/netscope/#/editor 生成网络图像，或者使用 caffe 提供的脚本，它在caffe/python/draw_net.py。 配置 SolverSolver 用来指定训练的一些参数，比如使用哪个网络、迭代次数、优化方法、使用 GPU 还是 CPU 等信息。 12345678910111213141516171819202122232425# The train/test net protocol buffer definitionnet: \"examples/mnist/lenet_train_test.prototxt\"# test_iter specifies how many forward passes the test should carry out.# In the case of MNIST, we have test batch size 100 and 100 test iterations,# covering the full 10,000 testing images.test_iter: 100# Carry out testing every 500 training iterations.test_interval: 500# The base learning rate, momentum and the weight decay of the network.base_lr: 0.01momentum: 0.9weight_decay: 0.0005# The learning rate policylr_policy: \"inv\"gamma: 0.0001power: 0.75# Display every 100 iterationsdisplay: 100# The maximum number of iterationsmax_iter: 10000# snapshot intermediate resultssnapshot: 5000snapshot_prefix: \"examples/mnist/lenet\"# solver mode: CPU or GPUsolver_mode: GPU 训练训练的时候指定 Solver，保存权值文件的路径，日志：./examples/mnist/train_lenet.sh -gpu 1 12345#!/usr/bin/env shset -eSOLVER=examples/mnist/lenet_solver.prototxtWEIGHTS=./lenet_iter_10000.caffemodel./build/tools/caffe train --solver=$&#123;SOLVER&#125; $@ 2&gt;&amp;1 | tee log.txt 通过解析日志，我们能够可视化模型训练过程中的 accuray 和 loss 的情况。 1234567891011121314151617181920# 解析日志文件,将 acc 和 loss 输出到 .refine 文件grep \"Test net output #0: accuracy =\" log.txt &gt; trainacc.refinegrep \"Test net output #1: loss =\" log.txt &gt; trainloss.refine# 生成如下所示的 .refine 文件# trainacc.refineI0615 10:16:15.671094 11161 solver.cpp:414] Test net output #0: accuracy = 0.9903I0615 10:16:16.061339 11161 solver.cpp:414] Test net output #0: accuracy = 0.9903I0615 10:16:16.451835 11161 solver.cpp:414] Test net output #0: accuracy = 0.988I0615 10:16:16.850911 11161 solver.cpp:414] Test net output #0: accuracy = 0.9906# trainloss.refineI0615 10:16:15.671116 11161 solver.cpp:414] Test net output #1: loss = 0.0298686 (* 1 = 0.0298686 loss)I0615 10:16:16.061362 11161 solver.cpp:414] Test net output #1: loss = 0.0285139 (* 1 = 0.0285139 loss)I0615 10:16:16.451859 11161 solver.cpp:414] Test net output #1: loss = 0.0348767 (* 1 = 0.0348767 loss)I0615 10:16:16.850932 11161 solver.cpp:414] Test net output #1: loss = 0.0284803 (* 1 = 0.0284803 loss)# 接着用 python 脚本解析 .refine 绘制 acc 和 loss 曲线python show_acc.pypython show_loss.py Python 脚本可以在 Github Gist 中查看，需要根据之前生成的日志进行小改动。 可以看到迭代 100 次 loss 已经降到很低了,之后略有降低,300次左右基本上就在 0.3, 0.4 左右 与之相对的，100 次左右准确率就有 0.96 了,缓慢上升,最后准确率稳定在 0.98 。 测试测试的方法和训练的方法基本一致，使用 ./examples/mnist/test_lenet.sh -gpu 1 123456789101112#!/usr/bin/env shset -eMODEL=examples/mnist/lenet_iter_10000.caffemodelTRAIN_NET=examples/mnist/lenet_train_test.prototxt./build/tools/caffe test -model $&#123;TRAIN_NET&#125; -weights $&#123;MODEL&#125; $@ 2&gt;&amp;1 | tee test_log.txt# 结果如下所示...I0616 15:29:32.107722 13441 caffe.cpp:304] Batch 49, accuracy = 1I0616 15:29:32.107739 13441 caffe.cpp:304] Batch 49, loss = 0.00704199I0616 15:29:32.107748 13441 caffe.cpp:309] Loss: 0.0412109I0616 15:29:32.107769 13441 caffe.cpp:321] accuracy = 0.9862I0616 15:29:32.107782 13441 caffe.cpp:321] loss = 0.0412109 (* 1 = 0.0412109 loss) 用上面同样的方法能够可视化测试的 loss 和 accuracy。 本文介绍了 Caffe 的基本使用流程，这样就可以把一些别人写好的网络用起来了。进一步我们需要了解如何用 Caffe 自定义网络结构，调整网络的参数，完成更加个性化的任务。 参考：Training LeNet on MNIST with Caffe A step by step guide to Caffe Caffe Caffe Model zoo","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"caffe","slug":"caffe","permalink":"http://shuiyujie.com/tags/caffe/"}]},{"title":"【Caffe】Caffe代码结构","slug":"Deeplearning/【Caffe】Caffe代码结构","date":"2019-06-17T05:31:53.000Z","updated":"2019-09-30T14:45:51.765Z","comments":true,"path":"post/c2c24b74.html","link":"","permalink":"http://shuiyujie.com/post/c2c24b74.html","excerpt":"Caffe 安装看 Installation, 第一个例子可以看 Training LeNet on MNIST with caffe 的示例.跟着示例做一遍,精确度能有 98% 左右,做完很有成就感.跑完之后可以跑更多的官方示例,它包括 Notebook Example 和 Command Line Example. Caffe Tutorial介绍了 Caffe 的基础,其中也包括各种数据结构 http://caffe.berkeleyvision.org/tutorial/ Model Zoo有很多实现的网络和训练好的模型 接下来重点看一下 Caffe Tutorial 了解基本数据结构","text":"Caffe 安装看 Installation, 第一个例子可以看 Training LeNet on MNIST with caffe 的示例.跟着示例做一遍,精确度能有 98% 左右,做完很有成就感.跑完之后可以跑更多的官方示例,它包括 Notebook Example 和 Command Line Example. Caffe Tutorial介绍了 Caffe 的基础,其中也包括各种数据结构 http://caffe.berkeleyvision.org/tutorial/ Model Zoo有很多实现的网络和训练好的模型 接下来重点看一下 Caffe Tutorial 了解基本数据结构 目录结构 数据结构类src/caffe 目录下 blob.hpp / cpp layer.hpp / cpp net.hpp/cpp solver.hpp/cpp sgd_solvers.hpp/cpp blob , layer , net 的定义 solver _factory.hpp/cpp layer_factory.hpp/cpp 工厂类模板定义和普通 layer 的模板定义 caffe.hpp/cpp common.hpp/cpp 通用包含文件 internal thread.hpp/cpp parallel.hpp/cpp syncedmem.hpp/cpp gpu 编程和内存等较为底层的文件 IO 类不同格式的数据读取层 base_data_layer.hpp/cpp data_layer.npp/cpp window_data_layer.cpp parameter_layer.cpp memory_data_layer.cpp dummy_data_layer.cpp hdf5_data_layer.cpp hdf5_output_layer.cpp image_data_layer.hpp/cpp 不同格式的数据读取层 data_transformer.hpp/cpp 数据的预处理，增强等变换 基础函数类 math.hpp / cpp 基本数学操作，加减乘除 absval_layer.hpp / cpp exp_layer.hpp / cpp log_layer.hpp / cpp 基础数学函数变换 power_layer.hpp /cpp tanh_layer.hpp / cpp sigmoid_layer.hpp / cpp relu_layer.hpp / cpp 若干激活函数 形状处理类 flatten_layer.hpp/cpp slice_layer.hpp / cpp split_layer.hpp / cpp tile_layer.hpp / cpp concat_layer.hpp / cpp reduction_layer.hpp / cpp eltwise_layer.hpp/cpp crop_layer.hpp/cpp pooling_layer.hpp / cpp scale_layer.hpp / cpp 对 blob 进行各类形状变换 损失函数类用于分类，回归等任务的常见损失函数定义 1234567multinomial_logistic_loss_layer.hpp/cppsoftmax_loss_layer.hpp/cppeuclidean_loss_layer.hpp/cppsigmoid_cross_entropy_loss_layer.hpp/cppcontrastive_loss_layer.hpp/cpphinge_loss_layer.hpp/cppinfogain_loss_layer.hpp/cpp 卷积类卷积与反卷积定义 12345im2col_layer.cppbase_conv_layer.cppconv_layer.cppdeconv_layer.cppinner_product_layer.cpp","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"caffe","slug":"caffe","permalink":"http://shuiyujie.com/tags/caffe/"}]},{"title":"【Caffe】caffe的数据层次","slug":"Deeplearning/【Caffe】caffe的数据层次","date":"2019-06-17T05:21:53.000Z","updated":"2019-09-30T14:46:21.479Z","comments":true,"path":"post/691f534d.html","link":"","permalink":"http://shuiyujie.com/post/691f534d.html","excerpt":"Deep networks are compositional models that are naturally represented as a collection of inter-connected layers that work on chunks of data. Caffe defines a net layer-by-layer in its own model schema. The network defines the entire model bottom-to-top from input data to loss. As data and derivatives flow through the network in the forward and backward passes Caffe stores, communicates, and manipulates the information as blobs: the blob is the standard array and unified memory interface for the framework. The layer comes next as the foundation of both model and computation. The net follows as the collection and connection of layers. The details of blob describe how information is stored and communicated in and across layers and nets. Solving is configured separately to decouple modeling and optimization. We will go over the details of these components in more detail. https://caffe.berkeleyvision.org/tutorial/net_layer_blob.html","text":"Deep networks are compositional models that are naturally represented as a collection of inter-connected layers that work on chunks of data. Caffe defines a net layer-by-layer in its own model schema. The network defines the entire model bottom-to-top from input data to loss. As data and derivatives flow through the network in the forward and backward passes Caffe stores, communicates, and manipulates the information as blobs: the blob is the standard array and unified memory interface for the framework. The layer comes next as the foundation of both model and computation. The net follows as the collection and connection of layers. The details of blob describe how information is stored and communicated in and across layers and nets. Solving is configured separately to decouple modeling and optimization. We will go over the details of these components in more detail. https://caffe.berkeleyvision.org/tutorial/net_layer_blob.html blob 数据表示为四维张量 (N, C, H, W) N 是 batch Size 大小 C 是 channel H, W 为图片宽高 data 存储数据 diff 存储梯度 shape 分别为 blob 的尺度 count 是所有数据数目,即 NCH*W layerlayer 是基本计算单元,除了输入层没有 bottom,输出层没有 top,每一层都有 bottom 和 top,分别串接前一层和后一层.每一种 layer 都有对应的 layer_param,用于实现该层参数的配置。 需要实现 setup, forward, backward 登函数 使用 prototxt 配置 包含网络层参数,如输入数据层的 data_param Caffe 中的序列化 /caffe/src/caffe/proto/caffe.proto使用 protobuf 协议进行数据序列化和解析,实际使用的时候回编译成与所定义的数据结构对应的代码,从而实现数据的读取,解析和存储 Caffe 中的 LayerParameter 示例 123456789101112131415161718192021222324// NOTE// Update the next available ID when you add a new LayerParameter field.//// LayerParameter next available layer-specific ID: 149 (last added: clip_param)message LayerParameter &#123; optional string name = 1; // the layer name optional string type = 2; // the layer type repeated string bottom = 3; // the name of each bottom blob repeated string top = 4; // the name of each top blob // The train / test phase for computation. optional Phase phase = 10; // The amount of weight to assign each top blob in the objective. // Each layer assigns a default value, usually of either 0 or 1, // to each top blob. repeated float loss_weight = 5; // Specifies training parameters (multipliers on global learning constants, // and the name and other settings used for weight sharing). repeated ParamSpec param = 6; // The blobs containing the numeric parameters of the layer. repeated BlobProto blobs = 7; https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto netNet 就是一个由 layer 实例组成的完整的 CNN 模型 Caffe中的Net类是如何工作的 Caffe 中的训练参数 参数 含义 net 网络文件路径 test_iter 一次测试的 batch 数目，如果它等于 1，就说明支取一个 batch size 的数据来做测试，如果 batch size 太小，那么对于分类任务来说统计出来的指标也不可信，所以一次测试，用到所有测试数据，因为常令 test_iter *test_batchsize = 测试集合的大小 test_interval 测试间隔，即每隔多少次迭代进行一次测试 base_lr 学习率 lr_policy 学习率策略，base_lr 和 lr_policy 决定了学习率大小如何变化，而不同的学习率变化方法又有不同的参数 学习率的变化方法很多：fixed, step, exp, inv, multistep, poly, sigmoid type 优化方法 clip_gradients 固定梯度范围 momentum 动量项 momentum 优化策略 Adam 用到的参数 weight_decay 权重衰减率 display 显示间隔 debug_info max_iter 最大迭代次数 snapshot 存储模型间隔 snapshot_prefix 存储路径与前缀 solver_mode GPU/CPU 开关，可以指定用 GPU 或者 CPU 进行训","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"caffe","slug":"caffe","permalink":"http://shuiyujie.com/tags/caffe/"}]},{"title":"IDEA的使用","slug":"善用佳软/IDEA的使用","date":"2019-06-14T13:58:54.000Z","updated":"2019-09-22T08:18:47.750Z","comments":true,"path":"post/d3e6b9b.html","link":"","permalink":"http://shuiyujie.com/post/d3e6b9b.html","excerpt":"IDEA的基本使用：让你的IDEA有飞一般的感觉 IntelliJ IDEA 简体中文专题教程 IntelliJ IDEA 使用教程(2019图文版) – 从入门到上瘾 IDEA一定要懂的32条快捷键","text":"IDEA的基本使用：让你的IDEA有飞一般的感觉 IntelliJ IDEA 简体中文专题教程 IntelliJ IDEA 使用教程(2019图文版) – 从入门到上瘾 IDEA一定要懂的32条快捷键","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"idea","slug":"idea","permalink":"http://shuiyujie.com/tags/idea/"}]},{"title":"Markdown 记要","slug":"善用佳软/Markdown-记要","date":"2019-06-14T13:57:54.000Z","updated":"2019-09-18T15:41:34.767Z","comments":true,"path":"post/c002250d.html","link":"","permalink":"http://shuiyujie.com/post/c002250d.html","excerpt":"Markdown 是一种轻量级的’「标记语言」，支持插入图片、图表、超链接、数学公式，用简单的语法能够形成一篇形式丰富的文章。 Markdown 编辑器有很多，我使用的是 typora。","text":"Markdown 是一种轻量级的’「标记语言」，支持插入图片、图表、超链接、数学公式，用简单的语法能够形成一篇形式丰富的文章。 Markdown 编辑器有很多，我使用的是 typora。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 一级标题## 二级标题### 三级标题个人不建议使用三级以上的标题生成目录：[TOC]锚点：(能够链接到某个一级标题)[想要显示的名称](#锚点的名称)**粗体***斜体*`行内代码`&gt; 在一段话后面加上 ——某名人，这就是一句名言&lt;u&gt;下划线&lt;/u&gt;~~删除线~~[超链接名称](超链接路径)![图片名称](图片链接)有序列表1. 选项一2. 选项二3. 选项三无序列表- 列表一- 列表二- 列表三复选框* [ ] 第一件事* [x] 第二件事* [ ] 第三件事表格name | age---- | ---LearnShare | 12Mike | 32邮箱：&lt;123456@qq.com&gt;分隔线*** 微信公众号 makedown 排版工具：http://prod.zkqiang.cn/wxeditor","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://shuiyujie.com/tags/markdown/"}]},{"title":"矢量素材&图片素材","slug":"善用佳软/矢量素材-图片素材","date":"2019-06-14T11:03:30.000Z","updated":"2019-09-16T11:12:27.406Z","comments":true,"path":"post/4b821db7.html","link":"","permalink":"http://shuiyujie.com/post/4b821db7.html","excerpt":"免费矢量素材 高清无版权限制大图特供网站","text":"免费矢量素材 高清无版权限制大图特供网站 免费矢量素材http://www.vecteezy.com/ http://www.vectorportal.com/ http://vector4free.com/ http://www.freepik.com/ http://vectorart.org/ http://sxc.hu/ http://www.stockvault.net/ http://www.squidfingers.com/patterns/ http://lostandtaken.com/ http://www.cgtextures.com/ http://www.textureking.com/ http://subtlepatterns.com/ 12个高清无版权限制大图特供网站http://pixabay.com/ https://unsplash.com/ http://www.gratisography.com/ http://picjumbo.com/page/5/ http://www.lifeofpix.com/ http://www.imcreator.com/free http://www.freeimages.com/home http://deathtothestockphoto.com http://publicdomainarchive.com/ http://snapographic.com/ 20个下载免费矢量素材的最佳网站http://www.vectorportal.com/ http://www.vectorss.com/ http://www.vectorstock.com/ http://www.vecteezy.com/ http://qvectors.net/ http://www.freevectors.com/ http://www.uberpiglet.com/ http://dezignus.com/category/vector/ http://www.123freevectors.com/ http://coolvectors.com/ http://vector.t","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://shuiyujie.com/tags/网站/"},{"name":"图片素材","slug":"图片素材","permalink":"http://shuiyujie.com/tags/图片素材/"}]},{"title":"【Darknet】改进版Darknet","slug":"Deeplearning/【Darknet】改进版Darknet","date":"2019-06-13T13:19:45.000Z","updated":"2019-09-30T14:47:04.060Z","comments":true,"path":"post/3054b634.html","link":"","permalink":"http://shuiyujie.com/post/3054b634.html","excerpt":"AlexeyAB/darknet 在原版darknet的基础上做了一些改进，用得最舒服的两个点是：可以实时查看 loss 和 mAP；可以自动计算一些模型评估指标。 下面是对其 README.md 的翻译。","text":"AlexeyAB/darknet 在原版darknet的基础上做了一些改进，用得最舒服的两个点是：可以实时查看 loss 和 mAP；可以自动计算一些模型评估指标。 下面是对其 README.md 的翻译。 Requirements (and how to install dependecies) Pre-trained models Explanations in issues Yolo v3 in other frameworks (TensorRT, TensorFlow, PyTorch, OpenVINO, OpenCV-dnn,…) Datasets Improvements in this repository How to use How to compile on Linux How to compile on Windows Using vcpkg Legacy way How to train (Pascal VOC Data) How to train with multi-GPU: How to train (to detect your custom objects) How to train tiny-yolo (to detect your custom objects) When should I stop training How to calculate mAP on PascalVOC 2007 How to improve object detection How to mark bounded boxes of objects and create annotation files How to use Yolo as DLL and SO libraries 如何训练自定义的数据集使用 yolo，yolov2 的配置文件 yolov2-voc.cfg, yolov2-tiny-voc.cfg, yolo-voc.cfg, yolo-voc.2.0.cfg 进行训练 click by the link ##Yolo v3 Yolo v3 训练步骤： copy yolov3.cfg to yolo-obj.cfg and: change batch = 64 change subdivision = 8 change max_batch to (classes*2000), eg 比如 3 类，则 max_batches = 6000 max_batches 设置为 max_batch 的 80% 和 90%，接上面的则 step = 4800,5400 修改 [yolo] - layers 的 classes = 3 修改 [convolutions] 的 filters = (classes + 5)x3 在 build\\darknet\\x64\\data\\ 目录下创建 obj.names,存放检测目标的名称 在 build\\darknet\\x64\\data\\ 目录下创建 obj.data classes= 2 train = data/train.txt valid = data/test.txt names = data/obj.names backup = backup/ build\\darknet\\x64\\data\\obj\\ 放置图片,并且将 yolo 格式的 txt 文件和图片放在一起 在 build\\darknet\\x64\\data\\ 放置 train.txt, val.txt, test.txt 用来存放训练集, 验证集和测试集的图片列表 可以用相对路径也可以用绝对路径 更好的做法是将数据集统一放在一个文件夹下,可以用软连接的形式放到 data 目录下,或者直接用绝对路径 在 build\\darknet\\x64 放置供卷积层使用的预训练文件,点击下载 使用命令进行训练 ./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74 每迭代 100 次 yolo-obj_last.weights 会被保存到 build\\darknet\\x64\\backup\\ (在前面 obj.data 中可以设定) 每迭代 1000 次 yolo-obj_xxxx.weights 会被保存到 build\\darknet\\x64\\backup\\ darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show 可以禁止显示 Loss-Window,比如说我在远程主机上训练不想显示 loss 窗口 不通过 GUI 而是通过浏览器来查看 mAP 和 Loss-chart 可以使用 ./darknet detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map.接着在 http://ip-address:8090查看 训练的时候每 4 个 epoch 计算一次 mAP,需要在 obj.data 中设置好 valid=valid.txtor train.txt,然后使用命令 darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map 训练结束之后可以在 build\\darknet\\x64\\backup\\ 找到最终的权重文件 yolo-obj_final.weights 每迭代 100 次都可以停止训练,之后再重新开始训练, 比如说 2000 次迭代之后暂停并重新开始 bdarknet.exe detector train data/obj.data yolo-obj.cfg backup\\yolo-obj_2000.weights darknet 的原始项目中权重文件每 10 000 次迭代才会保存一次(iteration &gt; 1000) 当然也会保存所有之前的中间结果 Note 如果在训练中看到 avg(loss) 字段出现了 nan 表示训练出现了问题;如果其他字段出现 nan 训练依然正常 如果在 cfg-file 中修改 width= 或者 height=,它们必须要是 32 的倍数 测试目标检测效果可以使用 darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights 如果出现 Out of memory 的错误,可以修改 cfg-file 增大 subdivisions=16, 32 or 64 Yolov3 tiny训练步骤和 Yolov3 类似，下面几个地方有区别： 下载 yolov3-tiny 的 weights 文件: https://pjreddie.com/media/files/yolov3-tiny.weights 获取预训练权值文件 yolov3-tiny.conv.15: darknet.exe partial cfg/yolov3-tiny.cfg yolov3-tiny.weights yolov3-tiny.conv.15 15 更多 yolo 系列的模型可以使用 (DenseNet201-Yoloor ResNet50-Yolo), 预训练模型可以从这里获取: https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmdIf 。 多 GPU 训练多 GPU 训练，在迭代1000以上之后： 1darknet.exe detector train cfg/voc.data cfg/yolov3-voc.cfg /backup/yolov3-voc_1000.weights -gpus 0,1,2,3 什么时候停止训练一般来说每个类迭代 2000 就足够了，但是总的迭代次数不应少于 4000 次。但是如果想要更深入了解什么时候停止训练，请看下面这个手册： 在训练期间，你会看到很多的错误提示，当看到 0.XXXXXXX avg 不再增长的时候就应该停止训练 1Region Avg IOU: 0.798363, Class: 0.893232, Obj: 0.700808, No Obj: 0.004567, Avg Recall: 1.000000, count: 8 Region Avg IOU: 0.800677, Class: 0.892181, Obj: 0.701590, No Obj: 0.004574, Avg Recall: 1.000000, count: 8 当你看到 average loss 0.xxxxxx 在多个 iterations 中都不再减小时，你就应该停止训练。avgerage loss 最终一般在 0.05(小模型，简单数据集) 到 3.0(大模型，复杂数据集) 之间 训练停止之后，你应该从 darknet\\build\\darknet\\x64\\backup 中选择出 last .weights 中表现最好的那个 比如说，你在迭代 9000 次时停止训练，但是最好的训练结果可能是 9000 之前的（7000，8000，9000）某一个 weight 文件。这是由过拟合(overfitting)导致的，过拟合指的是你可以在训练集的数据上表现良好，但是在测试集上表现很糟。所以，你应该使用过拟合之前的 weight 文件，也就是 Early stopping point 之前。 如何获取在 Early Stopping Point 之前的 weights？ 首先需要在 obj.data 中设置好 valid = valid.txt (验证集图片的路径列表文件，如果没有验证集就使用训练集的文件改名为 vaild.txt) 如果在 9000 次迭代之后停止训练，那么就是使用 9000 次之前的几个 weights 进行验证 darknet.exe detector map data/obj.data yolo-obj.cfg backup\\yolo-obj_7000.weights darknet.exe detector map data/obj.data yolo-obj.cfg backup\\yolo-obj_8000.weights darknet.exe detector map data/obj.data yolo-obj.cfg backup\\yolo-obj_9000.weights 接着对比每一个 weight(7000,8000,9000)最后一行输出 选择 mAP (mean average precision) 或者 IoU (intersect over union) 高的 比如说 yolo-obj_8000.weights 的 mAP 最高，就使用这个 weight 进行检测 或者使用 -mAP 参数进行训练 darknet.exe detector train data/obj.data yolo-obj.cfg darknet53.conv.74 -map，那么就会在 Loss-chart 中看到 mAP-chart mAP 每 4 个 Epochs 会使用 valid=valid.txt 文件（1 Epoch = images_in_train_txt / batchiterations） 可以用通过修改 max_batches= 来改变 x 轴坐标的最大值，一般来说 max_batches=2000*classes 测试1234# 单张图测试./darknet detector test data/obj.data yolo-obj.cfg backup/yolo-obj_8000.weights# 计算 mAP./darknet detector map data/obj.data yolo-obj.cfg backup/yolo-obj_7000.weights","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"darknet","slug":"darknet","permalink":"http://shuiyujie.com/tags/darknet/"}]},{"title":"【Darknet】Darknet实战","slug":"Deeplearning/【Darknet】Darknet实战","date":"2019-06-13T13:09:45.000Z","updated":"2019-09-30T14:47:34.139Z","comments":true,"path":"post/5ba44047.html","link":"","permalink":"http://shuiyujie.com/post/5ba44047.html","excerpt":"Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation. —— https://pjreddie.com/darknet/ 本文是对使用 darknet 进行目标检测的小结，包括： 数据集准备：如何使用 labelimage 对数据进行标注，注意事项，文件格式转换 darknet 使用：如何编译和修改配置文件 模型评估：如何查看 loss、计算 IoU、recall、mAP","text":"Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation. —— https://pjreddie.com/darknet/ 本文是对使用 darknet 进行目标检测的小结，包括： 数据集准备：如何使用 labelimage 对数据进行标注，注意事项，文件格式转换 darknet 使用：如何编译和修改配置文件 模型评估：如何查看 loss、计算 IoU、recall、mAP 数据集准备 大多数情况下，数据集决定了任务的成败。一开始我认为标注数据是一件非常枯燥和乏味的事情，但是当模型指标一直上不去，检测和识别效果也一直不好时，回过头才会发现是因为数据标注的有问题。 这只有自己经历过之后才会有体会，得出这样几条经验： 保持类间差距大，类内差距小 一开始先标注少量的图片并训练模型查看效果，根据结果进行调整，否则等到标注了大量图片之后再回过头修改，得不偿失。 labelimage图片标注使用的是开源的工具 LabelImg，可以查看文档自行编译，下载之后的文件结构如下： 1234567.├── build-tools├── CONTRIBUTING.rst├── data |—— predefined_classes.txt├── labelImg.py... 在data/predefined_classes.txt文件配置进行图片的类别 界面展示及注意事项启动 labelimg 之后的界面如下图所示 注意： 存放图片文件夹和存放标记文件的文件夹需要保持一致 正确选择标记文件的格式，是需要 yolo 格式还是 xml 格式，默认为 xml 格式 右侧可以选择默认标签，当密集标注一个类的时候很实用 打完几张标签之后请确认标签是否正确，再去文件目录下确认以下标记文件是否生成且格式正确 快捷键123456789+------------+--------------------------------------------+| Space | 保存 |+------------+--------------------------------------------+| w | 创建矩形框 |+------------+--------------------------------------------+| d | 下一张图片 |+------------+--------------------------------------------+| a | 上一张图片 |+------------+--------------------------------------------+ 最常用的快捷键是上面这 4 个，用好快捷键可以调高打标效率。此外按住ctrl+鼠标滚轮可以调整图片大小，局部放大图片可以提高打标的精准度。 yolo格式和 xml 格式转换xml2yolo.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758\"\"\"1. 修改 classes 列表中的元素为当前标签列表2. 修改 list_xml 指向的 xml 文件保存的位置\"\"\"#import xml.etree.ElementTree as ETfrom xml.etree import ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinimport globclasses = ['rabbit']def convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = (box[0] + box[1]) / 2.0 - 1 y = (box[2] + box[3]) / 2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h)def convert_annotation(xml_file, txt_file): in_file = open(xml_file) tree = ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): cls = obj.find('name').text if cls not in classes: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w, h), b) txt_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')list_xml = []list_xml = glob.glob(r\"/file_path/*.xml\")for xml_file in list_xml: xml_name = xml_file.split('.')[0] txt_file = open('%s.txt' % (xml_name), 'w') convert_annotation(xml_file, txt_file) txt_file.close() yolo2xml.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123\"\"\"1. 修改 class_name2. 修改 src_img_dir/src_txt_dir/src_xml_dir 路径\"\"\"import os, sysimport globfrom PIL import Imageclass_name = ['switch_closed','switch_open','whirled_switch_closed','whirled_switch_open', 'lifting_switch_closed','lifting_switch_open','closure_switch_closed','closure_switch_open', 'color_switch01_red','color_switch01_green','color_switch02_red','color_switch02_green', 'isolation_switch_closed','isolation_switch_open','grounding_knife_switch01_closed','grounding_knife_switch01_open', 'grounding_knife_switch02_closed','grounding_knife_switch02_open']def convert_yolo_coordinates_to_voc(x_c_n, y_c_n, width_n, height_n, img_width, img_height): ## remove normalization given the size of the image x_c = float(x_c_n) * img_width y_c = float(y_c_n) * img_height width = float(width_n) * img_width height = float(height_n) * img_height ## compute half width and half height half_width = width / 2 half_height = height / 2 ## compute left, top, right, bottom ## in the official VOC challenge the top-left pixel in the image has coordinates (1;1) left = int(x_c - half_width) + 1 top = int(y_c - half_height) + 1 right = int(x_c + half_width) + 1 bottom = int(y_c + half_height) + 1 return left, top, right, bottom# 将标注的txt转换为 voc xml# VEDAI 图像存储位置src_img_dir = \"./switch_mAP\"# VEDAI 图像的 ground truth 的 txt 文件存放位置src_txt_dir = \"./switch_mAP\"src_xml_dir = \"./switch_mAP\"img_Lists = glob.glob(src_img_dir + '/*.txt')# 文件名(含扩展名)img_basenames = [] # e.g. 100for item in img_Lists: img_basenames.append(os.path.basename(item))img_names = [] # e.g. 100for item in img_basenames: # 文件名与扩展名 temp1, temp2 = os.path.splitext(item) img_names.append(temp1)for img in img_names: im = \"\" suffix = \"\" # 同一个文件夹下存在多种图片格式，在这里加上格式判断 if os.path.exists(src_img_dir + '/' + img + '.jpg'): im = Image.open((src_img_dir + '/' + img + '.jpg')) suffix = '.jpg' elif os.path.exists(src_img_dir + '/' + img + '.JPG'): im = Image.open((src_img_dir + '/' + img + '.JPG')) suffix = '.JPG' elif os.path.exists(src_img_dir + '/' + img + '.png'): im = Image.open((src_img_dir + '/' + img + '.png')) suffix = '.png' elif os.path.exists(src_img_dir + '/' + img + '.PNG'): im = Image.open((src_img_dir + '/' + img + '.PNG')) suffix = '.PNG' elif os.path.exists(src_img_dir + '/' + img + '.JPEG'): im = Image.open((src_img_dir + '/' + img + '.JPEG')) suffix = '.JPEG' elif os.path.exists(src_img_dir + '/' + img + '.jpeg'): im = Image.open((src_img_dir + '/' + img + '.jpeg')) suffix = '.jpeg' width, height = im.size \"\"\" 以下部分为 xml 解析 \"\"\" # open the crospronding txt file # 提取每一行并分割 gt = open(src_txt_dir + '/' + img + '.txt').read().splitlines() print(img + '\\n') # write in xml file # os.mknod(src_xml_dir + '/' + img + '.xml') xml_file = open((src_xml_dir + '/' + img + '.xml'), 'a') xml_file.write('&lt;annotation&gt;\\n') xml_file.write(' &lt;folder&gt;VOC2007&lt;/folder&gt;\\n') xml_file.write(' &lt;filename&gt;' + str(img) + suffix + '&lt;/filename&gt;\\n') xml_file.write(' &lt;size&gt;\\n') xml_file.write(' &lt;width&gt;' + str(width) + '&lt;/width&gt;\\n') xml_file.write(' &lt;height&gt;' + str(height) + '&lt;/height&gt;\\n') xml_file.write(' &lt;depth&gt;3&lt;/depth&gt;\\n') xml_file.write(' &lt;/size&gt;\\n') # write the region of image on xml file for img_each_label in gt: spt = img_each_label.split(' ') # 这里如果txt里面是以逗号‘，’隔开的，那么就改为spt = img_each_label.split(',')。 name = class_name[int(spt[0])] x_c,y_c,width_n,height_n = spt[1:] xmin,ymin,xmax,ymax = convert_yolo_coordinates_to_voc(x_c,y_c,width_n,height_n,width,height) xml_file.write(' &lt;object&gt;\\n') xml_file.write(' &lt;name&gt;' + name + '&lt;/name&gt;\\n') xml_file.write(' &lt;pose&gt;Unspecified&lt;/pose&gt;\\n') xml_file.write(' &lt;truncated&gt;0&lt;/truncated&gt;\\n') xml_file.write(' &lt;difficult&gt;0&lt;/difficult&gt;\\n') xml_file.write(' &lt;bndbox&gt;\\n') xml_file.write(' &lt;xmin&gt;' + str(xmin) + '&lt;/xmin&gt;\\n') xml_file.write(' &lt;ymin&gt;' + str(ymin) + '&lt;/ymin&gt;\\n') xml_file.write(' &lt;xmax&gt;' + str(xmax) + '&lt;/xmax&gt;\\n') xml_file.write(' &lt;ymax&gt;' + str(ymax) + '&lt;/ymax&gt;\\n') xml_file.write(' &lt;/bndbox&gt;\\n') xml_file.write(' &lt;/object&gt;\\n') xml_file.write('&lt;/annotation&gt;') 使用 darknet 训练模型安装和编译12# 从 Github 下载git clone https://github.com/pjreddie/darknet 进入darknet目录中，对编译文件Makefile进行如下修改 12345GPU=1CUDNN=1OPENCV=0OPENMP=0DEBUG=0 注: 画图或显示图片等操作需要配置OPENCV并设置为1 需要多线程相关操作需要将OPENMP设置为1 修改完成保存并执行make命令。 目录结构介绍12345678910111213.├── backup├── cfg |—— voc.data |—— yolov3-voc.cfg├── darknet├── data |—— voc.names |—— train.txt |—— val.txt |—— test.txt├── scripts... 编译完成之后，我们关注目录中的这几个文件和文件夹。 backup 存放训练出来的权值文件 cfg 保存配置文件，其中两个文件，在后面介绍 darknet 是可执行文件 scripts 下是一些脚本 data/voc.names 存放类别标签 train.txt 保存用于训练的图片全路径(自建，位置无特殊要求) val.txt 保存用于校正的图片全路径(自建，位置无特殊要求) test.txt 保存用于校正的图片全路径(自建，位置无特殊要求) 注: train.txt, val.txt, test.txt 中图片数量比例建议为 8:1:1 修改配置文件类别文件 voc.names修改data/voc.names，将我们的类别标签写这个文件，比如说有以下 5 类 12345dogcatmagpiepigeonnest 注意： 类别标签要与训练集包含的图片类别一一对应，训练集中有以上 5 类则voc.names包含以上 5 类 类别标签需要连续，如果不连续就必须要修改类别标签改成连续的 配置 voc.data修改cfg/voc.data文件 12345classes= class_numbertrain = /path/train.txtvalid = /path/val.txtnames = data/voc.namesbackup = backup classes 配置类别数量，与上一步voc.names中类别数量一致 train 配置为目录结构一章介绍的train.txt文件的位置 vaild 配置为目录结构一章介绍的val.txt文件的位置 names 配置为voc.names位置(默认不变即可) backup 配置为权值文件的位置(默认不变即可) 配置 yolov3-voc.cfg在文件开头位置 123456789101112131415[net]# Testing# batch=1 # 测试时开启，训练时关闭# subdivisions=1 # 测试时开启，训练时关闭# Trainingbatch=6 # 训练时开启，测试时关闭subdivisions=2 # 训练时开启，测试时关闭 ...learning_rate=0.0001 # 学习率，可以调整得小一点burn_in=1000max_batches = 50200policy=stepssteps=40000,45000scales=.1,.1 batch 和 subdivisions 在测试和训练的时候请按照上面注释开启或者关闭 batch 一批处理几张图片，没有超过显存的情况下越大越好 subdivisions 表示在 batch 中再划分的数量 当前配置的意思是每轮迭代从所有训练集中抽取 6 张图片，这 6 张样本图片又被分成 2 次，每次 3 张送入到网络参与训练。 接着在文件中搜索yolo，会有三条结果。每个yolo上下都要修改filters和classes，总共需要修改3 组共 6 个字段。 12345678910111213141516[convolutional]size=1stride=1pad=1filters=30 # 3*(classes+5)activation=linear[yolo]mask = 6,7,8anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326classes=5 # 类别(classes)数量num=9jitter=.3ignore_thresh = .5truth_thresh = 1random=1 训练方法一1./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg scripts/darknet53.conv.74 -gpus 0 方法二1nohup ./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg scripts/darknet53.conv.74 -gpus 0 &gt;train.log 2&gt;&amp;1 &amp; 我们需要保留训练的日志，所以用方法二更好。 查看 GPU 使用情况我们可以使用nvidia-smi来查看 gpu 的使用情况，注意先查看 gpu 的使用情况。 12345678910111213141516171819202122Fri Mar 29 16:33:31 2019 +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.78 Driver Version: 410.78 CUDA Version: 10.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 108... Off | 00000000:01:00.0 On | N/A || 59% 87C P2 221W / 250W | 4874MiB / 11175MiB | 98% Default |+-------------------------------+----------------------+----------------------+| 1 GeForce GTX 108... Off | 00000000:02:00.0 Off | N/A || 24% 42C P8 16W / 250W | 5107MiB / 11178MiB | 0% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1218 G /usr/lib/xorg/Xorg 69MiB || 0 24338 C ./darknet 4793MiB || 1 12712 C ./bin/psd_be 5095MiB |+-----------------------------------------------------------------------------+ 用nvidia-smi输出的是静态的信息，如果想要动态查看 gpu 的使用情况可以使用watch -n 1 nvidia-smi表示每一秒刷新一个 gpu 的使用情况。 查看日志输出信息123450181: 0.084739, 0.367084 avg, 0.000001 rate, 0.244283 seconds, 301086 imagesLoaded: 0.000030 secondsRegion 82 Avg IOU: 0.775331, Class: 0.611054, Obj: 0.963920, No Obj: 0.005009, .5R: 1.000000, .75R: 0.833333, count: 6Region 94 Avg IOU: 0.753700, Class: 0.962260, Obj: 0.777823, No Obj: 0.000218, .5R: 1.000000, .75R: 0.500000, count: 2 50181 表示迭代次数 0.084739 表示整体 Loss 0.367084 avg 表示平均 Loss 0.000001 rate 表示学习率，对应.cfg文件中的learning_rate 0.244283 seconds 表示当前批次训练花费了多少时间 301086 images 表示目前已经训练了多少照片 判断训练没有异常的标准 IOU 表示预测目标与真实目标的交集与并集之比，越接近于 1 越好，出现大量 -nan 表示训练异常 Class: 0.611054 表示标记物体的正确率，越接近于 1 越好 Obj: 0.963920 越接近 1 越好 No Obj 0.005009 越接近 0 越好 当训练整体趋势按照上面描述的一样向好的方向发展就表示训练正常，反之表示训练异常。 测试1./darknet detector test cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_50000.weights 1 注意：测试时，修改cfg/yolov3-voc.cfg配置文件，请参照之前配置 yolov3-voc.cfg这一节。 其他 训练集中各个类别的数量要尽可能保持均衡，不要有的类特别多或者有的特别少 train.txt 文件中配置了训练用的图片，这些图片必须要有对应的txt文件，且txt文件不可以为空 train.txt 文件不可以有空行，存在空行会导致训练失败 开始新的训练注意删除backup中原先的.weight文件 如何评估模型的效果darknet 编译格式123456./darknet detector test &lt;data_cfg&gt; &lt;models_cfg&gt; &lt;weights&gt; &lt;test_file&gt; [-thresh] [-out]./darknet detector train &lt;data_cfg&gt; &lt;models_cfg&gt; &lt;weights&gt; [-thresh] [-gpu] [-gpus] [-clear]./darknet detector valid &lt;data_cfg&gt; &lt;models_cfg&gt; &lt;weights&gt; [-out] [-thresh]./darknet detector recall &lt;data_cfg&gt; &lt;models_cfg&gt; &lt;weights&gt; [-thresh]'&lt;&gt;'必选项，’[ ]‘可选项 data_cfg：数据配置文件，eg：cfg/voc.data models_cfg：模型配置文件，eg：cfg/yolov3-voc.cfg weights：权重配置文件，eg：weights/yolov3.weights test_file：测试文件，eg：//*/test.txt -thresh：显示被检测物体中confidence大于等于 [-thresh] 的bounding-box，默认0.005 -out：输出文件名称，默认路径为results文件夹下，eg：-out “” //输出class_num个文件，文件名为class_name.txt；若不选择此选项，则默认输出文件名为comp4_det_test_”class_name”.txt -i/-gpu：指定单个gpu，默认为0，eg：-gpu 2 -gpus：指定多个gpu，默认为0，eg：-gpus 0,1,2 根据训练日志生成 loss-iter 曲线使用 drawcurve.py 解析训练日志，据此生成 loss-iter 曲线。该脚本通过训练日志计算 loss,不过训练日志格式可能会有区别,可能需要自己修改脚本来适应. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# coding: utf-8import argparseimport sysimport matplotlib.pyplot as pltdef main(argv): parser = argparse.ArgumentParser() parser.add_argument(\"log_file\", help = \"path to log file\" ) parser.add_argument( \"option\", help = \"0 -&gt; loss vs iter\" ) args = parser.parse_args() f = open(args.log_file) lines = [line.rstrip(\"\\n\") for line in f.readlines()] # skip the first 3 lines lines = lines[3:] numbers = &#123;'1','2','3','4','5','6','7','8','9', '0'&#125; iters = [] loss = [] for line in lines: print(line) #跳过空行 if not len(line): continue if line[0] in numbers: args = line.split(\" \") # print(args) if len(args) &gt; 4 and is_number(args[2]): iters.append(int(args[0][:-1])) loss.append(float(args[2])) plt.plot(iters,loss) plt.xlabel('iters') plt.ylabel('loss') plt.grid() plt.show()# 0.692735 seconds, 595200 imagesdef is_number(s): try: float(s) return True except ValueError: pass try: import unicodedata unicodedata.numeric(s) return True except (TypeError, ValueError): pass return Falseif __name__ == \"__main__\": main(sys.argv) 生成预测结果通过./darknet detector valid &lt;data_cfg&gt; &lt;models_cfg&gt; &lt;weights&gt; 可以批量生成模型的测试结果,测试结果保存在results目录下面,按照类别分成一个个文件. 12#!/bin/bash./darknet detector valid switch_18/switch.data switch_18/switch.cfg switch_18/switch.weights -i 1 -out \"\" 结果生成在 &lt;data_cfg&gt; 的指定的目录下以 &lt;out_file&gt; 开头的若干文件中，若&lt;data_cfg&gt;没有指定results，那么默认为&lt;darknet_root&gt;/results； &lt;models_cfg&gt; 文件中 batch 和 subdivisions 两项必须为1； 若 -out 未指定字符串，则在 results 文件夹下生成 comp4_det_test_[类名].txt 文件并保存测试结果； 本次实验在 results 文件夹下生成 [类名].txt 文件； 统计召回率 recall12#!/bin/bash./darknet detector recall switch_18/switch.data switch_18/switch.cfg switch_18/switch.weights 输出结果为 1234567891011121314151617835 35 45 RPs/Img: 1.81 IOU: 57.69% Recall:77.78%836 35 45 RPs/Img: 1.81 IOU: 57.69% Recall:77.78%837 35 45 RPs/Img: 1.81 IOU: 57.69% Recall:77.78%838 35 45 RPs/Img: 1.81 IOU: 57.69% Recall:77.78%839 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%840 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%841 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%842 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%843 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%844 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%845 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%846 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%847 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%848 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%849 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%850 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78%851 35 45 RPs/Img: 1.82 IOU: 57.69% Recall:77.78% 数据格式为Number Correct Total Rps/Img IOU Recall Number表示处理到第几张图片。 Correct 表示正确的识别除了多少bbox。这个值算出来的步骤是这样的，丢进网络一张图片，网络会预测出很多 bbox，每个bbox都有其置信概率，概率大于threshold的bbox与实际的bbox，也就是labels中txt的内容计算IOU，找出IOU最大的bbox，如果这个最大值大于预设的IOU的threshold，那么correct加一。 Total表示实际有多少个bbox。 Rps/img表示平均每个图片会预测出来多少个bbox。 IOU： 这个是预测出的bbox和实际标注的bbox的交集 除以 他们的并集。显然，这个数值越大，说明预测的结果越好。 Recall召回率， 意思是检测出物体的个数 除以 标注的所有物体个数。通过代码我们也能看出来就是Correct除以Total的值。 计算的是全部图片的 recall 和 IOU,而不是单类的,不是很方便. 计算 mAPreval_voc.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#!/usr/bin/env python# Adapt from -&gt;# --------------------------------------------------------# Fast R-CNN# Copyright (c) 2015 Microsoft# Licensed under The MIT License [see LICENSE for details]# Written by Ross Girshick# --------------------------------------------------------# &lt;- Written by Yaping Sun\"\"\"Reval = re-eval. Re-evaluate saved detections.\"\"\"import os, sys, argparseimport numpy as npimport cPicklefrom voc_eval import voc_evaldef parse_args(): \"\"\" Parse input arguments \"\"\" parser = argparse.ArgumentParser(description='Re-evaluate results') parser.add_argument('output_dir', nargs=1, help='results directory', type=str) parser.add_argument('--voc_dir', dest='voc_dir', default='data/VOCdevkit', type=str) parser.add_argument('--year', dest='year', default='2017', type=str) parser.add_argument('--image_set', dest='image_set', default='test', type=str) parser.add_argument('--classes', dest='class_file', default='data/voc.names', type=str) if len(sys.argv) == 1: parser.print_help() sys.exit(1) args = parser.parse_args() return argsdef get_voc_results_file_template(image_set, out_dir = 'results'): filename = 'comp4_det_' + image_set + '_&#123;:s&#125;.txt' path = os.path.join(out_dir, filename) return pathdef do_python_eval(devkit_path, year, image_set, classes, output_dir = 'results'): annopath = os.path.join( devkit_path, 'VOC' + year, 'Annotations', '&#123;:s&#125;.xml') imagesetfile = os.path.join( devkit_path, 'VOC' + year, 'ImageSets', 'Main', image_set + '.txt') cachedir = os.path.join(devkit_path, 'annotations_cache') aps = [] # The PASCAL VOC metric changed in 2010 use_07_metric = True if int(year) &lt; 2010 else False print('VOC07 metric? ' + ('Yes' if use_07_metric else 'No')) if not os.path.isdir(output_dir): os.mkdir(output_dir) for i, cls in enumerate(classes): if cls == '__background__': continue filename = get_voc_results_file_template(image_set).format(cls) rec, prec, ap = voc_eval( filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5, use_07_metric=use_07_metric) aps += [ap] print('AP for &#123;&#125; = &#123;:.4f&#125;'.format(cls, ap)) with open(os.path.join(output_dir, cls + '_pr.pkl'), 'w') as f: cPickle.dump(&#123;'rec': rec, 'prec': prec, 'ap': ap&#125;, f) print('Mean AP = &#123;:.4f&#125;'.format(np.mean(aps))) print('~~~~~~~~') print('Results:') for ap in aps: print('&#123;:.3f&#125;'.format(ap)) print('&#123;:.3f&#125;'.format(np.mean(aps))) print('~~~~~~~~') print('') print('--------------------------------------------------------------') print('Results computed with the **unofficial** Python eval code.') print('Results should be very close to the official MATLAB eval code.') print('-- Thanks, The Management') print('--------------------------------------------------------------')if __name__ == '__main__': args = parse_args() output_dir = os.path.abspath(args.output_dir[0]) with open(args.class_file, 'r') as f: lines = f.readlines() classes = [t.strip('\\n') for t in lines] print('Evaluating detections') do_python_eval(args.voc_dir, args.year, args.image_set, classes, output_dir) voc_eval.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211# --------------------------------------------------------# Fast/er R-CNN# Licensed under The MIT License [see LICENSE for details]# Written by Bharath Hariharan# --------------------------------------------------------import xml.etree.ElementTree as ETimport osimport pickleimport numpy as npdef parse_rec(filename): \"\"\" Parse a PASCAL VOC xml file \"\"\" tree = ET.parse(filename) objects = [] for obj in tree.findall('object'): obj_struct = &#123;&#125; obj_struct['name'] = obj.find('name').text obj_struct['pose'] = obj.find('pose').text obj_struct['truncated'] = int(obj.find('truncated').text) obj_struct['difficult'] = int(obj.find('difficult').text) bbox = obj.find('bndbox') obj_struct['bbox'] = [int(bbox.find('xmin').text), int(bbox.find('ymin').text), int(bbox.find('xmax').text), int(bbox.find('ymax').text)] objects.append(obj_struct) return objectsdef voc_ap(rec, prec, use_07_metric=False): \"\"\" ap = voc_ap(rec, prec, [use_07_metric]) Compute VOC AP given precision and recall. If use_07_metric is true, uses the VOC 07 11 point method (default:False). \"\"\" if use_07_metric: # 11 point metric ap = 0. for t in np.arange(0., 1.1, 0.1): if np.sum(rec &gt;= t) == 0: p = 0 else: p = np.max(prec[rec &gt;= t]) ap = ap + p / 11. else: # correct AP calculation # first append sentinel values at the end mrec = np.concatenate(([0.], rec, [1.])) mpre = np.concatenate(([0.], prec, [0.])) # compute the precision envelope for i in range(mpre.size - 1, 0, -1): mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i]) # to calculate area under PR curve, look for points # where X axis (recall) changes value i = np.where(mrec[1:] != mrec[:-1])[0] # and sum (\\Delta recall) * prec ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1]) return apdef voc_eval(detpath, annopath, imagesetfile, classname, cachedir, ovthresh=0.5, use_07_metric=False): \"\"\"rec, prec, ap = voc_eval(detpath, annopath, imagesetfile, classname, [ovthresh], [use_07_metric]) Top level function that does the PASCAL VOC evaluation. detpath: Path to detections detpath.format(classname) should produce the detection results file. annopath: Path to annotations annopath.format(imagename) should be the xml annotations file. imagesetfile: Text file containing the list of images, one image per line. classname: Category name (duh) cachedir: Directory for caching the annotations [ovthresh]: Overlap threshold (default = 0.5) [use_07_metric]: Whether to use VOC07's 11 point AP computation (default False) \"\"\" # assumes detections are in detpath.format(classname) # assumes annotations are in annopath.format(imagename) # assumes imagesetfile is a text file with each line an image name # cachedir caches the annotations in a pickle file # first load gt if not os.path.isdir(cachedir): os.mkdir(cachedir) cachefile = os.path.join(cachedir, 'annots.pkl') # read list of images with open(imagesetfile, 'r') as f: lines = f.readlines() imagenames = [x.strip() for x in lines] if not os.path.isfile(cachefile): # load annots recs = &#123;&#125; for i, imagename in enumerate(imagenames): recs[imagename] = parse_rec(annopath.format(imagename)) if i % 100 == 0: print ('Reading annotation for &#123;:d&#125;/&#123;:d&#125;'.format( i + 1, len(imagenames))) # save print ('Saving cached annotations to &#123;:s&#125;'.format(cachefile)) with open(cachefile, 'wb') as f: pickle.dump(recs, f) else: # load with open(cachefile, 'rb') as f: recs = pickle.load(f) # extract gt objects for this class class_recs = &#123;&#125; npos = 0 for imagename in imagenames: R = [obj for obj in recs[imagename] if obj['name'] == classname] difficult = np.array([x['difficult'] for x in R]).astype(np.bool) bbox = np.array([x['bbox'] for x in R]) #difficult = np.array([x['difficult'] for x in R]).astype(np.bool) det = [False] * len(R) npos = npos + sum(~difficult) class_recs[imagename] = &#123;'bbox': bbox, #'difficult': difficult, 'det': det&#125; # read dets detfile = detpath.format(classname) with open(detfile, 'r') as f: lines = f.readlines() splitlines = [x.strip().split(' ') for x in lines] image_ids = [x[0] for x in splitlines] confidence = np.array([float(x[1]) for x in splitlines]) BB = np.array([[float(z) for z in x[2:]] for x in splitlines]) # sort by confidence sorted_ind = np.argsort(-confidence) sorted_scores = np.sort(-confidence) BB = BB[sorted_ind, :] image_ids = [image_ids[x] for x in sorted_ind] # go down dets and mark TPs and FPs nd = len(image_ids) tp = np.zeros(nd) fp = np.zeros(nd) for d in range(nd): R = class_recs[image_ids[d]] bb = BB[d, :].astype(float) ovmax = -np.inf BBGT = R['bbox'].astype(float) if BBGT.size &gt; 0: # compute overlaps # intersection ixmin = np.maximum(BBGT[:, 0], bb[0]) iymin = np.maximum(BBGT[:, 1], bb[1]) ixmax = np.minimum(BBGT[:, 2], bb[2]) iymax = np.minimum(BBGT[:, 3], bb[3]) iw = np.maximum(ixmax - ixmin + 1., 0.) ih = np.maximum(iymax - iymin + 1., 0.) inters = iw * ih # union uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + (BBGT[:, 2] - BBGT[:, 0] + 1.) * (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters) overlaps = inters / uni ovmax = np.max(overlaps) jmax = np.argmax(overlaps) \"\"\" if ovmax &gt; ovthresh: if not False: if not R['det'][jmax]: tp[d] = 1. R['det'][jmax] = 1 else: fp[d] = 1. else: fp[d] = 1. \"\"\" if ovmax &gt; ovthresh: if not False: if not R['det'][jmax]: tp[d] = 1. R['det'][jmax] = 1 else: fp[d] = 1. else: fp[d] = 1. # compute precision recall fp = np.cumsum(fp) tp = np.cumsum(tp) rec = tp / float(npos) # avoid divide by zero in case the first detection matches a difficult # ground truth prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps) ap = voc_ap(rec, prec, use_07_metric) return rec, prec, ap computer_Single_ALL_mAP.py results_path = “填写前面生成的 results 文件的路径” /xxx/results/{}.txt - results_path 的路径 /xxx/{}.xml - xml 格式的标注文件 /xxx/list.txt - 只包含图片名字的列表文件 执行 computer_Single_ALL_mAP.py 将会生成各个分类的 mAP. 12345678910111213141516171819202122232425from voc_eval import voc_eval import os current_path = os.getcwd()results_path = \"填写前面生成的 results 文件的路径\"sub_files = os.listdir(results_path) mAP = [] for i in range(len(sub_files)): class_name = sub_files[i].split(\".txt\")[0] rec, prec, ap = voc_eval('/xxx/results/&#123;&#125;.txt', '/xxx/&#123;&#125;.xml', '/xxx/list.txt', class_name, '.') # print(\"&#123;&#125; :\\t &#123;&#125; \".format(class_name, ap)) print(\"&#123;&#125; :\\t &#123;&#125; \".format(class_name, ap)) mAP.append(ap) # class_name = 'switch_open'# rec, prec, ap = voc_eval('/aseit-data/program/darknet/results/&#123;&#125;.txt', '/aseit-data/data_set/swich/dataset_test/switch_mAP/&#123;&#125;.xml', '/aseit-data/data_set/swich/dataset_test/list.txt', class_name, '.')# print(\"&#123;&#125; :\\t &#123;&#125; \".format(class_name, ap)) # mAP.append(ap) mAP = tuple(mAP) print(\"***************************\") print(\"mAP :\\t &#123;&#125;\".format( float( sum(mAP)/len(mAP)) )) detector.c在 test 和计算 recall 的时候需要修改 examples/detector.c 如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include \"darknet.h\"#include &lt;sys/stat.h&gt;#include &lt;stdio.h&gt;#include &lt;time.h&gt;#include &lt;sys/types.h&gt;static int coco_ids[] = &#123;1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,27,28,31,32,33,34,35,36,37,38,39,40,41,42,43,44,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,67,70,72,73,74,75,76,77,78,79,80,81,82,84,85,86,87,88,89,90&#125;;char *GetFilename(char *fullname)&#123; int from,to,i; char *newstr,*temp; if(fullname!=NULL)&#123; //if not find dot if((temp=strchr(fullname,'.'))==NULL)&#123; newstr = fullname; &#125; else &#123; from = strlen(fullname) - strlen(temp); to = (temp-fullname); //the first dot's index for (i=from; i&lt;=to; i--)&#123; if (fullname[i]=='.') break;//find the last dot &#125; newstr = (char*)malloc(i+1); strncpy(newstr,fullname,i); *(newstr+i)=0; &#125; &#125; static char name[50] = &#123;\"\"&#125;; char *q = strrchr(newstr,'/') + 1; strncpy(name,q,40); return name;&#125;void train_detector(char *datacfg, char *cfgfile, char *weightfile, int *gpus, int ngpus, int clear)&#123; list *options = read_data_cfg(datacfg); //解析data文件，用自定义链表options存储训练集基本信息，函数位于option_list.c char *train_images = option_find_str(options, \"train\", \"data/train.list\"); //从options中找训练集 char *backup_directory = option_find_str(options, \"backup\", \"/backup/\"); //从options中找backup路径 srand(time(0)); //初始化随机种子数 char *base = basecfg(cfgfile); //此函数位于utils.c,返回cfg文件不带后缀的名字 printf(\"%s\\n\", base); float avg_loss = -1; network **nets = calloc(ngpus, sizeof(network)); srand(time(0)); int seed = rand(); int i; for(i = 0; i &lt; ngpus; ++i)&#123; srand(seed);#ifdef GPU cuda_set_device(gpus[i]);#endif nets[i] = load_network(cfgfile, weightfile, clear); nets[i]-&gt;learning_rate *= ngpus; &#125; srand(time(0)); network *net = nets[0]; int imgs = net-&gt;batch * net-&gt;subdivisions * ngpus; printf(\"Learning Rate: %g, Momentum: %g, Decay: %g\\n\", net-&gt;learning_rate, net-&gt;momentum, net-&gt;decay); data train, buffer; layer l = net-&gt;layers[net-&gt;n - 1]; int classes = l.classes; float jitter = l.jitter; list *plist = get_paths(train_images); //int N = plist-&gt;size; char **paths = (char **)list_to_array(plist); load_args args = get_base_args(net); args.coords = l.coords; args.paths = paths; args.n = imgs; args.m = plist-&gt;size; args.classes = classes; args.jitter = jitter; args.num_boxes = l.max_boxes; args.d = &amp;buffer; args.type = DETECTION_DATA; //args.type = INSTANCE_DATA; args.threads = 64; pthread_t load_thread = load_data(args); double time; int count = 0; //while(i*imgs &lt; N*120)&#123; while(get_current_batch(net) &lt; net-&gt;max_batches)&#123; if(l.random &amp;&amp; count++%10 == 0)&#123; printf(\"Resizing\\n\"); int dim = (rand() % 10 + 10) * 32; if (get_current_batch(net)+200 &gt; net-&gt;max_batches) dim = 608; //int dim = (rand() % 4 + 16) * 32; printf(\"%d\\n\", dim); args.w = dim; args.h = dim; pthread_join(load_thread, 0); train = buffer; free_data(train); load_thread = load_data(args); #pragma omp parallel for for(i = 0; i &lt; ngpus; ++i)&#123; resize_network(nets[i], dim, dim); &#125; net = nets[0]; &#125; time=what_time_is_it_now(); pthread_join(load_thread, 0); train = buffer; load_thread = load_data(args); /* int k; for(k = 0; k &lt; l.max_boxes; ++k)&#123; box b = float_to_box(train.y.vals[10] + 1 + k*5); if(!b.x) break; printf(\"loaded: %f %f %f %f\\n\", b.x, b.y, b.w, b.h); &#125; */ /* int zz; for(zz = 0; zz &lt; train.X.cols; ++zz)&#123; image im = float_to_image(net-&gt;w, net-&gt;h, 3, train.X.vals[zz]); int k; for(k = 0; k &lt; l.max_boxes; ++k)&#123; box b = float_to_box(train.y.vals[zz] + k*5, 1); printf(\"%f %f %f %f\\n\", b.x, b.y, b.w, b.h); draw_bbox(im, b, 1, 1,0,0); &#125; show_image(im, \"truth11\"); cvWaitKey(0); save_image(im, \"truth11\"); &#125; */ printf(\"Loaded: %lf seconds\\n\", what_time_is_it_now()-time); time=what_time_is_it_now(); float loss = 0;#ifdef GPU if(ngpus == 1)&#123; loss = train_network(net, train); &#125; else &#123; loss = train_networks(nets, ngpus, train, 4); &#125;#else loss = train_network(net, train);#endif if (avg_loss &lt; 0) avg_loss = loss; avg_loss = avg_loss*.9 + loss*.1; i = get_current_batch(net); printf(\"%ld: %f, %f avg, %f rate, %lf seconds, %d images\\n\", get_current_batch(net), loss, avg_loss, get_current_rate(net), what_time_is_it_now()-time, i*imgs); if(i%100==0)&#123;#ifdef GPU if(ngpus != 1) sync_nets(nets, ngpus, 0);#endif char buff[256]; sprintf(buff, \"%s/%s.backup\", backup_directory, base); save_weights(net, buff); &#125; if(i%10000==0 || (i &lt; 1000 &amp;&amp; i%100 == 0))&#123;#ifdef GPU if(ngpus != 1) sync_nets(nets, ngpus, 0);#endif char buff[256]; sprintf(buff, \"%s/%s_%d.weights\", backup_directory, base, i); save_weights(net, buff); &#125; free_data(train); &#125;#ifdef GPU if(ngpus != 1) sync_nets(nets, ngpus, 0);#endif char buff[256]; sprintf(buff, \"%s/%s_final.weights\", backup_directory, base); save_weights(net, buff);&#125;static int get_coco_image_id(char *filename)&#123; char *p = strrchr(filename, '/'); char *c = strrchr(filename, '_'); if(c) p = c; return atoi(p+1);&#125;static void print_cocos(FILE *fp, char *image_path, detection *dets, int num_boxes, int classes, int w, int h)&#123; int i, j; int image_id = get_coco_image_id(image_path); for(i = 0; i &lt; num_boxes; ++i)&#123; float xmin = dets[i].bbox.x - dets[i].bbox.w/2.; float xmax = dets[i].bbox.x + dets[i].bbox.w/2.; float ymin = dets[i].bbox.y - dets[i].bbox.h/2.; float ymax = dets[i].bbox.y + dets[i].bbox.h/2.; if (xmin &lt; 0) xmin = 0; if (ymin &lt; 0) ymin = 0; if (xmax &gt; w) xmax = w; if (ymax &gt; h) ymax = h; float bx = xmin; float by = ymin; float bw = xmax - xmin; float bh = ymax - ymin; for(j = 0; j &lt; classes; ++j)&#123; if (dets[i].prob[j]) fprintf(fp, \"&#123;\\\"image_id\\\":%d, \\\"category_id\\\":%d, \\\"bbox\\\":[%f, %f, %f, %f], \\\"score\\\":%f&#125;,\\n\", image_id, coco_ids[j], bx, by, bw, bh, dets[i].prob[j]); &#125; &#125;&#125;void print_detector_detections(FILE **fps, char *id, detection *dets, int total, int classes, int w, int h)&#123; int i, j; for(i = 0; i &lt; total; ++i)&#123; float xmin = dets[i].bbox.x - dets[i].bbox.w/2. + 1; float xmax = dets[i].bbox.x + dets[i].bbox.w/2. + 1; float ymin = dets[i].bbox.y - dets[i].bbox.h/2. + 1; float ymax = dets[i].bbox.y + dets[i].bbox.h/2. + 1; if (xmin &lt; 1) xmin = 1; if (ymin &lt; 1) ymin = 1; if (xmax &gt; w) xmax = w; if (ymax &gt; h) ymax = h; for(j = 0; j &lt; classes; ++j)&#123; if (dets[i].prob[j]) fprintf(fps[j], \"%s %f %f %f %f %f\\n\", id, dets[i].prob[j], xmin, ymin, xmax, ymax); &#125; &#125;&#125;void print_imagenet_detections(FILE *fp, int id, detection *dets, int total, int classes, int w, int h)&#123; int i, j; for(i = 0; i &lt; total; ++i)&#123; float xmin = dets[i].bbox.x - dets[i].bbox.w/2.; float xmax = dets[i].bbox.x + dets[i].bbox.w/2.; float ymin = dets[i].bbox.y - dets[i].bbox.h/2.; float ymax = dets[i].bbox.y + dets[i].bbox.h/2.; if (xmin &lt; 0) xmin = 0; if (ymin &lt; 0) ymin = 0; if (xmax &gt; w) xmax = w; if (ymax &gt; h) ymax = h; for(j = 0; j &lt; classes; ++j)&#123; int class = j; if (dets[i].prob[class]) fprintf(fp, \"%d %d %f %f %f %f %f\\n\", id, j+1, dets[i].prob[class], xmin, ymin, xmax, ymax); &#125; &#125;&#125;void validate_detector_flip(char *datacfg, char *cfgfile, char *weightfile, char *outfile)&#123; int j; list *options = read_data_cfg(datacfg); char *valid_images = option_find_str(options, \"valid\", \"data/valid.list\"); char *name_list = option_find_str(options, \"names\", \"data/names.list\"); char *prefix = option_find_str(options, \"results\", \"results\"); char **names = get_labels(name_list); char *mapf = option_find_str(options, \"map\", 0); int *map = 0; if (mapf) map = read_map(mapf); network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 2); fprintf(stderr, \"Learning Rate: %g, Momentum: %g, Decay: %g\\n\", net-&gt;learning_rate, net-&gt;momentum, net-&gt;decay); srand(time(0)); list *plist = get_paths(valid_images); char **paths = (char **)list_to_array(plist); layer l = net-&gt;layers[net-&gt;n-1]; int classes = l.classes; char buff[1024]; char *type = option_find_str(options, \"eval\", \"voc\"); FILE *fp = 0; FILE **fps = 0; int coco = 0; int imagenet = 0; if(0==strcmp(type, \"coco\"))&#123; if(!outfile) outfile = \"coco_results\"; snprintf(buff, 1024, \"%s/%s.json\", prefix, outfile); fp = fopen(buff, \"w\"); fprintf(fp, \"[\\n\"); coco = 1; &#125; else if(0==strcmp(type, \"imagenet\"))&#123; if(!outfile) outfile = \"imagenet-detection\"; snprintf(buff, 1024, \"%s/%s.txt\", prefix, outfile); fp = fopen(buff, \"w\"); imagenet = 1; classes = 200; &#125; else &#123; if(!outfile) outfile = \"comp4_det_test_\"; fps = calloc(classes, sizeof(FILE *)); for(j = 0; j &lt; classes; ++j)&#123; snprintf(buff, 1024, \"%s/%s%s.txt\", prefix, outfile, names[j]); fps[j] = fopen(buff, \"w\"); &#125; &#125; int m = plist-&gt;size; int i=0; int t; float thresh = .95; float nms = .45; int nthreads = 4; image *val = calloc(nthreads, sizeof(image)); image *val_resized = calloc(nthreads, sizeof(image)); image *buf = calloc(nthreads, sizeof(image)); image *buf_resized = calloc(nthreads, sizeof(image)); pthread_t *thr = calloc(nthreads, sizeof(pthread_t)); image input = make_image(net-&gt;w, net-&gt;h, net-&gt;c*2); load_args args = &#123;0&#125;; args.w = net-&gt;w; args.h = net-&gt;h; //args.type = IMAGE_DATA; args.type = LETTERBOX_DATA; for(t = 0; t &lt; nthreads; ++t)&#123; args.path = paths[i+t]; args.im = &amp;buf[t]; args.resized = &amp;buf_resized[t]; thr[t] = load_data_in_thread(args); &#125; double start = what_time_is_it_now(); for(i = nthreads; i &lt; m+nthreads; i += nthreads)&#123; fprintf(stderr, \"%d\\n\", i); for(t = 0; t &lt; nthreads &amp;&amp; i+t-nthreads &lt; m; ++t)&#123; pthread_join(thr[t], 0); val[t] = buf[t]; val_resized[t] = buf_resized[t]; &#125; for(t = 0; t &lt; nthreads &amp;&amp; i+t &lt; m; ++t)&#123; args.path = paths[i+t]; args.im = &amp;buf[t]; args.resized = &amp;buf_resized[t]; thr[t] = load_data_in_thread(args); &#125; for(t = 0; t &lt; nthreads &amp;&amp; i+t-nthreads &lt; m; ++t)&#123; char *path = paths[i+t-nthreads]; char *id = basecfg(path); copy_cpu(net-&gt;w*net-&gt;h*net-&gt;c, val_resized[t].data, 1, input.data, 1); flip_image(val_resized[t]); copy_cpu(net-&gt;w*net-&gt;h*net-&gt;c, val_resized[t].data, 1, input.data + net-&gt;w*net-&gt;h*net-&gt;c, 1); network_predict(net, input.data); int w = val[t].w; int h = val[t].h; int num = 0; detection *dets = get_network_boxes(net, w, h, thresh, .5, map, 0, &amp;num); if (nms) do_nms_sort(dets, num, classes, nms); if (coco)&#123; print_cocos(fp, path, dets, num, classes, w, h); &#125; else if (imagenet)&#123; print_imagenet_detections(fp, i+t-nthreads+1, dets, num, classes, w, h); &#125; else &#123; print_detector_detections(fps, id, dets, num, classes, w, h); &#125; free_detections(dets, num); free(id); free_image(val[t]); free_image(val_resized[t]); &#125; &#125; for(j = 0; j &lt; classes; ++j)&#123; if(fps) fclose(fps[j]); &#125; if(coco)&#123; fseek(fp, -2, SEEK_CUR); fprintf(fp, \"\\n]\\n\"); fclose(fp); &#125; fprintf(stderr, \"Total Detection Time: %f Seconds\\n\", what_time_is_it_now() - start);&#125;void validate_detector(char *datacfg, char *cfgfile, char *weightfile, char *outfile)&#123; int j; list *options = read_data_cfg(datacfg); char *valid_images = option_find_str(options, \"valid\", \"data/valid.list\"); char *name_list = option_find_str(options, \"names\", \"data/voc.names\"); char *prefix = option_find_str(options, \"results\", \"results\"); char **names = get_labels(name_list); char *mapf = option_find_str(options, \"map\", 0); int *map = 0; if (mapf) map = read_map(mapf); network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 1); fprintf(stderr, \"Learning Rate: %g, Momentum: %g, Decay: %g\\n\", net-&gt;learning_rate, net-&gt;momentum, net-&gt;decay); srand(time(0)); list *plist = get_paths(valid_images); char **paths = (char **)list_to_array(plist); layer l = net-&gt;layers[net-&gt;n-1]; int classes = l.classes; char buff[1024]; char *type = option_find_str(options, \"eval\", \"voc\"); FILE *fp = 0; FILE **fps = 0; int coco = 0; int imagenet = 0; if(0==strcmp(type, \"coco\"))&#123; if(!outfile) outfile = \"coco_results\"; snprintf(buff, 1024, \"%s/%s.json\", prefix, outfile); fp = fopen(buff, \"w\"); fprintf(fp, \"[\\n\"); coco = 1; &#125; else if(0==strcmp(type, \"imagenet\"))&#123; if(!outfile) outfile = \"imagenet-detection\"; snprintf(buff, 1024, \"%s/%s.txt\", prefix, outfile); fp = fopen(buff, \"w\"); imagenet = 1; classes = 200; &#125; else &#123; if(!outfile) outfile = \"comp4_det_test_\"; fps = calloc(classes, sizeof(FILE *)); for(j = 0; j &lt; classes; ++j)&#123; snprintf(buff, 1024, \"%s/%s%s.txt\", prefix, outfile, names[j]); fps[j] = fopen(buff, \"w\"); &#125; &#125; int m = plist-&gt;size; int i=0; int t; float thresh = .95; float nms = .45; int nthreads = 4; image *val = calloc(nthreads, sizeof(image)); image *val_resized = calloc(nthreads, sizeof(image)); image *buf = calloc(nthreads, sizeof(image)); image *buf_resized = calloc(nthreads, sizeof(image)); pthread_t *thr = calloc(nthreads, sizeof(pthread_t)); load_args args = &#123;0&#125;; args.w = net-&gt;w; args.h = net-&gt;h; //args.type = IMAGE_DATA; args.type = LETTERBOX_DATA; for(t = 0; t &lt; nthreads; ++t)&#123; args.path = paths[i+t]; args.im = &amp;buf[t]; args.resized = &amp;buf_resized[t]; thr[t] = load_data_in_thread(args); &#125; double start = what_time_is_it_now(); for(i = nthreads; i &lt; m+nthreads; i += nthreads)&#123; fprintf(stderr, \"%d\\n\", i); for(t = 0; t &lt; nthreads &amp;&amp; i+t-nthreads &lt; m; ++t)&#123; pthread_join(thr[t], 0); val[t] = buf[t]; val_resized[t] = buf_resized[t]; &#125; for(t = 0; t &lt; nthreads &amp;&amp; i+t &lt; m; ++t)&#123; args.path = paths[i+t]; args.im = &amp;buf[t]; args.resized = &amp;buf_resized[t]; thr[t] = load_data_in_thread(args); &#125; for(t = 0; t &lt; nthreads &amp;&amp; i+t-nthreads &lt; m; ++t)&#123; char *path = paths[i+t-nthreads]; char *id = basecfg(path); float *X = val_resized[t].data; network_predict(net, X); int w = val[t].w; int h = val[t].h; int nboxes = 0; detection *dets = get_network_boxes(net, w, h, thresh, .5, map, 0, &amp;nboxes); if (nms) do_nms_sort(dets, nboxes, classes, nms); if (coco)&#123; print_cocos(fp, path, dets, nboxes, classes, w, h); &#125; else if (imagenet)&#123; print_imagenet_detections(fp, i+t-nthreads+1, dets, nboxes, classes, w, h); &#125; else &#123; print_detector_detections(fps, id, dets, nboxes, classes, w, h); &#125; free_detections(dets, nboxes); free(id); free_image(val[t]); free_image(val_resized[t]); &#125; &#125; for(j = 0; j &lt; classes; ++j)&#123; if(fps) fclose(fps[j]); &#125; if(coco)&#123; fseek(fp, -2, SEEK_CUR); fprintf(fp, \"\\n]\\n\"); fclose(fp); &#125; fprintf(stderr, \"Total Detection Time: %f Seconds\\n\", what_time_is_it_now() - start);&#125;void validate_detector_recall(char *datacfg, char *cfgfile, char *weightfile)&#123; network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 1); fprintf(stderr, \"Learning Rate: %g, Momentum: %g, Decay: %g\\n\", net-&gt;learning_rate, net-&gt;momentum, net-&gt;decay); srand(time(0)); list *options = read_data_cfg(datacfg); char *valid_images = option_find_str(options, \"valid\", \"data/valid.list\"); list *plist = get_paths(valid_images); char **paths = (char **)list_to_array(plist); //layer l = net-&gt;layers[net-&gt;n-1]; int j, k; int m = plist-&gt;size; //测试的图片总数 int i=0; float thresh = .95; float iou_thresh = .5; float nms = .4; int total = 0; //实际有多少个bbox int correct = 0; //正确识别出了多少个bbox int proposals = 0; //测试集预测的bbox总数 float avg_iou = 0; //printf(\"l.w*l.h*l.n = %d\\n\",l.w*l.h*l.n); for(i = 0; i &lt; m; ++i)&#123; char *path = paths[i]; image orig = load_image_color(path, 0, 0); image sized = resize_image(orig, net-&gt;w, net-&gt;h); char *id = basecfg(path); network_predict(net, sized.data); int nboxes = 0; detection *dets = get_network_boxes(net, sized.w, sized.h, thresh, .5, 0, 1, &amp;nboxes); if (nms) do_nms_obj(dets, nboxes, 1, nms); char labelpath[4096]; find_replace(path, \"images\", \"labels\", labelpath); find_replace(labelpath, \"JPEGImages\", \"labels\", labelpath); find_replace(labelpath, \".jpg\", \".txt\", labelpath); find_replace(labelpath, \".JPEG\", \".txt\", labelpath); int num_labels = 0; //测试集实际的标注框数量 box_label *truth = read_boxes(labelpath, &amp;num_labels); for(k = 0; k &lt; nboxes; ++k)&#123; if(dets[k].objectness &gt; thresh)&#123; ++proposals; &#125; &#125; for (j = 0; j &lt; num_labels; ++j) &#123; ++total; box t = &#123;truth[j].x, truth[j].y, truth[j].w, truth[j].h&#125;; //printf(\"truth x=%f, y=%f, w=%f,h=%f\\n\",truth[j].x,truth[j].y,truth[j].w,truth[j].h); float best_iou = 0; //对每一个标注的框 for(k = 0; k &lt; nboxes; ++k)&#123; //for(k = 0; k &lt; l.w*l.h*l.n; ++k)&#123; float iou = box_iou(dets[k].bbox, t); //printf(\"predict=%f iou=%f x=%f, y=%f, w=%f,h=%f\\n\",dets[k].objectness,iou,dets[k].bbox.x,dets[k].bbox.y,dets[k].bbox.w,dets[k].bbox.h); if(dets[k].objectness &gt; thresh &amp;&amp; iou &gt; best_iou)&#123; best_iou = iou; &#125; &#125; //printf(\"best_iou=%f\\n\",best_iou); avg_iou += best_iou; if(best_iou &gt; iou_thresh)&#123; ++correct; &#125; &#125; fprintf(stderr, \"%5d %5d %5d\\tRPs/Img: %.2f\\tIOU: %.2f%%\\tRecall:%.2f%%\\n\", i, correct, total, (float)proposals/(i+1), avg_iou*100/total, 100.*correct/total); free(id); free_image(orig); free_image(sized); &#125;&#125;void test_detector(char *datacfg, char *cfgfile, char *weightfile, char *filename, float thresh, float hier_thresh, char *outfile, int fullscreen)&#123; list *options = read_data_cfg(datacfg); //options存储分类的标签等基本训练信息 char *name_list = option_find_str(options, \"names\", \"data/names.list\"); //抽取标签名称 char **names = get_labels(name_list); image **alphabet = load_alphabet(); //加载位于data/labels下的字符图片，用于显示矩形框名称 network *net = load_network(cfgfile, weightfile, 0); //用netweork.h中自定义的network结构体存储模型文件,函数位于parser.c set_batch_network(net, 1); srand(2222222); double start_time; double end_time; double img_time; double sum_time=0.0; char buff[256]; char *input = buff; float nms=.45; int i=0; while(1)&#123; //读取结构对应的权重文件 if(filename)&#123; strncpy(input, filename, 256); image im = load_image_color(input,0,0); image sized = letterbox_image(im, net-&gt;w, net-&gt;h); //输入图片大小经过resize至输入大小 //image sized = resize_image(im, net-&gt;w, net-&gt;h); //image sized2 = resize_max(im, net-&gt;w); //image sized = crop_image(sized2, -((net-&gt;w - sized2.w)/2), -((net-&gt;h - sized2.h)/2), net-&gt;w, net-&gt;h); //resize_network(net, sized.w, sized.h); layer l = net-&gt;layers[net-&gt;n-1]; float *X = sized.data; //X指向图片的data元素，即图片像素 start_time=what_time_is_it_now(); network_predict(net, X); //network_predict函数负责预测当前图片的数据X end_time=what_time_is_it_now(); img_time= end_time - start_time; printf(\"%s: Predicted in %f seconds.\\n\", input, img_time); int nboxes = 0; detection *dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, 0, 1, &amp;nboxes); //printf(\"%d\\n\", nboxes); //if (nms) do_nms_obj(boxes, probs, l.w*l.h*l.n, l.classes, nms); if (nms) do_nms_sort(dets, nboxes, l.classes, nms); draw_detections(im, dets, nboxes, thresh, names, alphabet, l.classes); free_detections(dets, nboxes); if(outfile) &#123; save_image(im, outfile); &#125; else&#123; //save_image(im, \"predictions\"); char image[2048]; sprintf(image,\"./data/predict/%s\",GetFilename(filename)); save_image(im,image); printf(\"predict %s successfully!\\n\",GetFilename(filename));#ifdef OPENCV cvNamedWindow(\"predictions\", CV_WINDOW_NORMAL); if(fullscreen)&#123; cvSetWindowProperty(\"predictions\", CV_WND_PROP_FULLSCREEN, CV_WINDOW_FULLSCREEN); &#125; show_image(im, \"predictions\"); cvWaitKey(0); cvDestroyAllWindows();#endif &#125; free_image(im); free_image(sized); if (filename) break; &#125; else &#123; printf(\"Enter Image Path: \"); fflush(stdout); input = fgets(input, 256, stdin); if(!input) return; strtok(input, \"\\n\"); list *plist = get_paths(input); char **paths = (char **)list_to_array(plist); printf(\"Start Testing!\\n\"); int m = plist-&gt;size; if(access(\"/aseit-data/XCM_WorkSpace/darknet_test/darknet/test_out\",0)==-1)//修改成自己的路径 &#123; if (mkdir(\"/aseit-data/XCM_WorkSpace/darknet_test/darknet/test_out\",0777))//修改成自己的路径 &#123; printf(\"creat file bag failed!!!\"); &#125; &#125; for(i = 0; i &lt; m; ++i)&#123; char *path = paths[i]; image im = load_image_color(path,0,0); image sized = letterbox_image(im, net-&gt;w, net-&gt;h); //输入图片大小经过resize至输入大小 //image sized = resize_image(im, net-&gt;w, net-&gt;h); //image sized2 = resize_max(im, net-&gt;w); //image sized = crop_image(sized2, -((net-&gt;w - sized2.w)/2), -((net-&gt;h - sized2.h)/2), net-&gt;w, net-&gt;h); //resize_network(net, sized.w, sized.h); layer l = net-&gt;layers[net-&gt;n-1]; float *X = sized.data; //X指向图片的data元素，即图片像素 start_time = what_time_is_it_now(); network_predict(net, X); //network_predict函数负责预测当前图片的数据X end_time = what_time_is_it_now(); img_time = end_time - start_time; sum_time = sum_time+img_time; printf(\"Try Very Hard:\"); printf(\"%s: Predicted in %f seconds.\\n\", path, img_time); int nboxes = 0; detection *dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, 0, 1, &amp;nboxes); //printf(\"%d\\n\", nboxes); //if (nms) do_nms_obj(boxes, probs, l.w*l.h*l.n, l.classes, nms); if (nms) do_nms_sort(dets, nboxes, l.classes, nms); draw_detections(im, dets, nboxes, thresh, names, alphabet, l.classes); free_detections(dets, nboxes); if(outfile)&#123; save_image(im, outfile); &#125; else&#123; char b[2048]; sprintf(b,\"/aseit-data/XCM_WorkSpace/darknet_test/darknet/test_out/%s\",GetFilename(path));//修改成自己的路径 save_image(im, b); printf(\"save %s successfully!\\n\",GetFilename(path));#ifdef OPENCV cvNamedWindow(\"predictions\", CV_WINDOW_NORMAL); if(fullscreen)&#123; cvSetWindowProperty(\"predictions\", CV_WND_PROP_FULLSCREEN, CV_WINDOW_FULLSCREEN); &#125; show_image(im, \"predictions\"); cvWaitKey(0); cvDestroyAllWindows();#endif &#125; free_image(im); free_image(sized); if (filename) break; &#125; printf(\"fps: %.2f totall image %d\\n\",(float)m/sum_time,m); &#125; &#125;&#125;/*void test_detector(char *datacfg, char *cfgfile, char *weightfile, char *filename, float thresh, float hier_thresh, char *outfile, int fullscreen)&#123; list *options = read_data_cfg(datacfg); char *name_list = option_find_str(options, \"names\", \"data/names.list\"); char **names = get_labels(name_list); image **alphabet = load_alphabet(); network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 1); srand(2222222); double time; char buff[256]; char *input = buff; float nms=.45; while(1)&#123; if(filename)&#123; strncpy(input, filename, 256); &#125; else &#123; printf(\"Enter Image Path: \"); fflush(stdout); input = fgets(input, 256, stdin); if(!input) return; strtok(input, \"\\n\"); &#125; image im = load_image_color(input,0,0); image sized = letterbox_image(im, net-&gt;w, net-&gt;h); //image sized = resize_image(im, net-&gt;w, net-&gt;h); //image sized2 = resize_max(im, net-&gt;w); //image sized = crop_image(sized2, -((net-&gt;w - sized2.w)/2), -((net-&gt;h - sized2.h)/2), net-&gt;w, net-&gt;h); //resize_network(net, sized.w, sized.h); layer l = net-&gt;layers[net-&gt;n-1]; float *X = sized.data; time=what_time_is_it_now(); network_predict(net, X); printf(\"%s: Predicted in %f seconds.\\n\", input, what_time_is_it_now()-time); int nboxes = 0; detection *dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, 0, 1, &amp;nboxes); //printf(\"%d\\n\", nboxes); //if (nms) do_nms_obj(boxes, probs, l.w*l.h*l.n, l.classes, nms); if (nms) do_nms_sort(dets, nboxes, l.classes, nms); draw_detections(im, dets, nboxes, thresh, names, alphabet, l.classes); free_detections(dets, nboxes); if(outfile)&#123; save_image(im, outfile); &#125; else&#123; save_image(im, \"predictions\");#ifdef OPENCV make_window(\"predictions\", 512, 512, 0); show_image(im, \"predictions\", 0);#endif &#125; free_image(im); free_image(sized); if (filename) break; &#125;&#125;void censor_detector(char *datacfg, char *cfgfile, char *weightfile, int cam_index, const char *filename, int class, float thresh, int skip)&#123;#ifdef OPENCV char *base = basecfg(cfgfile); network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 1); srand(2222222); CvCapture * cap; int w = 1280; int h = 720; if(filename)&#123; cap = cvCaptureFromFile(filename); &#125;else&#123; cap = cvCaptureFromCAM(cam_index); &#125; if(w)&#123; cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_WIDTH, w); &#125; if(h)&#123; cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_HEIGHT, h); &#125; if(!cap) error(\"Couldn't connect to webcam.\\n\"); cvNamedWindow(base, CV_WINDOW_NORMAL); cvResizeWindow(base, 512, 512); float fps = 0; int i; float nms = .45; while(1)&#123; image in = get_image_from_stream(cap); //image in_s = resize_image(in, net-&gt;w, net-&gt;h); image in_s = letterbox_image(in, net-&gt;w, net-&gt;h); layer l = net-&gt;layers[net-&gt;n-1]; float *X = in_s.data; network_predict(net, X); int nboxes = 0; detection *dets = get_network_boxes(net, in.w, in.h, thresh, 0, 0, 0, &amp;nboxes); //if (nms) do_nms_obj(boxes, probs, l.w*l.h*l.n, l.classes, nms); if (nms) do_nms_sort(dets, nboxes, l.classes, nms); for(i = 0; i &lt; nboxes; ++i)&#123; if(dets[i].prob[class] &gt; thresh)&#123; box b = dets[i].bbox; int left = b.x-b.w/2.; int top = b.y-b.h/2.; censor_image(in, left, top, b.w, b.h); &#125; &#125; show_image(in, base); cvWaitKey(10); free_detections(dets, nboxes); free_image(in_s); free_image(in); float curr = 0; fps = .9*fps + .1*curr; for(i = 0; i &lt; skip; ++i)&#123; image in = get_image_from_stream(cap); free_image(in); &#125; &#125; #endif&#125;void extract_detector(char *datacfg, char *cfgfile, char *weightfile, int cam_index, const char *filename, int class, float thresh, int skip)&#123;#ifdef OPENCV char *base = basecfg(cfgfile); network *net = load_network(cfgfile, weightfile, 0); set_batch_network(net, 1); srand(2222222); CvCapture * cap; int w = 1280; int h = 720; if(filename)&#123; cap = cvCaptureFromFile(filename); &#125;else&#123; cap = cvCaptureFromCAM(cam_index); &#125; if(w)&#123; cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_WIDTH, w); &#125; if(h)&#123; cvSetCaptureProperty(cap, CV_CAP_PROP_FRAME_HEIGHT, h); &#125; if(!cap) error(\"Couldn't connect to webcam.\\n\"); cvNamedWindow(base, CV_WINDOW_NORMAL); cvResizeWindow(base, 512, 512); float fps = 0; int i; int count = 0; float nms = .45; while(1)&#123; image in = get_image_from_stream(cap); //image in_s = resize_image(in, net-&gt;w, net-&gt;h); image in_s = letterbox_image(in, net-&gt;w, net-&gt;h); layer l = net-&gt;layers[net-&gt;n-1]; show_image(in, base); int nboxes = 0; float *X = in_s.data; network_predict(net, X); detection *dets = get_network_boxes(net, in.w, in.h, thresh, 0, 0, 1, &amp;nboxes); //if (nms) do_nms_obj(boxes, probs, l.w*l.h*l.n, l.classes, nms); if (nms) do_nms_sort(dets, nboxes, l.classes, nms); for(i = 0; i &lt; nboxes; ++i)&#123; if(dets[i].prob[class] &gt; thresh)&#123; box b = dets[i].bbox; int size = b.w*in.w &gt; b.h*in.h ? b.w*in.w : b.h*in.h; int dx = b.x*in.w-size/2.; int dy = b.y*in.h-size/2.; image bim = crop_image(in, dx, dy, size, size); char buff[2048]; sprintf(buff, \"results/extract/%07d\", count); ++count; save_image(bim, buff); free_image(bim); &#125; &#125; free_detections(dets, nboxes); free_image(in_s); free_image(in); float curr = 0; fps = .9*fps + .1*curr; for(i = 0; i &lt; skip; ++i)&#123; image in = get_image_from_stream(cap); free_image(in); &#125; &#125; #endif&#125;*//*void network_detect(network *net, image im, float thresh, float hier_thresh, float nms, detection *dets)&#123; network_predict_image(net, im); layer l = net-&gt;layers[net-&gt;n-1]; int nboxes = num_boxes(net); fill_network_boxes(net, im.w, im.h, thresh, hier_thresh, 0, 0, dets); if (nms) do_nms_sort(dets, nboxes, l.classes, nms);&#125;*/// ./darknet [xxx]中如果命令如果第二个xxx参数是detector，则调用这个void run_detector(int argc, char **argv)&#123; char *prefix = find_char_arg(argc, argv, \"-prefix\", 0); //寻找是否有参数prefix, 默认参数0，argv为二维数组，存储了参数字符串 float thresh = find_float_arg(argc, argv, \"-thresh\", .5); //寻找是否有参数thresh，thresh为输出的阈值,默认参数0.24 float hier_thresh = find_float_arg(argc, argv, \"-hier\", .5); //寻找是否有参数hier_thresh,默认0.5 int cam_index = find_int_arg(argc, argv, \"-c\", 0); //寻找是否有参数c，默认0 int frame_skip = find_int_arg(argc, argv, \"-s\", 0); //寻找是否有参数s，默认0 int avg = find_int_arg(argc, argv, \"-avg\", 3); //如果输入参数小于4个，输出正确语法如何使用 //printf 等价于 fprintf(stdout, ...)，这里stderr和stdout默认输出设备都是屏幕，但是stderr一般指标准出错输入设备 if(argc &lt; 4)&#123; fprintf(stderr, \"usage: %s %s [train/test/valid] [cfg] [weights (optional)]\\n\", argv[0], argv[1]); return; &#125; char *gpu_list = find_char_arg(argc, argv, \"-gpus\", 0); //寻找是否有参数gpus，默认0 char *outfile = find_char_arg(argc, argv, \"-out\", 0); //检查是否指定GPU运算,默认0 int *gpus = 0; int gpu = 0; int ngpus = 0; if(gpu_list)&#123; printf(\"%s\\n\", gpu_list); int len = strlen(gpu_list); ngpus = 1; int i; for(i = 0; i &lt; len; ++i)&#123; if (gpu_list[i] == ',') ++ngpus; &#125; gpus = calloc(ngpus, sizeof(int)); for(i = 0; i &lt; ngpus; ++i)&#123; gpus[i] = atoi(gpu_list); gpu_list = strchr(gpu_list, ',')+1; &#125; &#125; else &#123; gpu = gpu_index; gpus = &amp;gpu; ngpus = 1; &#125; int clear = find_arg(argc, argv, \"-clear\"); //检查clear参数 int fullscreen = find_arg(argc, argv, \"-fullscreen\"); int width = find_int_arg(argc, argv, \"-w\", 0); int height = find_int_arg(argc, argv, \"-h\", 0); int fps = find_int_arg(argc, argv, \"-fps\", 0); //int class = find_int_arg(argc, argv, \"-class\", 0); char *datacfg = argv[3]; //存data文件路径 char *cfg = argv[4]; //存cfg文件路径 char *weights = (argc &gt; 5) ? argv[5] : 0; //存weight文件路径 char *filename = (argc &gt; 6) ? argv[6]: 0; //存待检测文件路径 //根据第三个参数的内容，调用不同的函数，并传入之前解析的参数 if(0==strcmp(argv[2], \"test\")) test_detector(datacfg, cfg, weights, filename, thresh, hier_thresh, outfile, fullscreen); else if(0==strcmp(argv[2], \"train\")) train_detector(datacfg, cfg, weights, gpus, ngpus, clear); else if(0==strcmp(argv[2], \"valid\")) validate_detector(datacfg, cfg, weights, outfile); else if(0==strcmp(argv[2], \"valid2\")) validate_detector_flip(datacfg, cfg, weights, outfile); else if(0==strcmp(argv[2], \"recall\")) validate_detector_recall(datacfg, cfg, weights); else if(0==strcmp(argv[2], \"demo\")) &#123; list *options = read_data_cfg(datacfg); int classes = option_find_int(options, \"classes\", 20); char *name_list = option_find_str(options, \"names\", \"data/names.list\"); char **names = get_labels(name_list); demo(cfg, weights, thresh, cam_index, filename, names, classes, frame_skip, prefix, avg, hier_thresh, width, height, fps, fullscreen); &#125; //else if(0==strcmp(argv[2], \"extract\")) extract_detector(datacfg, cfg, weights, cam_index, filename, class, thresh, frame_skip); //else if(0==strcmp(argv[2], \"censor\")) censor_detector(datacfg, cfg, weights, cam_index, filename, class, thresh, frame_skip);&#125; 参考YOLO-V3实战（darknet） Darknet 评估训练好的网络的性能 yolov3实战(darknet) yolo_person_detect Yolo-v3 and Yolo-v2 for Windows and Linux [pjreddie/darknet](","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://shuiyujie.com/categories/DeepLearning/"}],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"http://shuiyujie.com/tags/deeplearning/"},{"name":"darknet","slug":"darknet","permalink":"http://shuiyujie.com/tags/darknet/"}]},{"title":"目录-C/C++学习笔记","slug":"C:C++/目录-C-C-学习笔记","date":"2019-06-08T06:51:13.000Z","updated":"2019-06-08T07:13:42.823Z","comments":true,"path":"post/789284ab.html","link":"","permalink":"http://shuiyujie.com/post/789284ab.html","excerpt":"","text":"C 语言学习笔记 C语言学习笔记-静态库、动态库的制作和使用 C语言学习笔记-写一个最简单的makefile C语言学习笔记-文件操作 C语言学习笔记-结构体 C语言学习笔记-const关键字 C语言学习笔记-字符串 C语言学习笔记-指针01 C++ 学习笔记 C++学习笔记-引用 C++学习笔记-内联函数、默认参数和函数重载 C++学习笔记-构造函数、析构函数、拷贝构造函数 C++学习笔记-静态方法、静态函数、this指针 C++学习笔记-继承 C++学习笔记-多态 零散知识点 C/C++ 中的命令行参数 C 语言-格式化输出函数 printf() STL 学习笔记 STL-STL基础概念 STL-string 容器 STL-vector 容器 STL-deque 容器 STL-queue 容器 STL-stack 容器 STL-list 容器 STL-set and multiset STL-map and multimap STL-常用算法","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"STL-常用算法","slug":"C:C++/STL-常用算法","date":"2019-06-08T06:50:13.000Z","updated":"2019-06-14T14:21:44.783Z","comments":true,"path":"post/b974f564.html","link":"","permalink":"http://shuiyujie.com/post/b974f564.html","excerpt":"常用遍历、查找、拷贝和替换、算数生成、集合算法。","text":"常用遍历、查找、拷贝和替换、算数生成、集合算法。 遍历算法12345678910111213141516171819/*遍历算法,遍历容器元素@param beg 开始迭代器@param end 结束迭代器@param _callback 函数回调或者函数对象@return 函数对象*/for_each(iterator beg, iterator end, _callback);/*transform 算法,将指定容器区间元素搬运到另一容器中注意: transform 不会给目标容器分配内存，所以需要我们提前分配好内存@param beg1 源容器开始迭代器@param end1 源容器结束迭代器@param beg2 目标容器开始迭代器@param _cakkback 回调函数或者函数对象@return 返回目标容器迭代器*/transform(iterator beg1, iterator end1, iterator beg2, _callbakc) 查找算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*find 算法, 查找元素@param beg 容器开始迭代器@param end 容器结束迭代器@param value 查找的元素@return 返回查找元素的位置*/find(iterator beg, iterator end, value) /*adjacent_find 算法,查找相邻重复元素@param beg 容器开始迭代器@param end 容器结束迭代器@param _callback 回调函数或者谓词(返回 bool 类型的函数对象)@return 返回相邻元素的第一个位置的迭代器*/adjacent_find(iterator beg, iterator end, _callback);/*binary_search 算法二分查找法注意: 在无序序列中不可用@param beg 容器开始迭代器@param end 容器结束迭代器@param value 查找的元素@return bool 查找返回 true 否则 false*/bool binary_search(iterator beg, iterator end, value);/*find_if 算法 条件查找@param beg 容器开始迭代器@param end 容器结束迭代器@param callback 回调函数或者谓词(返回 bool 类型的函数对象)@return bool 查找返回 true 否则 false*/find_if(iterator beg, iterator end, _callback);/*count 算法 统计元素出现次数@param beg 容器开始迭代器@param end 容器结束迭代器@param value 回调函数或者谓词(返回 bool 类型的函数对象)@return int 返回元素个数*/count(iterator beg, iterator end, value);/*count 算法 统计元素出现次数@param beg 容器开始迭代器@param end 容器结束迭代器@param callback 回调函数或者谓词(返回 bool 类型的函数对象)@return int 返回元素个数*/count_if(iterator beg, iterator end, _callback); 排序算法1234567891011121314151617181920212223242526272829303132/*merge 算法 容器元素合并，并存储到另一容器中@param beg1 容器 1 开始迭代器@param end1 容器 1 结束迭代器@param beg2 容器 2 开始迭代器@param end2 容器 2 结束迭代器@param dest 目标容器开始迭代器*/merge(iterator beg1, iterator end1, iterator beg2, iterator end2, iterator dest) /*sort 算法 容器元素排序注意:两个容器必须是有序的@param beg 容器 1 开始迭代器@param end 容器 1 结束迭代器@param _callback 回调函数或者谓词(返回 bool 类型的函数对象)*/sort(iterator beg, iterator end, _callback)/*sort 算法 对指定范围内的元素随机调整次序@param beg 容器开始迭代器@param end 容器结束迭代器*/random_shuffle(iterator beg, iterator end) /*reverse 算法 反转指定范围的元素@param beg 容器开始迭代器@param end 容器结束迭代器*/reverse(iterator beg, iterator end) 拷贝和替换算法1234567891011121314151617181920212223242526272829303132/*copy 算法 将容器内指定范围的元素拷贝到另一容器中@param beg 容器开始迭代器@param end 容器结束迭代器@param dest 目标容器结束迭代器*/copy(iterator beg, iterator end, iterator dest) /*replace 算法 将容器内指定范围的旧元素修改为新元素@param beg 容器开始迭代器@param end 容器结束迭代器@param oldvalue 旧元素@param oldvalue 新元素*/replace(iterator beg, iterator end, oldvalue, newvalue) /*replace_if 算法 将容器内指定范围满足条件的元素替换为新元素@param beg 容器开始迭代器@param end 容器结束迭代器@param callback 函数回调或者谓词(返回 Bool 类型的函数对象)@param oldvalue 新元素*/replace_if(iterator beg, iterator end, _callback, newvalue)/*swap 算法 互换两个容器的元素@param c1 容器 1@param c2 容器 2*/swap(container c1, container c2) 算数生成算法123456789101112131415/*accumulate 算法 计算容器元素累计总和@param beg 容器开始迭代器@param end 容器结束迭代器@param value 累加值*/accumulate(iterator beg, iterator end, value)/*fill 算法 向容器中添加元素@param beg 容器开始迭代器@param end 容器结束迭代器@param value t 填充元素*/fill(iterator beg, iterator end, value) 集合算法1234567891011121314151617181920212223242526272829303132333435/*set_intersection 算法 求两个 set 集合的交集注意:两个集合必须是有序序列@param beg1 容器 1 开始迭代器@param end1 容器 1 结束迭代器@param beg2 容器 2 开始迭代器@param end2 容器 2 结束迭代器@param dest 目标容器开始迭代器@return 目标容器的最后一个元素的迭代器地址*/set_intersection(iterator beg1, iterator end1, iterator beg2, iterator end2, iterator dest) /*set_union 算法 求两个 set 集合的并集注意:两个集合必须是有序序列@param beg1 容器 1 开始迭代器@param end1 容器 1 结束迭代器@param beg2 容器 2 开始迭代器@param end2 容器 2 结束迭代器@param dest 目标容器开始迭代器@return 目标容器的最后一个元素的迭代器地址*/set_union(iterator beg1, iterator end1, iterator beg2, iterator end2, iterator dest)/*set_difference 算法 求两个 set集合的差集注意:两个集合必须是有序序列@param beg1 容器 1 开始迭代器@param end1 容器 1 结束迭代器@param beg2 容器 2 开始迭代器@param end2 容器 2 结束迭代器@param dest 目标容器开始迭代器@return 目标容器的最后一个元素的迭代器地址*/set_difference(iterator beg1, iterator end1, iterator beg2, iterator end2, iterator dest)","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-map and multimap","slug":"C:C++/STL-map-and-multimap","date":"2019-06-08T04:15:12.000Z","updated":"2019-06-08T06:45:07.025Z","comments":true,"path":"post/8d66854b.html","link":"","permalink":"http://shuiyujie.com/post/8d66854b.html","excerpt":"map 以键值对的形式存储数据，所有元素根据键值自动排序。pair 的第 一元素被称为键值，第二元素被称为实值。map 也是以红黑树为底层实现机制。 map 和 multimap 区别在于，map 不允许相同 key 值存在，multimap 则允许相同 key 值存在。","text":"map 以键值对的形式存储数据，所有元素根据键值自动排序。pair 的第 一元素被称为键值，第二元素被称为实值。map 也是以红黑树为底层实现机制。 map 和 multimap 区别在于，map 不允许相同 key 值存在，multimap 则允许相同 key 值存在。 对组类模板：template struct pair. 12345678910111213//第一种方法创建一个对组pair&lt;string, int&gt; pair1(string(\"name\"), 20);cout &lt;&lt; pair1.first &lt;&lt; endl; //访问 pair 第一个值cout &lt;&lt; pair1.second &lt;&lt; endl;//访问 pair 第二个值// 第二种pair&lt;string, int&gt; pair2 = make_pair(\"name\", 30);cout &lt;&lt; pair2.first &lt;&lt; endl;cout &lt;&lt; pair2.second &lt;&lt; endl;//pair=赋值pair&lt;string, int&gt; pair3 = pair2;cout &lt;&lt; pair3.first &lt;&lt; endl;cout &lt;&lt; pair3.second &lt;&lt; endl; map 构造函数12map&lt;T1, T2&gt; mapTT;//map 默认构造函数:map(const map &amp;mp);//拷贝构造函数 map 赋值操作12map&amp; operator=(const map &amp;mp);//重载等号操作符swap(mp);//交换两个集合容器 map 大小操作12size();//返回容器中元素的数目empty();//判断容器是否为空 map 插入数据元素操作1234567891011map.insert(...);//往容器插入元素，返回pair&lt;iterator,bool&gt;map&lt;int, string&gt; mapStu; // 第一种通过 pair 的方式插入对象mapStu.insert(pair&lt;int, string&gt;(3, \"小张\"));// 第二种通过 pair 的方式插入对象mapStu.inset(make_pair(-1, \"校长\"));// 第三种通过 value_type 的方式插入对象mapStu.insert(map&lt;int, string&gt;::value_type(1,\"小李\"));// 第四种通过数组的方式插入值 mapStu[3] = \"小刘\";mapStu[5] = \"小王\"; map 删除操作1234clear();//删除所有元素erase(pos);//删除 pos 迭代器所指的元素，返回下一个元素的迭代器。erase(beg,end);//删除区间[beg,end)的所有元素 ，返回下一个元素的迭代器。erase(keyElem);//删除容器中 key 为 keyElem 的对组。 map 查找操作12345find(key);//查找键 key 是否存在,若存在， 返回该键的元素的迭代器；/若不存在，返回 map.end();count(keyElem);//返回容器中 key 为 keyElem 的对组个数。对 map 来说，要么是 0，要么是 1。对 multimap 来说，值可能大于 1。lower_bound(keyElem);//返回第一个 key&lt;=keyElem 元素的迭代器。upper_bound(keyElem);//返回第一个 key&gt;keyElem 元素的迭代器。equal_range(keyElem);//返回容器中 key 与 keyElem 相等的上下限的两个迭代器。","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-set and multiset","slug":"C:C++/STL-set-and-multiset","date":"2019-06-08T04:15:00.000Z","updated":"2019-06-08T06:45:07.026Z","comments":true,"path":"post/adaa2861.html","link":"","permalink":"http://shuiyujie.com/post/adaa2861.html","excerpt":"set 的底层是红黑树。具有良好的查找效率。set 容器中不允许出现重复的元素，multiset 允许重复元素。","text":"set 的底层是红黑树。具有良好的查找效率。set 容器中不允许出现重复的元素，multiset 允许重复元素。 set 构造函数123set&lt;T&gt; st;//set 默认构造函数：mulitset&lt;T&gt; mst; //multiset 默认构造函数:set(const set &amp;st);//拷贝构造函数 set 赋值操作12set&amp; operator=(const set &amp;st);//重载等号操作符 swap(st);//交换两个集合容器 set 大小操作12size();//返回容器中元素的数目empty();//判断容器是否为空 set 插入和删除操作12345insert(elem);//在容器中插入元素。clear();//清除所有元素erase(pos);//删除 pos 迭代器所指的元素，返回下一个元素的迭代器。erase(beg, end);//删除区间[beg,end)的所有元素 ，返回下一个元素的迭代器。erase(elem);//删除容器中值为 elem 的元素。 set 查找操作1234find(key);//查找键 key 是否存在,若存在， 返回该键的元素的迭代器；若不存在， 返回 map.end();lower_bound(keyElem);//返回第一个 key&gt;=keyElem 元素的迭代器。upper_bound(keyElem);//返回第一个 key&gt;keyElem 元素的迭代器。equal_range(keyElem);//返回容器中 key 与 keyElem相等的上下限的两个迭代器。","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-list 容器","slug":"C:C++/STL-list-容器","date":"2019-06-08T04:13:44.000Z","updated":"2019-06-08T06:45:07.024Z","comments":true,"path":"post/b0cfebdb.html","link":"","permalink":"http://shuiyujie.com/post/b0cfebdb.html","excerpt":"链表通过数据域保存元素，通过指针域表示相邻元素之间的关系。 采用动态存储分配，不会造成内存浪费和溢出 链表执行插入和删除操作的效率高 链表灵活，但是空间和时间额外耗费较大","text":"链表通过数据域保存元素，通过指针域表示相邻元素之间的关系。 采用动态存储分配，不会造成内存浪费和溢出 链表执行插入和删除操作的效率高 链表灵活，但是空间和时间额外耗费较大 list 构造函数1234list&lt;T&gt; lstT;//list 采用采用模板类实现,对象的默认构造形式：list(beg,end);//构造函数将[beg, end)区间中的元素拷贝给本身。list(n,elem);//构造函数将 n 个 elem 拷贝给本身。list(const list &amp;lst);//拷贝构造函数。 list 数据元素插入和删除操作1234567891011push_back(elem);//在容器尾部加入一个元素pop_back();//删除容器中最后一个元素push_front(elem);//在容器开头插入一个元素pop_front();//从容器开头移除第一个元素insert(pos,elem);//在 pos 位置插 elem 元素的拷贝，返回新数据的位置。insert(pos,n,elem);//在 pos 位置插入 n 个 elem 数据，无返回值。insert(pos,beg,end);//在 pos 位置插入[beg,end)区间的数据，无返回值。clear();//移除容器的所有数据erase(beg,end);//删除[beg,end)区间的数据，返回下一个数据的位置。erase(pos);//删除 pos 位置的数据，返回下一个数据的位置。remove(elem);//删除容器中所有与 elem 值匹配的元素。 list 大小操作1234size();//返回容器中元素的个数empty();//判断容器是否为空resize(num);//重新指定容器的长度为 num，若容器变长，则以默认值填充新位置。如果容器变短，则末尾超出容器长度的元素被删除。resize(num, elem);//重新指定容器的长度为 num， 若容器变长，则以 elem 值填充新位置。如果容器变短，则末尾超出容器长度的元素被删除。 list 赋值操作1234assign(beg, end);//将[beg, end)区间中的数据拷贝赋值给本身。assign(n, elem);//将 n 个 elem 拷贝赋值给本身。list&amp; operator=(const list &amp;lst);//重载等号操作符swap(lst);//将 lst 与本身的元素互换。 list 数据的存取12front();//返回第一个元素。back();//返回最后一个元素。 list 反转排列排序12reverse();//反转链表，比如 lst 包含 1,3,5 元素，运行此方法后， lst 就包含 5,3,1 元素。sort(); //list 排序","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-stack 容器","slug":"C:C++/STL-stack-容器","date":"2019-06-08T03:53:08.000Z","updated":"2019-06-08T04:12:43.345Z","comments":true,"path":"post/ac4a7cfa.html","link":"","permalink":"http://shuiyujie.com/post/ac4a7cfa.html","excerpt":"栈 stack 是一种先进后出(first in last out,FILO)的数据结构，它只有一个出口，stack 只允许在栈顶新增元素，移除元素。 栈不能遍历,不支持随机存取，只能通过 top 从栈顶获取和删除元素。","text":"栈 stack 是一种先进后出(first in last out,FILO)的数据结构，它只有一个出口，stack 只允许在栈顶新增元素，移除元素。 栈不能遍历,不支持随机存取，只能通过 top 从栈顶获取和删除元素。 stack 构造函数12stack&lt;T&gt; stkT;//stack 采用模板类实现, stack 对象的默认构造形式：stack(const stack &amp;stk);//拷贝构造函数 stack 赋值操作1stack&amp; operator=(const stack &amp;stk);//重载等号操作符 stack 数据存取操作123push(elem);//向栈顶添加元素pop();//从栈顶移除第一个元素top();//返回栈顶元素 stack 大小操作12empty();//判断堆栈是否为空size();//返回堆栈的大小","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-queue 容器","slug":"C:C++/STL-queue-容器","date":"2019-06-07T15:43:13.000Z","updated":"2019-06-08T04:08:11.301Z","comments":true,"path":"post/4244b416.html","link":"","permalink":"http://shuiyujie.com/post/4244b416.html","excerpt":"queue 是一种先进先出(first in first out, FIFO)的数据类型,他有两个口，只允许从队尾插入，队头弹出。 必须从一个口数据元素入队，另一个口数据元素出队。 不能随机存取，不支持遍历","text":"queue 是一种先进先出(first in first out, FIFO)的数据类型,他有两个口，只允许从队尾插入，队头弹出。 必须从一个口数据元素入队，另一个口数据元素出队。 不能随机存取，不支持遍历 queue 构造函数12queue&lt;T&gt; queT;//queue 采用模板类实现，queue 对象的默认构造形式：queue(const queue &amp;que);//拷贝构造函数 queue 存取、插入和删除操作1234push(elem);//往队尾添加元素pop();//从队头移除第一个元素back();//返回最后一个元素front();//返回第一个元素 queue 赋值操作1queue&amp; operator=(const queue &amp;que);//重载等号操作符 queue 大小操作12empty();//判断队列是否为空size();//返回队列的大小","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-deque 容器","slug":"C:C++/STL-deque-容器","date":"2019-06-07T15:43:04.000Z","updated":"2019-06-08T01:23:51.926Z","comments":true,"path":"post/564ccba3.html","link":"","permalink":"http://shuiyujie.com/post/564ccba3.html","excerpt":"deque 是 “double-ended queue”的缩写,和 vector 一样，deque 也支持随机存取。 与 vector 不同的是，vector 是单向开口的，deque 是双向开口的，在两端进行插入和删除操作的时间复杂度为 O(1)。此外，deque 没有容量的概念，因为它是动态的以分段的连续空间组合而成，随时可 以增加一段新的空间并链接起来，换句话说，像 vector 那样“因旧空间不足而重新分配一 块更大的空间，然后再复制元素，释放空间”这样的操作不会发生在 deque 身上，也因此 deque 没有必要提供所谓的空间保留功能。 双端插入和删除元素效率较高. 指定位置插入也会导致数据元素移动,降低效率. 可随机存取,效率高.","text":"deque 是 “double-ended queue”的缩写,和 vector 一样，deque 也支持随机存取。 与 vector 不同的是，vector 是单向开口的，deque 是双向开口的，在两端进行插入和删除操作的时间复杂度为 O(1)。此外，deque 没有容量的概念，因为它是动态的以分段的连续空间组合而成，随时可 以增加一段新的空间并链接起来，换句话说，像 vector 那样“因旧空间不足而重新分配一 块更大的空间，然后再复制元素，释放空间”这样的操作不会发生在 deque 身上，也因此 deque 没有必要提供所谓的空间保留功能。 双端插入和删除元素效率较高. 指定位置插入也会导致数据元素移动,降低效率. 可随机存取,效率高. deque 构造函数1234deque&lt;T&gt; deqT;//默认构造形式deque(beg, end);//构造函数将[beg, end)区间中的元素拷贝给本身。deque(n, elem);//构造函数将 n 个 elem 拷贝给本身。deque(const deque &amp;deq);//拷贝构造函数。 deque 赋值操作1234assign(beg, end);//将[beg, end)区间中的数据拷贝赋值给本身。assign(n, elem);//将 n 个 elem 拷贝赋值给本身。deque&amp; operator=(const deque &amp;deq); //重载等号操作符swap(deq);// 将 deq 与本身的元素互换 deque 大小操作1234deque.size();//返回容器中元素的个数deque.empty();//判断容器是否为空deque.resize(num);//重新指定容器的长度为 num,若容器变长，则以默认值填充新位置。如果容器 变短，则末尾超出容器长度的元素被删除。deque.resize(num, elem); //重新指定容器的长度为 num,若容器变长，则以 elem 值填充新位置,如果容器变短，则末尾超出容器长度的元素被删除。 deque 双端插入和删除操作1234push_back(elem);//在容器尾部添加一个数据push_front(elem);//在容器头部插入一个数据pop_back();//删除容器最后一个数据pop_front();//删除容器第一个数据 deque 数据存取1234at(idx);//返回索引 idx 所指的数据，如果 idx 越界，抛出 out_of_range。operator[];//返回索引 idx 所指的数据，如果 idx 越界，不抛出异常，直接出错。front();//返回第一个数据。back();//返回最后一个数据 deque 插入操作123insert(pos,elem);//在 pos 位置插入一个 elem 元素的拷贝，返回新数据的位置。insert(pos,n,elem);//在 pos 位置插入 n 个 elem 数据，无返回值。insert(pos,beg,end);//在 pos 位置插入[beg,end)区间的数据，无返回值。 deque 是分段连续的内存空间，通过中控器维持一种连续内存空间的状态， 其实现复杂性要大于 vector queue stack 等容器，其迭代器的实现也更加复杂。 在需要对 deque 容器元素进行排序的时候，建议先将 deque 容器中数据数据元素拷贝到 vector 容 器中，对 vector 进行排序，然后再将排序完成的数据拷贝回 deque 容器。 deque 删除操作123clear();//移除容器的所有数据erase(beg,end);//删除[beg,end)区间的数据，返回下一个数据的位置。erase(pos);//删除 pos 位置的数据，返回下一个数据的位置。","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-vector 容器","slug":"C:C++/STL-vector-容器","date":"2019-06-07T15:42:53.000Z","updated":"2019-06-08T03:50:27.282Z","comments":true,"path":"post/39bf26c3.html","link":"","permalink":"http://shuiyujie.com/post/39bf26c3.html","excerpt":"vector 是动态数组，连续内存空间，具有随机存取效率高的优点。 vector 是单口容器，在队尾插入和删除元素效率高，在指定位置插入会导致数据 元素移动，效率低。","text":"vector 是动态数组，连续内存空间，具有随机存取效率高的优点。 vector 是单口容器，在队尾插入和删除元素效率高，在指定位置插入会导致数据 元素移动，效率低。 vector 构造函数12345678vector&lt;T&gt; v; //采用模板实现类实现，默认构造函数vector(v.begin(), v.end());//将 v[begin(), end())区间中的元素拷贝给本身。vector(n, elem);//构造函数将 n 个 elem 拷贝给本身。vector(const vector &amp;vec);//拷贝构造函数。//例子 使用第二个构造函数我们可以...int arr[] = &#123;2,3,4,1,9&#125;;vector&lt;int&gt; v1(arr, arr + sizeof(arr)/sizeof(int)); vector 常用赋值操作12345678assign(beg, end);//将[beg, end)区间中的数据拷贝赋值给本身。assign(n, elem);//将 n 个 elem 拷贝赋值给本身。vector&amp; operator=(const vector &amp;vec);//重载等号操作符swap(vec);// 将 vec 与本身的元素互换。//第一个赋值函数，可以这么写：int arr[] = &#123; 0, 1, 2, 3, 4 &#125;;assign(arr, arr + 5);//使用数组初始化 vector vector 大小操作123456size();//返回容器中元素的个数empty();//判断容器是否为空resize(int num);//重新指定容器的长度为 num，若容器变长，则以默认值填充新位置。如果容器变短，则末尾超出容器长度的元素被删除。resize(int num, elem);//重新指定容器的长度为 num，若容器变长，则以 elem 值填充新位置。如果容器变短，则末尾超出容器长&gt;度的元素被删除。capacity();//容器的容量reserve(int len);//容器预留 len个元素长度，预留位置不初始化，元素不可访问。 resize 若容器变长，则以默认值填充新位置。如果容器变短，则末尾超出容器长度的 元素被删除。 reserve 是容器预留空间，但在空间内不真正创建元素对象，所以在没有添加新的对 象之前，不能引用容器内的元素. resize 是改变容器的大小，且在创建对象，因此，调用这个函数之后，就可以引用容器内的对象了. vector 数据存取操作1234at(int idx); //返回索引 idx 所指的数据，如果 idx 越界，抛出 out_of_range 异常。operator[];//返回索引 idx 所指的数据，越界时，运行直接报错front();//返回容器中第一个数据元素back();//返回容器中最后一个数据元素 vector 插入和删除操作123456insert(const_iterator pos, int count,ele);//迭代器指向位置 pos 插入 count 个元素 ele.push_back(ele); //尾部插入元素 elepop_back();//删除最后一个元素erase(const_iterator start, const_iterator end);//删除迭代器从 start 到 end 之间的元素erase(const_iterator pos);//删除迭代器指向的元素clear();//删除容器中所有元素 总结vector 是个动态数组，当空间不足的时候插入新元素，vector 会重新申请一块更大的 内存空间，将旧空间数据拷贝到新空间，然后释放旧空间。vector 是单口容器，所以在尾 端插入和删除元素效率较高，在指定位置插入，势必会引起数据元素移动，效率较低。","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-string 容器","slug":"C:C++/STL-string-容器","date":"2019-06-07T15:42:37.000Z","updated":"2019-06-07T16:26:51.012Z","comments":true,"path":"post/2d250e1c.html","link":"","permalink":"http://shuiyujie.com/post/2d250e1c.html","excerpt":"string 是对 char * 的封装，具有如下特性： char 是一个指针，string 是一个类。string 实现了对 char 的封装，是 char* 的容器 string 封装了许多字符串操作的方法，比如 find、copy、delete、replace、insert 等 不用考虑内存释放和越界，内存由 string 类负责维护","text":"string 是对 char * 的封装，具有如下特性： char 是一个指针，string 是一个类。string 实现了对 char 的封装，是 char* 的容器 string 封装了许多字符串操作的方法，比如 find、copy、delete、replace、insert 等 不用考虑内存释放和越界，内存由 string 类负责维护 string 与 char* 互相转换1234567// string 转 char*string str = \"www.shuiyujie.com\";const char* cstr = str.c_str();// char* 装 stringchar * s = \"www.shuiyujie.com\";string sstr(s); string 的构造函数123456789101112131415string();//创建一个空的字符串,例如: string str;string(const string&amp; str);//使用一个string对象初始化另一个string对象string(const char* s);//使用字符串 s 初始化 string(int n, charc);//使用 n 个字符 c 初始化//例子://默认构造函数string s1;//拷贝构造函数string s2(s1);string s2 = s1;//带参数构造函数char* str = \"www.shuiyujie.com\";string s3(str);string s4(10, 'a'); string 基本赋值操作12345678string&amp; operator=(const char* s);//char*类型字符串赋值给当前的字符串string&amp; operator=(const string &amp;s);//把字符串 s 赋给当前的字符串string&amp; operator=(char c);//字符赋值给当前的字符串string&amp; assign(const char *s);//把字符串 s 赋给当前的字符串string&amp; assign(const char *s, int n);//把字符串 s 的前 n 个字符赋给当前的字符串string&amp; assign(const string &amp;s);//把字符串 s 赋给当前字符串string&amp; assign(int n, char c);//用 n 个字符 c 赋给当前字符串string&amp; assign(const string &amp;s, int start, int n);//将 s 从 start 开始 n 个字符赋值给字符串 string 取字符串操作1234567char&amp; operator[](int n);//通过[]方式取字符char&amp; at(int n);//通过 at 方法获取字符//例子:string s = \"www.shuiyujie.com\";char c = s[];c = s.at(1); Q: string 中存取字符[]和 at 的异同? A: 相同,[]和 at 都可以返回第 n 个字符 不同，at 访问越界会抛出异常，[]越界会直接程序会挂掉。 string 拼接操作12345678string&amp; operator+=(const string&amp; str);//重载+=操作符string&amp; operator+=(const char* str);//重载+=操作符string&amp; operator+=(const char c);//重载+=操作符string&amp; append(const char *s);//把字符串 s 连接到当前字符串结尾string&amp; append(const char *s, int n);//把字符串 s 的前 n 个字符连接到当前字符串结尾string&amp; append(const string &amp;s);//同 operator+=()string&amp; append(const string &amp;s, int pos, int n);//把字符串 s 中从 pos 开始的 n 个字符连接到当前字符串结尾string&amp; append(int n, char c);//在当前字符串结尾添加 n 个字符 c string 查找和替换12345678910int find(const string&amp; str, int pos = 0) const; //查找 str 第一次出现位置,从 pos 开始查找int find(const char* s, int pos = 0) const; //查找 s 第一次出现位置,从 pos 开始查找int find(const char* s, int pos, int n) const; //从 pos 位置查找 s 的前 n 个字符第一次位置int find(const char c, int pos = 0) const; //查找字符 c 第一次出现位置int rfind(const string&amp; str, int pos = npos) const;//查找 str 最后一次位置,从 pos 开始查找int rfind(const char* s, int pos = npos) const;//查找 s 最后一次出现位置,从 pos 开始查找int rfind(const char* s, int pos, int n) const;//从 pos 查找 s 的前 n 个字符最后一次位置int rfind(const char c, int pos = 0) const; //查找字符 c 最后一次出现位置string&amp; replace(int pos, int n, const string&amp; str); //替换从 pos 开始 n 个字符为字符串 strstring&amp; replace(int pos, int n, const char* s); //替换从 pos 开始的 n 个字符为字符串 s string 比较操作12345678/*compare 函数在&gt;时返回 1， &lt;时返回 -1，==时返回 0。比较区分大小写，比较时参考字典顺序，排越前面的越小。大写的 A 比小写的 a 小。*/int compare(const string &amp;s) const;//与字符串 s 比较int compare(const char *s) const;//与字符串 s 比较 string 子串1string substr(int pos = 0, int n = npos) const;//返回由 pos 开始的 n 个字符组成的字符串 string 插入和删除操作1234string&amp; insert(int pos, const char* s); //插入字符串 string&amp; insert(int pos, const string&amp; str); //插入字符串string&amp; insert(int pos, int n, char c);//在指定位置插入 n 个字符 cstring&amp; erase(int pos, int n = npos);//删除从 Pos 开始的 n 个字符","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"STL-STL基础概念","slug":"C:C++/STL-STL基础概念","date":"2019-06-07T15:15:18.000Z","updated":"2019-06-08T05:03:05.593Z","comments":true,"path":"post/543335f8.html","link":"","permalink":"http://shuiyujie.com/post/543335f8.html","excerpt":"STL(Standard Template Library,标准模板库)，是惠普实验室开发的一系列软件的统 称。现在主要出现在 c++ 中，但是在引入 c++ 之前该技术已经存在很长时间了。 STL 从广义上分为: 容器(container) 算法(algorithm) 迭代器(iterator),容器和算法之 间通过迭代器进行无缝连接。STL 几乎所有的代码都采用了模板类或者模板函数，这相比传 统的由函数和类组成的库来说提供了更好的代码重用机会。","text":"STL(Standard Template Library,标准模板库)，是惠普实验室开发的一系列软件的统 称。现在主要出现在 c++ 中，但是在引入 c++ 之前该技术已经存在很长时间了。 STL 从广义上分为: 容器(container) 算法(algorithm) 迭代器(iterator),容器和算法之 间通过迭代器进行无缝连接。STL 几乎所有的代码都采用了模板类或者模板函数，这相比传 统的由函数和类组成的库来说提供了更好的代码重用机会。 STL(Standard Template Library)标准模板库,在我们 c++ 标准程序库中隶属于 STL 的 占到了 80%以上。 在 c++标准中，STL 被组织成以下 13 个头文件： 1&lt;algorithm&gt;、&lt;deque&gt;、&lt;functional&gt;、&lt;iterator&gt;、&lt;vector&gt;、&lt;list&gt;、&lt;map&gt;、 &lt;memory&gt;、&lt;numeric&gt;、&lt;queue&gt;、&lt;set&gt;、&lt;stack&gt; 、&lt;utility&gt; 容器 容器即数据结构，比如说 vector 是动态数组，stack 是栈。 容器可以嵌套容器 容器可以分为序列化容器和关联式容器。序列化容器即按照元素进入容器的先后顺序来排列；关联式容器即按照容器定义的其他规则来存放元素。 迭代器：迭代器是为了遍历容器中的元素，可以理解是指针 算法：STL 为我们提供的算法，算法即用有限的步骤解决问题 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;//----------------------------------// stl 包括三部分：容器、迭代器、算法//----------------------------------using namespace std;int main()&#123; vector&lt;int&gt; v; // 容器，stl 的标准容器之一，动态数组 v.push_back(1); // Vector 容器提供的插入数据的方法 v.push_back(3); v.push_back(6); v.push_back(4); v.push_back(3); vector&lt;int&gt;::iterator pBegin = v.begin(); // 迭代器，获取指向第一个元素的指针 vector&lt;int&gt;::iterator pEnd = v.end(); // 迭代器，获取指向最后一个元素的指针 int n = count(pBegin, pEnd, 3); // 算法，统计 3 出现的次数 cout &lt;&lt; \"n = \" &lt;&lt; n &lt;&lt; endl; // 使用迭代器遍历 vector while(pBegin != pEnd) &#123; cout &lt;&lt; *pBegin &lt;&lt; endl; pBegin++; &#125; return 0;&#125; 接下来一系列的文章将介绍 STL 中的常用容器和常用算法。","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"stl","slug":"stl","permalink":"http://shuiyujie.com/tags/stl/"}]},{"title":"图像处理——边缘检测","slug":"图像处理/边缘检测","date":"2019-06-02T11:08:59.000Z","updated":"2019-06-02T16:49:29.990Z","comments":true,"path":"post/1d0f563b.html","link":"","permalink":"http://shuiyujie.com/post/1d0f563b.html","excerpt":"边缘检测是为了识别物体的边缘，而边缘是由数字图像中亮度变化明显的点连接而成的，主要可以通过基于图像强度的一阶和二阶导数来寻找到这些点。 边缘检测可以分成三个步骤： 滤波。由于导数对噪声敏感，所以在边缘检测之前可以先试着降低图片的噪声，常用的是高斯滤波。 增强。增强算法可以将图像灰度点邻域强度值有显著变化的点凸显出来，可以通过计算梯度幅值来确定。 检测。根据梯度幅值就可以检测出物体的边缘，由于经过图像增强，有些店并不是要找的边缘值，我们还可以通过阈值化的方式来筛选。","text":"边缘检测是为了识别物体的边缘，而边缘是由数字图像中亮度变化明显的点连接而成的，主要可以通过基于图像强度的一阶和二阶导数来寻找到这些点。 边缘检测可以分成三个步骤： 滤波。由于导数对噪声敏感，所以在边缘检测之前可以先试着降低图片的噪声，常用的是高斯滤波。 增强。增强算法可以将图像灰度点邻域强度值有显著变化的点凸显出来，可以通过计算梯度幅值来确定。 检测。根据梯度幅值就可以检测出物体的边缘，由于经过图像增强，有些店并不是要找的边缘值，我们还可以通过阈值化的方式来筛选。 canny Canny边缘检测算子是澳洲计算机科学家约翰·坎尼（John F. Canny）于1986年开发出来的一个多级边缘检测算法。更为重要的是Canny创立了“边缘检测计算理论”（computational theory of edge detection）解释这项技术如何工作。 Canny的目标是找到一个最优的边缘检测算法，最优边缘检测的含义是： 好的检测 - 算法能够尽可能多地标识出图像中的实际边缘。 好的定位 - 标识出的边缘要与实际图像中的实际边缘尽可能接近。 最小响应 - 图像中的边缘只能标识一次，并且可能存在的图像噪声不应标识为边缘。 为了满足这些要求Canny使用了变分法，这是一种寻找满足特定功能的函数的方法。最优检测使用四个指数函数项的和表示，但是它非常近似于高斯函数的一阶导数。 —— 维基百科 我们可以使用 OpenCV 中的 Canny() 来做边缘检测。 1234567void cv::Canny ( InputArray image, OutputArray edges, double threshold1, double threshold2, int apertureSize = 3, bool L2gradient = false ) src: 源图像，8位即可 edges: 输出的边缘图，要求与源图像保持一样的尺寸和类型 threshold1: 第一个滞后性阈值 threshold2: 第二个滞后性阈值 apertureSize: 表示应该用 Sobel 算子的空间大小，默认为 3 L2gradient: 计算图像梯度幅值的标识，默认为 false 注：threshold1 和 threshold2 中较小的用于边缘连接，交大的用来控制强边缘的初始段，推荐的高低阈值比在 2:1 到 3:1 之间。 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include \"opencv2/highgui.hpp\"#include \"opencv2/imgcodecs.hpp\"#include \"opencv2/imgproc.hpp\"int main()&#123; cv::Mat src = cv::imread(\"../images/persimmon.jpg\", cv::IMREAD_COLOR); cv::Mat dst, edge, gray; // 【1】创建与src同类型和大小的矩阵(dst) dst.create(src.size(), src.type()); // 【2】将原图像转换为灰度图像 cv::cvtColor(src, gray, cv::COLOR_BGR2GRAY); // 【3】运行Canny算子 cv::Canny(gray, edge, 150, 50, 3); // 【4】显示效果图 cv::namedWindow(\"【原图】边缘检测\", cv::WINDOW_NORMAL); cv::namedWindow(\"【效果图】Canny边缘检测\", cv::WINDOW_NORMAL); imshow(\"【原图】边缘检测\", gray); imshow(\"【效果图】Canny边缘检测\", edge); cv::waitKey(0);&#125; sobel 索伯算子(Sobel operator) 是图像处理中的算子之一，有时又称为索伯-费德曼算子或索贝滤波器，在影像处理及电脑视觉领域中常被用来做边缘检测。索伯算子最早是由美国计算机科学家艾尔文·索伯（Irwin Sobel）及盖瑞·费德曼（Gary Feldman）于1968年在史丹佛大学的人工智能实验室(SAIL)所提出，因此为了表扬他们的贡献，才用他们的名字命名。在技术上，它是一离散性差分算子，用来运算图像亮度函数的梯度之近似值。在图像的任何一点使用此算子，索伯算子的运算将会产生对应的梯度向量或是其范数。概念上，索伯算子就是一个小且是整数的滤波器对整张影像在水平及垂直方向上做卷积，因此它所需的运算资源相对较少，另一方面，对于影像中的频率变化较高的地方，它所得的梯度之近似值也比较粗糙。 —— 维基百科 Sobel 函数使用扩展的 Sobel 算子来计算一阶、二阶、三阶或者混合图像的差分，来看一下 OpenCV 中的 Sobel()。 12345678910void cv::Sobel ( InputArray src, OutputArray dst, int ddepth, int dx, int dy, int ksize = 3, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT ) src: 源图像，Mat 类型 dst: 输出图像，尺寸和类型需要与 src 相同 ddepth: 输出图像深度，支持如下 src.depth() 和 ddepth 的组合 dx: x 方向上的差分阶数 dy: y 方向上的差分阶数 ksize: Sobel 卷积核的代销，默认值为 3，必须取 1、3、5 或 7 scale: 计算导数值时可选的缩放因子，默认值是 1，表示默认情况下是没有应用缩放的，更多可以查看文档 delta: 表示结果存入输出图像之前可选的 delta 值，默认为 0 borderType: 推断图像外包部像素的某种边界模式，一般用默认的即可 注： 当内核大小为 3 时，Sobel 内核可能产生比较明显的误差。为此 OpenCV 提供了结果更加精确的 Scharr() ，它仅作用于大小为 3 的卷积核，运行速度与 Sobel() 一样快，当结果更精确。 计算图像 X 方向导数是可以取 xorder = 1, uorder = 0, ksize = 3 计算图像 Y 方向导数是可以取 xorder = 0, uorder = 1, ksize = 3 123456789101112131415161718192021222324252627282930313233343536373839//---------------【边缘检测】----------------// 描述：sobel 函数用法示例//------------------------------------------#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace cv;int main()&#123; //【0】创建 grad_x 和 grad_y 矩阵 Mat grad_x, grad_y; Mat abs_grad_x, abs_grad_y,dst; //【1】载入原始图 Mat src = imread(\"images/2.jpg\"); //工程目录下应该有一张名为1.jpg的素材图 //【2】显示原始图 imshow(\"【原始图】sobel边缘检测\", src); //【3】求 X方向梯度 Sobel( src, grad_x, CV_16S, 1, 0, 3, 1, 1, BORDER_DEFAULT ); convertScaleAbs( grad_x, abs_grad_x ); imshow(\"【效果图】 X方向Sobel\", abs_grad_x); //【4】求Y方向梯度 Sobel( src, grad_y, CV_16S, 0, 1, 3, 1, 1, BORDER_DEFAULT ); convertScaleAbs( grad_y, abs_grad_y ); imshow(\"【效果图】Y方向Sobel\", abs_grad_y); //【5】合并梯度(近似) addWeighted( abs_grad_x, 0.5, abs_grad_y, 0.5, 0, dst ); imshow(\"【效果图】整体方向Sobel\", dst); waitKey(0); return 0; &#125; Laplacian 在数学以及物理中，拉普拉斯算子或是拉普拉斯算符（英语：Laplace operator, Laplacian）是由欧几里得空间中的一个函数的梯度的散度给出的微分算子，通常写成 、 或。 这名字是为了纪念法国数学家皮埃尔-西蒙·拉普拉斯（1749–1827）而命名的。他在研究天体力学在数学中首次应用算子，当它被施加到一个给定的重力位（Gravitational potential）的时候，其中所述算子给出的质量密度的常数倍。经拉普拉斯算子运算为零∆f=0的函数称为调和函数，现在称为拉普拉斯方程，和代表了在自由空间中的可能的重力场。 拉普拉斯算子有许多用途，此外也是椭圆算子中的一个重要例子。 拉普拉斯算子出现描述许多物理现象的微分方程里。例如，常用于波方程的数学模型、热传导方程、流体力学以及亥姆霍兹方程。在静电学中，拉普拉斯方程和泊松方程的应用随处可见。在量子力学中，其代表薛定谔方程中的动能项。 拉普拉斯算子是最简单的椭圆算子，并且拉普拉斯算子是霍奇理论的核心，并且是德拉姆上同调的结果。在图像处理和计算机视觉中，拉普拉斯算子已经被用于诸如斑点检测和边缘检测等的各种任务。 Laplacian 使用了图像梯度，它内部的代码调用了 Sobel 算子。让一幅图像减去它的 Laplacian 算子可以增强对比度。 123456789void cv::Laplacian ( InputArray src, OutputArray dst, int ddepth, int ksize = 1, double scale = 1, double delta = 0, int borderType = BORDER_DEFAULT ) src: 源图像，Mat 类型，需要为单通道 8 位图像 dst: 输出图像，尺寸和类型需要与 src 相同 ddepth: 输出图像深度 ksize: 用于计算二阶大数的滤波器的孔径尺寸，大小必须正奇数，默认值为 1 scale: 计算拉普拉斯值的时候可选的比例因子，默认值为 1 delta: 在结果存入输出图像全可选的 delta 值，默认值为 0 borderType: 推断图像外包部像素的某种边界模式，一般用默认的即可 注：Laplacian() 函数使用 sobel 运算，加上 sobel 算子运算出图像 dx 和 dy，来得到载入图像的拉普拉斯变换的结果。 12345678910111213141516171819202122232425262728293031323334//---------------【边缘检测】----------------// 描述：laplacian 函数用法示例//------------------------------------------#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;int main()&#123; cv::Mat src, src_gray, dst, abs_dst; // 载入原图像 src = cv::imread(\"images/3.jpg\", cv::IMREAD_COLOR);; cv::imshow(\"【原图】\", src); // 使用高斯滤波降噪 cv::GaussianBlur(src, src, cv::Size(3, 3), 0, 0, cv::BORDER_DEFAULT); // 转换为灰度图 cv::cvtColor(src, src_gray, cv::COLOR_BGR2GRAY); // 使用 Laplacian 函数 cv::Laplacian(src_gray, dst, CV_16S, 3, 1, 0, cv::BORDER_DEFAULT); //【6】计算绝对值，并将结果转换成8位 convertScaleAbs( dst, abs_dst ); //【7】显示效果图 cv::imshow( \"【效果图】图像Laplace变换\", abs_dst ); cv::waitKey(0); return 0;&#125;","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://shuiyujie.com/categories/图像处理/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"},{"name":"图像处理","slug":"图像处理","permalink":"http://shuiyujie.com/tags/图像处理/"}]},{"title":"OpenCV-Mat表达式","slug":"OpenCV/OpenCV-Mat表达式","date":"2019-06-02T09:49:20.000Z","updated":"2019-06-02T09:56:58.835Z","comments":true,"path":"post/1208a55a.html","link":"","permalink":"http://shuiyujie.com/post/1208a55a.html","excerpt":"","text":"利用 C++中的运算符重载，OpenCV 2 中引入了 Mat 运算表达式。这一新特 点使得使用 C++进行编程时，就如同写 Matlab 脚本，代码变得简洁易懂，也便于维护。 下面给出 Mat 表达式所支持的运算。下面的列表中使用 A 和 B 表示 Mat 类 型的对象，使用 s 表示 Scalar 对象，alpha 表示 double 值。 加法，减法，取负：A+B，A-B，A+s，A-s，s+A，s-A，-A 缩放取值范围：A*alpha 矩阵对应元素的乘法和除法： A.mul(B)，A/B，alpha/A 矩阵乘法：A*B （注意此处是矩阵乘法，而不是矩阵对应元素相乘） 矩阵转置：A.t() 矩阵求逆和求伪逆：A.inv() 矩阵比较运算：A cmpop B，A cmpop alpha，alpha cmpop A。此处 cmpop 可以是&gt;，&gt;=，==，!=，&lt;=，&lt;。如果条件成立，则结果矩阵（8U 类型矩阵）的对应元素被置为 255；否则置 0。 矩阵位逻辑运算：A logicop B，A logicop s，s logicop A，~A，此处 logicop 可以是&amp;，|和^。 矩阵对应元素的最大值和最小值：min(A, B)，min(A, alpha)，max(A, B)， max(A, alpha)。 矩阵中元素的绝对值：abs(A) 叉积和点积：A.cross(B)，A.dot(B)","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://shuiyujie.com/categories/OpenCV/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"}]},{"title":"图像处理——图像降噪","slug":"图像处理/图像降噪","date":"2019-06-02T09:49:20.000Z","updated":"2019-08-24T01:19:47.187Z","comments":true,"path":"post/d966aba8.html","link":"","permalink":"http://shuiyujie.com/post/d966aba8.html","excerpt":"数字图像在数字化和成像过程中会受到成像设备或外界环境的影响，受到干扰产生的图像叫做噪声图像。 按照噪声的引入方式分类，可以将噪声分成加性噪声和乘法性噪声。加性噪声的幅度与信号的幅度无关，是叠加在图像上的，比较容易去除。成性噪声的幅度与信号的幅度成正比，比较难去除。不过乘性噪声可以通过取对数的方式转化为加性噪声，实际上大部分去噪算法都会假设噪声为加性高斯白噪声。 按照噪声的性质分类，可以将噪声分成脉冲噪声(Impluse Noise))，椒盐噪声(Pepper-Salt noise)和⾼斯⽩噪声(Gaussian white noise)，莱斯噪声(Racian noise)等。 传统降噪算法根据降噪的原理不同可分为基于邻域像素特征的⽅法，基于频域变换的⽅法，和基于特定模型的⽅法。本文主要讲述基于邻域像素特征的方法，并用 OpenCV 实现相应算法。","text":"数字图像在数字化和成像过程中会受到成像设备或外界环境的影响，受到干扰产生的图像叫做噪声图像。 按照噪声的引入方式分类，可以将噪声分成加性噪声和乘法性噪声。加性噪声的幅度与信号的幅度无关，是叠加在图像上的，比较容易去除。成性噪声的幅度与信号的幅度成正比，比较难去除。不过乘性噪声可以通过取对数的方式转化为加性噪声，实际上大部分去噪算法都会假设噪声为加性高斯白噪声。 按照噪声的性质分类，可以将噪声分成脉冲噪声(Impluse Noise))，椒盐噪声(Pepper-Salt noise)和⾼斯⽩噪声(Gaussian white noise)，莱斯噪声(Racian noise)等。 传统降噪算法根据降噪的原理不同可分为基于邻域像素特征的⽅法，基于频域变换的⽅法，和基于特定模型的⽅法。本文主要讲述基于邻域像素特征的方法，并用 OpenCV 实现相应算法。 基于邻域像素特征的⽅法，是通过分析在⼀定⼤⼩的窗口内，中⼼像素与其他相邻像素之间在灰度空间的直接联系，来获取新的中⼼像素值的⽅法，因此往往都会存在⼀个典型的输⼊参数，即滤波半径 r。此滤波半径可能被⽤于在该局部窗⼜内计算像素的相似性，也可能是⼀些⾼斯或拉普拉斯算⼦的计算窗口。在邻域滤波⽅法⾥⾯，最具有代表性的滤波⽅法有以下⼏种。 算数均值滤波 上图演示 6x6 的图片，采用边框补零的方式，与 3x3 的卷积核做步长为 1 的卷积操作。 原理：均值滤波用像素邻域的平均灰度来代替像素值，适用于脉冲噪声，因为脉冲噪声一般与周围像素的灰度级不相关，而且亮度高出其他像素很多。 缺点：均值滤波随着半径取值增大而变得越来越模糊，因为均值操作之后，噪声被分散到了周围像素点，但是噪声区域也不可避免地增大了。 解决：采用设定阈值的方式可以一定程度上避免这一问题。比较噪声和领域像素的灰度，只有当灰度差达到设定的阈值才判定为噪声。 实现：可以使用 OpenCV 中的 blur() 实现算数均值滤波的效果。 123456void cv::blur ( InputArray src, OutputArray dst, Size ksize, Point anchor = Point(-1,-1), int borderType = BORDER_DEFAULT ) src: 带有噪声的原图像，深度必须为 CV_8U, CV_16U, CV_16S, CV_32F or CV_64F 中的一种 dst: 去噪之后的图像，要求图像大小与噪声图像一样，可以使用 Mat::Clone 的方法实现 ksize: 卷积核的大小，比如说 Size(3, 3) 表示大小为 3x3 的矩阵 point: Point 类型的 anchor，采用默认值表示取卷积核的中心为锚点 borderType: 推断图像外包部像素的某种边界模式，一般用默认的即可 1cv::blur(src, meanFilter, cv::Size(6, 6)); 高斯滤波原理：用一个卷积核扫描图像中的每一个像素，确定领域内像素的加权平均灰度值去代替模板中心像素点的值。图像高斯模糊的过程就是图像与正态分布做卷积，正态分布也叫高斯分布，所以也叫高斯模糊技术。 优点：相比于均值滤波，高斯滤波矩阵的权值随着与中心像素点距离的增加呈现高斯衰减的特性，因此对距离算子越远的像素点作用越小，从⽽能在⼀定程度上保持图像的边缘特征。通过调节⾼斯平滑参数，可以在图像特征过分模糊和⽋平滑之间取得折中。 实现：可以使用 OpenCV 中的 GaussianBlur() 实现算数均值滤波的效果。 1234567void cv::GaussianBlur ( InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY = 0, int borderType = BORDER_DEFAULT ) src: 带有噪声的原图像，深度必须为 CV_8U, CV_16U, CV_16S, CV_32F or CV_64F 中的一种 dst: 去噪之后的图像，要求图像大小与噪声图像一样，可以使用 Mat::Clone 的方法实现 ksize: 高斯滤波矩阵的代销，ksize.width and ksize.height 可以不同，但必须为正数、奇数或零，它们是用过 signma 计算得到的。 sigmaX: 高斯核函数在 X 方向的标准偏差 sigmaY: 高斯核函数在 Y 方向的标准偏差。若 sigmaY 为 0 就将其设为 sigmaX；若 sigmaX 和 sigmaY 都为 0，则通过 ksize.width and ksize.height 计算得出。 borderType: 推断图像外包部像素的某种边界模式，一般用默认的即可 1cv::GaussianBlur(src, gaussianBlur, cv::Size(5, 5), 0, 0); 统计中值滤波原理：对窗口内的像素值进⾏排序，然后使⽤灰度值的中间值代替窗口中⼼位置像素的灰度，适⽤于椒盐噪声和脉冲噪声。 优点：中值滤波属于非线性滤波，当噪声是散粒噪声而不是高斯噪声时，即图像偶尔会出现很大的值时，用高斯滤波对图像进行模糊噪声像素不会被去除，而中值滤波⽐相同尺⼨的线性平滑滤波器引起的模糊更少，能较好的保持边缘。 缺点：中值滤波会使图像中的⼩⽬标丢失。当噪声像素个数⼤于窗口像素总数的⼀半时，由于灰度排序的中间值仍为噪声像素灰度值，会导致滤波效果很差，且中值滤波花费的时间是均值滤波的 5 倍以上。 实现：可以使用 OpenCV 中的 medianBlur() 实现中值滤波。 1234void cv::medianBlur ( InputArray src, OutputArray dst, int ksize) src: 带有噪声的原图像，深度必须为 CV_8U, CV_16U, CV_16S, CV_32F or CV_64F 中的一种 dst: 去噪之后的图像，要求图像大小与噪声图像一样，可以使用 Mat::Clone 的方法实现 ksize: aperture linear size，必须为大于 1 的奇数，比如：3、5、7、9…… 1cv::medianBlur(src, medianBlur, 3); 双边滤波原理：滤波器是由两个函数构成。⼀个函数是由⼏何空间距离决定滤波器系数，另⼀个由像素差值决定滤波器系数。双边滤波器中，输出像素的值依赖于邻域像素的值的加权组合，它同时考虑了空间域与值域的差别。 优点：可以做到边缘保存，在边缘附近，距离较远的像素不会对边缘上的像素造成太大影响。 缺点：由于保存了过多的高频信息，对于图像里的高频噪声，双边滤波器不能干净地过滤掉，只能对于低频信号进行较好的滤波。 实现：可以使用 OpenCV 中的 bilateralFilter() 实现双边滤波的效果。 1234567void cv::bilateralFilter ( InputArray src, OutputArray dst, int d, double sigmaColor, double sigmaSpace, int borderType = BORDER_DEFAULT ) src: 带有噪声的原图像，深度必须为 CV_8U, CV_16U, CV_16S, CV_32F or CV_64F 中的一种 dst: 去噪之后的图像，要求图像大小与噪声图像一样，可以使用 Mat::Clone 的方法实现 d: 表示在过滤过程中每个像素邻域的直径，如果这个值被设为非正数，那么 OpenCV 会从第 5 个参数 sigmaSpace 来计算出它 sigmaColor: 颜色空间滤波器 sigma 值，这个参数的值越大，就表名该像素邻域范围内有越宽广的颜色会被很合到一起，产生较大的半相等颜色区域 sigmaSpace: 做表空间中滤波器的 sigma 值，坐标空间的标注方差。它的数值越大，意味着越远的像素会相互影响，从而使更大的区域中足够相似的颜色获得相同的颜色。当 d&gt;0 时，d 指定了邻域大小且与 sigmaSpace 无关。否则，d 正比于 sigmaSpace。 borderType: 推断图像外包部像素的某种边界模式，一般用默认的即可 1cv::bilateralFilter(src,bilateralfilter,25,25*2,25/2); 代码与效果演示1234567891011121314151617181920212223242526272829303132333435363738/** * 使用 4 种空域滤波方法,对 noise 图进行降噪 * * 1. 算数均值滤波 * 2. 高斯滤波 * 3. 统计中值滤波 * 4. 双边滤波 * **/#include &lt;iostream&gt;#include \"opencv2/imgproc.hpp\"#include \"opencv2/highgui.hpp\"#include \"opencv2/imgcodecs.hpp\"int main()&#123; cv::Mat src = cv::imread(\"../images/noise.jpg\", cv::IMREAD_COLOR); cv::Mat meanFilter; cv::Mat medianBlur; cv::Mat gaussianBlur; cv::Mat bilateralfilter; cv::blur(src, meanFilter, cv::Size(6, 6)); cv::GaussianBlur(src, gaussianBlur, cv::Size(5, 5), 0, 0); cv::medianBlur(src, medianBlur, 3); cv::bilateralFilter(src,bilateralfilter,25,25*2,25/2); cv::imshow(\"1. src\", src); cv::imshow(\"2. meanFilter\", meanFilter); cv::imshow(\"3. gaussianBlur\", gaussianBlur); cv::imshow(\"4. medianBlur\", medianBlur); cv::imshow(\"5. bilateralfilter\", bilateralfilter); cv::waitKey(0); return 0;&#125;","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://shuiyujie.com/categories/图像处理/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"},{"name":"图像处理","slug":"图像处理","permalink":"http://shuiyujie.com/tags/图像处理/"}]},{"title":"OpenCV-选取图像的感兴趣区域","slug":"OpenCV/OpenCV-选取图像的感兴趣区域","date":"2019-06-02T09:41:57.000Z","updated":"2019-06-02T09:47:13.928Z","comments":true,"path":"post/d4c8e253.html","link":"","permalink":"http://shuiyujie.com/post/d4c8e253.html","excerpt":"Mat 类提供了多种方便的方法来选择图像的局部区域。使用这些方法时需要注意,这些方法并不进行内存的复制操作。如果将局部区域赋值给新的 Mat 对象,新对象与原始对象共用相同的数据区域,不新申请内存,因此这些方法的执行速度都比较快。","text":"Mat 类提供了多种方便的方法来选择图像的局部区域。使用这些方法时需要注意,这些方法并不进行内存的复制操作。如果将局部区域赋值给新的 Mat 对象,新对象与原始对象共用相同的数据区域,不新申请内存,因此这些方法的执行速度都比较快。 单行或者单列选择提取矩阵的一行或者一列可以使用函数 row()或 col()。函数的声明如下: 12Mat Mat::row(int i) constMat Mat::col(int j) const 参数 i 和 j 分别是行标和列标。例如取出 A 矩阵的第 i 行可以使用如下代码: 1Mat line = A.row(i); 例如取出 A 矩阵的第 i 行,将这一行的所有元素都乘以 2,然后赋值给第 j 行,可以这样写: 1A.row(j) = A.row(i)*2; 用 Range 选择多行或多列Range 是 OpenCV 中新增的类,该类有两个关键变量 star 和 end。Range 对象可以用来表示矩阵的多个连续的行或者多个连续的列。其表示的范围为从 start到 end,包含 start,但不包含 end。Range 类的定义如下: 123456class Range&#123; public: ... int start, end;&#125;; Range 类还提供了一个静态方法 all(),这个方法的作用如同 Matlab 中的“:”,表示所有的行或者所有的列。 1234567//创建一个单位阵Mat A = Mat::eye(10, 10, CV_32S);//提取第 1 到 3 列(不包括 3)Mat B = A(Range::all(), Range(1, 3));//提取 B 的第 5 至 9 行(不包括 9)//其实等价于 C = A(Range(5, 9), Range(1, 3))Mat C = B(Range(5, 9), Range::all()); 感兴趣区域从图像中提取感兴趣区域(Region of interest)有两种方法,一种是使用构造函数,如下例所示: 1234//创建宽度为 320,高度为 240 的 3 通道图像Mat img(Size(320,240),CV_8UC3);//roi 是表示 img 中 Rect(10,10,100,100)区域的对象Mat roi(img, Rect(10,10,100,100)); 除了使用构造函数,还可以使用括号运算符,如下: 1Mat roi2 = img(Rect(10,10,100,100)); 当然也可以使用 Range 对象来定义感兴趣区域,如下: 1234//使用括号运算符Mat roi3 = img(Range(10,100),Range(10,100));//使用构造函数Mat roi4(img, Range(10,100),Range(10,100)); 取对角线元素矩阵的对角线元素可以使用 Mat 类的 diag()函数获取,该函数的定义如下: 1Mat Mat::diag(int d) const 参数 d=0 时,表示取主对角线;当参数 d&gt;0 是,表示取主对角线下方的次对角线,如 d=1 时,表示取主对角线下方,且紧贴主多角线的元素;当参数 d&lt;0 时,表示取主对角线上方的次对角线。 如同 row()和 col()函数, diag()函数也不进行内存复制操作,其复杂度也是 O(1)。","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://shuiyujie.com/categories/OpenCV/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"}]},{"title":"OpenCV-基本数据结构","slug":"OpenCV/OpenCV-基本数据结构","date":"2019-05-25T04:22:04.000Z","updated":"2019-05-25T05:03:27.692Z","comments":true,"path":"post/463fc5c6.html","link":"","permalink":"http://shuiyujie.com/post/463fc5c6.html","excerpt":"本文介绍 OpenCV 的基本数据结构，做到心中有数就不会在阅读示例代码的时候发憷。","text":"本文介绍 OpenCV 的基本数据结构，做到心中有数就不会在阅读示例代码的时候发憷。 Mat 类Mat 是 OpenCV 中最重要的一种数据结构，OpenCV 将其定义为一个类，用于存储图像矩阵。 属性 释义 dims 矩阵的维度，如 3x4x5 的矩阵为 3 维 data uchar 类型指针, 指向矩阵数据内存 rows, cols 矩阵的行数、列数 type 矩阵元素类型 + 通道数 depth 像素位数(bist) channels 通道数量 elemSize 矩阵中每一个元素的数据大小 elemSize1 单通道的矩阵元素占用的数据大小 type 表示矩阵元素类型和通道数。矩阵元素类型一般都是 8 位无符号整数，即 CV_8U，彩色图像一般为 3 通道，灰度图像则为单通道。所以灰度图像的 type 可以表示为 CV_8UC1，3 通道的 RGB 图像的 type 可以表示为 CV_8UC3。 以 CV_8UC3 为例，depth = CV_8U，channels = 3。 elemSize = channels depth / 8 ，type是CV_8UC3，elemSize = 3 8 / 8 = 3bytes。 elemSize1 = depth / 8，type是CV_8UC3，elemSize1 = 8 / 8 = 1bytes。 通过遍历图像可以体会 Mat 类的使用，可以参考 OpenCV-图像的遍历。 Ponit 类 - 点的表示Point 类表示二维坐标系下的点，即 (x, y) 形式的 2D 点。 1234567// 方法一Point point1 = Point(10, 8);// 方法二Point point2;point2.x = 8;point2.y = 10; 其中 Point_&lt;int&gt;, Point2i, Point 等价，Point_&lt;float&gt;, Point2f 等价。 Scalar 类 - 颜色的表示Scalar() 表示具有 4 个元素的数组，用于传递像素值，如 RGB 颜色值。RGB 颜色值只有三个参数，Scalar() 用不到第四个参数可以缺省。 1Mat M(3, 2, CV_8UC3, Scalar(0,0,255)); 其中 Scalar(0,0,255) 表示 BGR 的值分别为 (0, 0, 255) 是红色。 Size 类 - 尺寸的表示12345typedef Size_&lt;int&gt; Size2i;typedef Size_&lt;int64&gt; Size2l;typedef Size_&lt;float&gt; Size2f;typedef Size_&lt;double&gt; Size2d;typedef Size2i Size; 源代码中的定义上述代码所示。Size_ 是模板类，Size_&lt;int&gt; 表示其模板类型为 int。之后给 Size_&lt;int&gt; 别名为 Size2i，再给 Size2i 别名为 Size。由此可见，Size_&lt;int&gt;, Size2i, Size 等价。 Size size = Size(8, 10); 表示宽和长分别为 8 和 10。 Rect 类 - 矩形的表示 Rect 类的成员变量有 x, y, width, heigh Size() 返回值为 Size area() 返回矩形面积 contains(Point) 判断点是否在矩形内 inside(Rect) 判断矩形是否在该矩形内 tl() 返回左上角点坐标 br() 返回右下角点坐标 12345678// 矩形交集Rect rect = rect1 &amp; rect2;// 矩形并集Rect rect = rect1 | rect2;// 平移操作Rect rectShift = rect + point;// 缩放操作Rect rectScale = rect + size;","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://shuiyujie.com/categories/OpenCV/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"}]},{"title":"OpenCV-图像的遍历","slug":"OpenCV/OpenCV-图像的遍历","date":"2019-05-25T01:19:43.000Z","updated":"2019-05-25T04:57:48.868Z","comments":true,"path":"post/dfe68fea.html","link":"","permalink":"http://shuiyujie.com/post/dfe68fea.html","excerpt":"计算机使用 0/1 编码存储图像，数字图像在计算机中同样也使用 0/1 编码来存储。在计算机看来图像是一堆亮度不同的点组成的矩阵。一般灰度图用 2 维矩阵来表示，彩色图片是多通道的，则用 3 维矩阵来表示。 我们一般接触的图像都是 8 位整数（CV_8U），所以灰度图像包含 0～255 灰度，其中 0 代表最⿊，1表⽰最⽩。 彩色图像比如 RGB 图像，每个像素用三个字节来表示，而 OpenCV 中存储 RGB 图像以 BGR 的顺序存储图像，所以存储方式如上所示。 本文将介绍遍历灰度图像和彩色图像的方法，其本质即为遍历图像矩阵，可以对比二维数组的遍历来学习。","text":"计算机使用 0/1 编码存储图像，数字图像在计算机中同样也使用 0/1 编码来存储。在计算机看来图像是一堆亮度不同的点组成的矩阵。一般灰度图用 2 维矩阵来表示，彩色图片是多通道的，则用 3 维矩阵来表示。 我们一般接触的图像都是 8 位整数（CV_8U），所以灰度图像包含 0～255 灰度，其中 0 代表最⿊，1表⽰最⽩。 彩色图像比如 RGB 图像，每个像素用三个字节来表示，而 OpenCV 中存储 RGB 图像以 BGR 的顺序存储图像，所以存储方式如上所示。 本文将介绍遍历灰度图像和彩色图像的方法，其本质即为遍历图像矩阵，可以对比二维数组的遍历来学习。 使用 at() 遍历图像1234567891011121314151617181920212223242526272829303132333435363738#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;int main()&#123; // 使用构造器创建 Mat，注意类型 cv::Mat grayImage(400, 500, CV_8UC1); cv::Mat colorImage(400, 500, CV_8UC3); for(int i = 0; i &lt; grayImage.rows; i++) &#123; for(int j = 0; j &lt; grayImage.cols; j++) &#123; // 灰度图像是单通道的 grayImage.at&lt;uchar&gt;(i, j) = (i + j) % 255; &#125; &#125; for(int i = 0; i &lt; colorImage.rows; i++) &#123; for(int j = 0; j &lt; colorImage.cols; j++) &#123; // 三通道用向量来表示 cv::Vec3b pixel; pixel[0] = i % 255; pixel[1] = j % 255; pixel[2] = 0; colorImage.at&lt;cv::Vec3b&gt;(i, j) = pixel; &#125; &#125; cv::imshow(\"1. GrayImage\", grayImage); cv::imshow(\"2. ColorImage\", colorImage); cv::waitKey(0); return 0;&#125; at() 的优点在于可读性强，缺点在于效率不高。图像遍历操作时常用且开销很大的操作，所以不推荐使用 at() 做图像的遍历操作。 使用指针遍历1234567891011121314151617181920212223242526272829303132333435363738#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;int main()&#123; cv::Mat grayImage(400, 500, CV_8UC1); cv::Mat colorImage(400, 500, CV_8UC3); for(int i = 0; i &lt; grayImage.rows; i++) &#123; uchar *p = grayImage.ptr&lt;uchar&gt;(i); for(int j =0; j &lt; grayImage.cols; j++) &#123; p[j] = (i + j) % 255; &#125; &#125; for(int i = 0; i &lt; colorImage.rows; i++) &#123; cv::Vec3b *p = colorImage.ptr&lt;cv::Vec3b&gt;(i); for(int j =0; j &lt; colorImage.cols; j++) &#123; p[j][0] = i % 255; p[j][1] = j % 255; p[j][2] = 0; &#125; &#125; cv::imshow(\"1. GrayImage\", grayImage); cv::imshow(\"2. ColorImage\", colorImage); cv::waitKey(0); return 0;&#125; 程序是用 image.ptr() 返回矩阵每一行的头指针，紧接着遍历这一行的元素。 指针的优点在于访问速度快，缺点在于指针较为复杂，不熟悉指针的朋友容易犯错且不易查找错误。 迭代器遍历1234567891011121314151617181920212223242526272829303132#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;int main()&#123; cv::Mat grayImage(400, 500, CV_8UC1); cv::Mat colorImage(400, 500, CV_8UC3); cv::MatIterator_&lt;uchar&gt; grayit, grayend; for (grayit = grayImage.begin&lt;uchar&gt;(), grayend = grayImage.end&lt;uchar&gt;(); grayit != grayend; grayit++) &#123; *grayit = rand() % 255; &#125; cv::MatIterator_&lt;cv::Vec3b&gt; colorit, colorend; for (colorit = colorImage.begin&lt;cv::Vec3b&gt;(), colorend = colorImage.end&lt;cv::Vec3b&gt;(); colorit != colorend; colorit++) &#123; (*colorit)[0] = rand() % 255; (*colorit)[1] = rand() % 255; (*colorit)[2] = rand() % 255; &#125; cv::imshow(\"1. GrayImage\", grayImage); cv::imshow(\"2. ColorImage\", colorImage); cv::waitKey(0); return 0;&#125; 在 C++ 的 STL 库，或者 Java, Python 等语言中都提供了对迭代器的支持，OpenCV 也支持使用迭代器的方式进行遍历，代码如上所示。 本文介绍了遍历 Mat 的三种方式: 使用 at() 遍历，使用指针遍历，以及使用迭代器遍历。推荐使用指针的方式遍历图像，其效率最高，在编写图像处理的算法时，效率是我们不可忽视的要素。","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://shuiyujie.com/categories/OpenCV/"}],"tags":[{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"}]},{"title":"C 语言-格式化输出函数 printf()","slug":"C:C++/C-语言-格式化输出函数-printf","date":"2019-05-23T13:39:34.000Z","updated":"2019-05-23T13:48:13.687Z","comments":true,"path":"post/d37b0970.html","link":"","permalink":"http://shuiyujie.com/post/d37b0970.html","excerpt":"","text":"printf “格式字符串” 候选字符 格式字符串 作用 %d 将整数转成十进制 %f 将整数转成浮点数 %u 十进制无符号整数 %o 将整数转成八进制 %c 将整数转成对应的 ASCII 字符 %s 将整数转成字符串 %x 整数转成小写十六进制 %X 整数转成大写十六进制 %p 输出 %% 输出百分比符号，不进行转换 printf 中特殊规定的字符 规定字符 作用 \\n 换行操作 \\f 清屏并换页 \\r 回车 \\t Tab 符 \\xth 用 16 进制表示的 ASCII 码","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C/C++ 中的命令行参数","slug":"C:C++/C-C-中的命令行参数","date":"2019-05-23T13:27:25.000Z","updated":"2019-05-23T13:36:54.022Z","comments":true,"path":"post/b482364a.html","link":"","permalink":"http://shuiyujie.com/post/b482364a.html","excerpt":"C/C++ 中的 main 函数经常带有 argc, argv ,比如 int main(int argc, char** argv) 或者 int main(int argc, char* argv[])，其中 argc 表示我们从命令行键入的参数，argv[] 即为参数列表。 Java 中的 public static void main(String argc[]) 和 Python 中的 sys.argv 中也都带有命令行参数。 通过命令行参数我们可以就能由 main 函数入口传递参数到程序内部。","text":"C/C++ 中的 main 函数经常带有 argc, argv ,比如 int main(int argc, char** argv) 或者 int main(int argc, char* argv[])，其中 argc 表示我们从命令行键入的参数，argv[] 即为参数列表。 Java 中的 public static void main(String argc[]) 和 Python 中的 sys.argv 中也都带有命令行参数。 通过命令行参数我们可以就能由 main 函数入口传递参数到程序内部。 1234567891011#include &lt;stdio.h&gt; int main(int argc, char ** argv) &#123; int i; for (i=0; i &lt; argc; i++) printf(\"Argument %d is %s.\\n\", i, argv[i]); return 0; &#125; 加入我们编译之后运行 ./hello a b c d 将会输出 12345Argument 0 is ./hello.Argument 1 is a.Argument 2 is b.Argument 3 is c.Argument 4 is d. 由此得出两个结论 参数列表中包含 ./hello 参数个数要算上 ./hello","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C++学习笔记-多态","slug":"C:C++/C-学习笔记-多态","date":"2019-05-18T15:03:11.000Z","updated":"2019-05-19T01:59:43.934Z","comments":true,"path":"post/e7609a12.html","link":"","permalink":"http://shuiyujie.com/post/e7609a12.html","excerpt":"多态指不同的对象发送同一个消息，不同对象对应同一消息产生不同行为。多态存在于具有继承关系的类中，当父类函数被申明为虚函数，子类重写了父类虚函数时，就具备了多态发生的条件。","text":"多态指不同的对象发送同一个消息，不同对象对应同一消息产生不同行为。多态存在于具有继承关系的类中，当父类函数被申明为虚函数，子类重写了父类虚函数时，就具备了多态发生的条件。 多态1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* C++学习笔记：面向对象，多态*/#include &lt;iostream&gt;class Animal&#123;public: void bark() &#123; std::cout &lt;&lt; \"动物叫...\" &lt;&lt; std::endl; &#125;&#125;;class Dog: public Animal&#123;public: void bark() &#123; std::cout &lt;&lt; \"wang wang wang ...\" &lt;&lt; std::endl; &#125;&#125;;class Cat: public Animal&#123;public: void bark() &#123; std::cout &lt;&lt; \"miao miao miao ...\" &lt;&lt; std::endl; &#125;&#125;;void animalBark(Animal *animal)&#123; animal-&gt;bark();&#125;int main()&#123; Animal *animal = new Animal(); Dog *dog = new Dog(); Cat *cat = new Cat(); animalBark(animal); animalBark(dog); animalBark(cat); return 0;&#125; 123动物叫...动物叫...动物叫... 父类函数添加 virtual ，将其声明为虚函数。 12345678class Animal&#123;public: virtual void bark() &#123; std::cout &lt;&lt; \"动物叫...\" &lt;&lt; std::endl; &#125;&#125;; 123动物叫...wang wang wang ...miao miao miao ... 基类中的虚函数是一个使用关键字 virtual 声明的函数。派生类中已经对函数进行定义的情况下，定义一个基类的虚函数，就是要告诉编译器我们不想对这个函数进行静态链接。 我们所希望的是根据调用函数的对象的类型对程序中在任何给定指针中被调用的函数的选择。这种操作被称为动态链接，或者后期绑定。 多态的三个条件 类间存在继承 要有虚函数重写 父类指针或引用指向子类对象 重载、重写、重定义重载：同一个类中函数名相同，参数列表相同。以下代码表示 bark() 函数的重载。 12345678910111213141516class Animal&#123;private: std::string name;public: virtual void bark() &#123; std::cout &lt;&lt; \"动物叫...\" &lt;&lt; std::endl; &#125; virtual void bark(std::string name) &#123; std::cout &lt;&lt; name &lt;&lt; \"正在叫...\" &lt;&lt; std::endl; &#125;&#125;; 重写：指子类覆盖父类的虚函数，要求函数名和参数均相同，如下所示： 1234567891011121314151617class Animal&#123;public: virtual void bark() &#123; std::cout &lt;&lt; \"动物叫...\" &lt;&lt; std::endl; &#125;&#125;;class Dog: public Animal&#123;public: void bark() &#123; std::cout &lt;&lt; \"wang wang wang ...\" &lt;&lt; std::endl; &#125;&#125;;","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C++学习笔记-继承","slug":"C:C++/C-学习笔记-继承","date":"2019-05-18T07:11:25.000Z","updated":"2019-05-18T08:27:29.483Z","comments":true,"path":"post/5afabdd4.html","link":"","permalink":"http://shuiyujie.com/post/5afabdd4.html","excerpt":"面向对象有三个特性：封装继承和多态。实现一个类就是实现了一个对象的封装。继承是为了表示对象和对象之间的关系，生物学中有界门纲目科属种的说法，比如说喜鹊、麻雀、燕子它们有各自的特性，可以将它们看成一个个类，它们都属于鸟类，因为它们有鸟类共同的特征，会飞啊、有羽毛的啊。 继承就是抽象出对象的共性成为一个父类，子类用继承的方式获得父类的共性，并且子类有权拥有自己的个性，也能对父类的共性做个性化的修改。 本文并不详细介绍继承的概念，而是记录继承中一些关键点和注意事项。","text":"面向对象有三个特性：封装继承和多态。实现一个类就是实现了一个对象的封装。继承是为了表示对象和对象之间的关系，生物学中有界门纲目科属种的说法，比如说喜鹊、麻雀、燕子它们有各自的特性，可以将它们看成一个个类，它们都属于鸟类，因为它们有鸟类共同的特征，会飞啊、有羽毛的啊。 继承就是抽象出对象的共性成为一个父类，子类用继承的方式获得父类的共性，并且子类有权拥有自己的个性，也能对父类的共性做个性化的修改。 本文并不详细介绍继承的概念，而是记录继承中一些关键点和注意事项。 访问控制 访问控制符 作用 public 公有继承，公开的大家都能访问 protected 保护继承，子类允许父类 private 私有继承，只能通过 get() 这样的函数访问 访问控制的存在能够保证让对象在继承中任然保持封装特性，亲兄弟也得明算账，能让你访问的就让你访问，不允许的我用 private 提前说明，你也别来碰。 代码示例 类的兼容性原则类的兼容性原则意思就是在使用父类的地方可以使用其子类来替代。因为子类通过公有继承已经获得了父类除了构造函数、析构函数之外的属性和方法。 子类对象可以当做父类对象来使用 子类对象可以赋值给父类对象 父类对象可以用子类对象来初始化 父类指针可以指向子类对象 子类创建和销毁的过程子类继承了父类，有了父类的属性和方法，所以再创建和销毁的过程中必不可少地要调用父类的构造方法和析构函数。 子类创建到销毁的过程 调用父类的构造函数 执行子类的构造函数 当父类的构造函数有参数时，需要在子类的初始化列表中显示调用 析构函数调用的先后顺序与构造函数相反 代码示例 多继承和虚继承C++ 允许一个类继承自多个类，同时拥有多个类的特性，这被称为多继承。多继承采用 class C: public Child1, public B 的形式实现，在继承的类之间添加逗号即可。 虚继承指的是类实现了多继承，其中继承的多个类中有两个或者以上都继承自同一个父类。 虚地址会造成一个二义性问题，我们知道类继承自另外一个类就会获得另外一个类的属性，在多继承中被继承的两个类来自于同一父类，就会产生下图所示的情况。 class C 有重复获取了两次 class B 中的变量 b，而 class B 有一个变量 b，class C 就面对使用哪个 b 的二义性问题。 解决方案就是让 class B1 和 class B2 在继承时都使用 vitural 修饰。 virtual 对子类声明为虚继承，使这个子类成为虚基类，使公共父类在子类中只产生一个子对象，","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C++学习笔记-静态方法、静态函数、this指针","slug":"C:C++/C-学习笔记-静态方法、静态函数、this指针","date":"2019-05-18T01:44:28.000Z","updated":"2019-05-18T07:18:02.260Z","comments":true,"path":"post/cd8fc6b5.html","link":"","permalink":"http://shuiyujie.com/post/cd8fc6b5.html","excerpt":"静态变量、静态方法和 this 指针都是为了更好表现类的概念。 类中的静态变量能够实现同类对象中的信息共享，共享状态能够实现许多实用的功能，比如说计数。 静态变量不能用普通函数直接访问，就需要用到静态函数来管理静态变量。 this 指针用 this-&gt;name = name 的形式解决类中函数传入的参数与类中成员变量冲突的问题。 总之，通过代码来说明问题更容易理解。","text":"静态变量、静态方法和 this 指针都是为了更好表现类的概念。 类中的静态变量能够实现同类对象中的信息共享，共享状态能够实现许多实用的功能，比如说计数。 静态变量不能用普通函数直接访问，就需要用到静态函数来管理静态变量。 this 指针用 this-&gt;name = name 的形式解决类中函数传入的参数与类中成员变量冲突的问题。 总之，通过代码来说明问题更容易理解。 静态变量、静态方法和 this 指针使用代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* 面向对象，静态变量、静态方法, this 指针 */#include &lt;iostream&gt;#include &lt;cstring&gt;class Student&#123;private: // 学生编号 int number; // 学生分数 int score;public: // 静态变量，学生总数 static int studentCount; // 静态变量，所有学生总分 static int sumScore;public: Student(int number, int score) &#123; this-&gt;number = number; this-&gt;score = score; studentCount++; sumScore += score; &#125; // 静态方法 static double getAvg() &#123; return sumScore / studentCount; &#125; Student(/* args */);&#125;;// 静态变量赋值，所有对象共享int Student::sumScore = 0;int Student::studentCount = 0;int main()&#123; Student stu1(111, 50); Student stu2(112, 70); Student stu3(114, 90); // 静态方法调用 double avg = Student::getAvg(); std::cout &lt;&lt; \"average score = \" &lt;&lt; avg &lt;&lt; std::endl; return 0; &#125; 静态成员变量： static 成员变量实现了同类对象间信息共享。 static 成员类外存储,求类大小,并不包含在内。 static 成员是命名空间属于类的全局变量,存储在 data 区 static 成员只能类外初始化。 可以通过类名访问(无对象生成时亦可),也可以通过对象访问 静态成员函数： 静态成员函数的意义,不在于信息共享,数据沟通,而在于管理静态数据成员, 完成对静态数据成员的封装。 静态成员函数只能访问静态数据成员。原因:非静态成员函数,在调用时 this 指针被当作参数传进。而静态成员函数属于类,而不属于对象,没有 this 指针 this 指针: 类成员变量的形参和类的属性名字相同，可以使用 this 指针，如 this-&gt;name = name","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C++学习笔记-构造函数、析构函数、拷贝构造函数","slug":"C:C++/C-学习笔记-构造函数、析构函数、拷贝构造函数","date":"2019-05-18T01:10:03.000Z","updated":"2019-05-18T01:54:15.261Z","comments":true,"path":"post/a51fdb12.html","link":"","permalink":"http://shuiyujie.com/post/a51fdb12.html","excerpt":"C++ 引入了面向对象的概念，面向对象的思想能够抽象出现实世界的实体，以此解决更加复杂的问题。 类是面向对象的核心，类即是我们对事物抽象之后的产物。C++ 中的类用 class 关键字修饰，后面紧跟着类名 class ClassName。 现实世界的实体应该怎么抽象？一部分抽象为是事物的基本属性，一部分抽象为事物的行为。以抽象一只狗为例，狗的基本属性比如说有品种、年龄、毛色、重量等，而它的行为比如说有奔跑、行走、吃东西、叫等。 类中就用变量来表示事物的基本属性，行为则用函数来表示，这就是面向对象思想的核心。在此基础上又有继承、多态等概念，也是为了进一步表现对象与对象之间的关系。 本文讲的是 C++ 中类的构造函数、析构函数以及拷贝构造函数。构造函数区别于普通的函数，是专门用来初始化一个对象的；析构函数则是在函数被销毁时自动调用做善后工作的；拷贝构造函数是另一种初始化类的方法，它直接用类来初始化一个新类。","text":"C++ 引入了面向对象的概念，面向对象的思想能够抽象出现实世界的实体，以此解决更加复杂的问题。 类是面向对象的核心，类即是我们对事物抽象之后的产物。C++ 中的类用 class 关键字修饰，后面紧跟着类名 class ClassName。 现实世界的实体应该怎么抽象？一部分抽象为是事物的基本属性，一部分抽象为事物的行为。以抽象一只狗为例，狗的基本属性比如说有品种、年龄、毛色、重量等，而它的行为比如说有奔跑、行走、吃东西、叫等。 类中就用变量来表示事物的基本属性，行为则用函数来表示，这就是面向对象思想的核心。在此基础上又有继承、多态等概念，也是为了进一步表现对象与对象之间的关系。 本文讲的是 C++ 中类的构造函数、析构函数以及拷贝构造函数。构造函数区别于普通的函数，是专门用来初始化一个对象的；析构函数则是在函数被销毁时自动调用做善后工作的；拷贝构造函数是另一种初始化类的方法，它直接用类来初始化一个新类。 构造函数12345678910111213141516171819202122232425class Dog&#123;private: char name[50]; int age;public: Dog(char *dogName, int dogAge) &#123; strcpy(name, dogName); age = dogAge; &#125; void initDog(char *dogName, int dogAge) &#123; strcpy(name, dogName); age = dogAge; &#125; void bark() &#123; std::cout &lt;&lt; name &lt;&lt; \": wang wang~~\" &lt;&lt; std::endl; &#125;&#125;; 初始化一个类，需要把参数传给一个类，比如说用 void initDog(char *dogName, int dogAge) 这个方法。C++ 给我提供了一种特殊的函数叫做构造函数就像 Dog(char *dogName, int dogAge) 这样。我们可以用 Dog dog2(&quot;bbb&quot;, 5); 的方式来调用构造函数并初始化类。 1234567class 类名&#123; 类名(参数列表) &#123; 构造体 &#125;&#125;; 构造函数的格式，及注意事项： 没有返回值 方法名和类名一致 构造函数可以重载，让参数列表不同 默认存在无参构造函数，当自定义构造函数之后无参构造函数失效，此时需要手动定义 构造函数代码示例 析构函数C++中的类可以定义一个特殊的成员函数清理对象,这个特殊的成员函数叫做析构函数。 1234567class 类名&#123; ~类名(参数列表) &#123; 析构体 &#125;&#125;; 析构函数格式，及注意事项: 无返回值 方法名和类名一致，前面有个 ~ 符号 默认存在一个析构函数 析构函数不可重载 123456789101112131415161718192021222324252627282930313233class Dog&#123;private: char *name; int age;public: Dog(char *dogName, int dogAge) &#123; std::cout &lt;&lt; \"调用了构造函数.\" &lt;&lt; std::endl; name = new char[50]; strcpy(name, dogName); age = dogAge; &#125; void bark() &#123; std::cout &lt;&lt; name &lt;&lt; \": wang wang~~\" &lt;&lt; std::endl; &#125; ~Dog() &#123; std::cout &lt;&lt; \"调用了析构函数.\" &lt;&lt; std::endl; if(name != NULL) &#123; delete name; name = NULL; &#125; &#125;&#125;; 同样是 Dog 类，和文章开头的 Dog 类别比较有一点小的改动。private 类型变量 name，我将其由字符数组类型更改为了字符指针类型。新的 Dog 类就需要我们在使用的时候为 name 申明一块内存空间，就想我在其构造函数中所做的一样。 与之相对的就是在 Dog 类使用完成之后 free() 之前申明的内存空间。 析构函数代码示例 拷贝构造函数在第一小节我们看到了普通的构造函数是怎么使用的，之前还提到了构造函数有无参构造函数和有参数构造函数。在没有显示申明构造函数的时候，类自带一个无参构造函数，而当我们显示申明构造函数之后，默认的无参构造函数就失效了，此时如果任然需要无参构造函数，那就需要我们自己显示申明。 接下来再介绍一种构造函数叫做拷贝构造函数，由一个以存在的类来初始化一个新的类就可以使用拷贝构造函数，此时将不由构造器来初始化，而是由拷贝构造器来初始化。 拷贝构造函数与构造函数、析构函数一样，也存在默认的拷贝构造函数，我们可以直接使用如下 12345Dog dog(\"bbb\", 5);dog.bark();Dog dog2(dog);dog2.bark(); 如果想要自定一个拷贝构造函数格式如下 1234567class 类名&#123; 类名(const 类名 &amp;dog) &#123; 拷贝构造体 &#125;&#125;; 123456789101112131415161718192021222324252627282930class Dog&#123;private: char name[50]; int age;public: Dog(char *dogName, int dogAge) &#123; std::cout &lt;&lt; \"调用了构造函数.\" &lt;&lt; std::endl; strcpy(name, dogName); age = dogAge; &#125; Dog(const Dog &amp;dog) &#123; std::cout &lt;&lt; \"调用了拷贝构造函数.\" &lt;&lt; std::endl; strcpy(name, dog.name); age = dog.age; &#125; void bark() &#123; std::cout &lt;&lt; name &lt;&lt; \": wang wang~~\" &lt;&lt; std::endl; &#125;&#125;; 深拷贝和浅拷贝 与结构体一样，类也存在深拷贝和浅拷贝的问题。浅拷贝即拷贝之后两个变量共用一块内存空间，深拷贝即新建一块内存来保存被拷贝的信息。 浅拷贝不是真正意义上的拷贝。在结构体发生浅拷贝时，如果我们释放该结构体，将会影响其他引用该结构体的其他变量。在类中的浅拷贝，如果类中包含的数据元素全部在栈上,浅拷贝也可以满足需求的。但如果堆上的数据,则会发生多次析构行为。 注：关于结构体的深拷贝和浅拷贝可以看我这篇文章 C语言学习笔记-结构体。 系统提供的默认拷贝构造器执行的就是浅拷贝操作，如果我们想要将其修改为深拷贝就需要自己手动来实现拷贝构造函数。 12345678Dog(const Dog &amp;dog)&#123; std::cout &lt;&lt; \"调用了拷贝构造函数.\" &lt;&lt; std::endl; name = new char[50]; strcpy(name, dog.name); age = dog.age;&#125;","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C++学习笔记-内联函数、默认参数和函数重载","slug":"C:C++/C-学习笔记-内联函数、默认参数和函数重载","date":"2019-05-18T01:01:43.000Z","updated":"2019-05-18T01:54:15.259Z","comments":true,"path":"post/e95932f5.html","link":"","permalink":"http://shuiyujie.com/post/e95932f5.html","excerpt":"内联函数能够将函数体直接插入在函数调用的位置，减少了函数调用时出栈、入栈的过程，提高程序性能。 默认参数与占位符让我们在定义函数的时候能够设置默认值，提高了函数定义的灵活性。 函数的重载则允许我们在参数列表不同的情况下使用同名函数，解除了 C 语言中函数名必须不同的限制。 以上都是 C++ 在 C 语言基础上对函数定义的扩展，可以放在一起进行学习。","text":"内联函数能够将函数体直接插入在函数调用的位置，减少了函数调用时出栈、入栈的过程，提高程序性能。 默认参数与占位符让我们在定义函数的时候能够设置默认值，提高了函数定义的灵活性。 函数的重载则允许我们在参数列表不同的情况下使用同名函数，解除了 C 语言中函数名必须不同的限制。 以上都是 C++ 在 C 语言基础上对函数定义的扩展，可以放在一起进行学习。 inline 内联函数123456789101112// 主函数循环调用 my_min() 函数for(int i = 0; i &lt; 1000; i++)&#123; min = my_min(a, b); cout &lt;&lt; \"min = \" &lt;&lt; min &lt;&lt; endl;&#125;// 比较两个值的大小int my_min(int a, int b)&#123; return (a &lt; b ? a : b);&#125; 当主函数调用函数的时候会有频繁的入栈、出栈操作。如果想减少这部分时间开销，在 C 语言中我们使用宏定义，在 C++ 中使用内联函数。 1234567#define MY_MAX(a, b) ((a) &lt; (b) ? (a) : (b))for(int i = 0; i &lt; 1000; i++)&#123; min = MY_MAX(a, b); cout &lt;&lt; \"min = \" &lt;&lt; min &lt;&lt; endl;&#125; 使用 C 语言中的宏定义我们可以让被调用的函数由预编译器展开，不会再有出栈入栈的过程，然而这样做存在一个问题。由于宏定义在编译的时候就展开，如果使用 MY_MAX(a++, b++) 的方式调用就会被展开成 MY_MAX(a++, b++) ((a++) &lt; (b++) ? (a++) : (b++))。 C++ 采用 inline 关键字来申明一个内联函数。内联函数是一种特殊的函数,具有普通函数的特征，但是C++编译器直接将函数体插入在函数调用的地方。 12345// inline 关键字修饰的内联函数inline int my_max(int a, int b)&#123; return (a &gt; b ? a : b);&#125; 最后是内联函数使用的注意事项: 不能存在任何形式的循环语句 不能存在过多的条件判断语句 不能对函数进行取址操作 函数内联声明必须在调用语句之前 默认参数与占位符默认参数与占位符用于 C++ 定义函数的时候，相对于 C 语言有有更多的特性。 1234567891011121314151617#include &lt;iostream&gt;int function_name(int a, int b = 10)&#123; std::cout &lt;&lt; \"a = \" &lt;&lt; a &lt;&lt; \" b = \" &lt;&lt; b &lt;&lt; std::endl;&#125;int main()&#123; int a = 20; int b = 30; function_name(a, b); function_name(a); return 0;&#125; 在函数 int function_name(int a, int b = 10) 中，我们在函数定义的过程中给 b 一个默认值，当函数调用时如果未传入 b 就会使用默认的 b 的值。 关于默认参数有这样几条规则 可以有多个默认值 默认值必须从后面写起 typename function_name(int a, int b, ... int x =10, int y =20) 一旦在一个函数调用中开始使用默认参数值,那么这个参数后的所有参数都必须使用默认参数值 占位符就感觉比较鸡肋了,我们使用一个没有名字的只有类型的形参作为占位符，但因为它没有名字，在函数中也没有办法使用。 1234567891011// 占位符int func(int a, int b, int) &#123; return (a + b);&#125;// 占位符并赋予默认值int func(int a, int b, int = 10) &#123; return (a + b);&#125; 函数重载12int funtion_name(int a, int b);int funtion_name(int a); 以上这段代码在 C 语言中是会报错的，因为 C 语言禁止函数同名，而在 C++ 中这样是允许的，被称为函数重载。 一个函数包含这样三个部分 返回值类型 方法名名称 参数列表（参数类型，参数个数） 1234void func(int a);//okvoid func(char a);//ok, 参数个数相同，参数类型不同 void func(int a, int b);//ok 参数个数不同char func(int a);//error,与第一一个函数有冲突","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C++学习笔记-引用","slug":"C:C++/C-学习笔记-引用","date":"2019-05-18T00:49:53.000Z","updated":"2019-05-18T01:54:15.260Z","comments":true,"path":"post/824d7309.html","link":"","permalink":"http://shuiyujie.com/post/824d7309.html","excerpt":"引用格式一个已存在变量的别名，通过这个别名就可以用来修改指向的变量的值。引用和指针很像，在很多情况下就是简化版的指针。引用比指针弱的地方在于引用必须在创建时初始化，并且不能再改变指向的对象，而指针可以为空，也可以改变指向的内存地址。","text":"引用格式一个已存在变量的别名，通过这个别名就可以用来修改指向的变量的值。引用和指针很像，在很多情况下就是简化版的指针。引用比指针弱的地方在于引用必须在创建时初始化，并且不能再改变指向的对象，而指针可以为空，也可以改变指向的内存地址。 引用的基本规则12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;int main()&#123; int a = 10; // 定义一个 a int &amp;b = a; // 定义一个 a 的引用 int *p = &amp;a; // 定义一个指向 a 的指针 *p = 20; std::cout &lt;&lt; \"a = \" &lt;&lt; a &lt;&lt; std::endl; b = 30; std::cout &lt;&lt; \"a = \" &lt;&lt; a &lt;&lt; \" , b = \" &lt;&lt; b &lt;&lt; std::endl; ///////////////////////////////////////////////////// int c; // &amp;b = c; 错误，b 已经是 a 的引用，不可修改引用关系 // float &amp;f = c; 错误，引用类型不匹配 int &amp;bb = b; // 可以引用一个引用 bb = 40; std::cout &lt;&lt; \"a = \" &lt;&lt; a &lt;&lt; \" , b = \" &lt;&lt; b &lt;&lt; \" , bb = \" &lt;&lt; bb &lt;&lt; std::endl; return 0;&#125; 引用的使用方式 typename &amp;b = a, &amp; 符号前有符号表示引用，否则表示取地址 引用是一种关系申明，表示它和某个变量的关系，它们类型相同，并共享一片内存 引用一经申明就不可以修改 可以引用一个引用 引用作为函数参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;iostream&gt;struct Person&#123; char name[50]; int age;&#125;;void printfA(Person *p);void printfB(Person &amp;p);void printfC(Person p);int main()&#123; Person person = &#123;\"aaaa\", 18&#125;; std::cout &lt;&lt; \"==========参数为指针类型==========\" &lt;&lt; std::endl; printfA(&amp;person); std::cout &lt;&lt; \"third time = \" &lt;&lt; person.age &lt;&lt; std::endl; std::cout &lt;&lt; \"==========参数为引用类型==========\" &lt;&lt; std::endl; printfB(person); std::cout &lt;&lt; \"third time = \" &lt;&lt; person.age &lt;&lt; std::endl; std::cout &lt;&lt; \"==========参数为结构体类型==========\" &lt;&lt; std::endl; printfC(person); std::cout &lt;&lt; \"third time = \" &lt;&lt; person.age &lt;&lt; std::endl; return 0;&#125;// 指针类型，同一块内存上操作void printfA(Person *p)&#123; std::cout &lt;&lt; \"first time = \" &lt;&lt; p-&gt;age &lt;&lt; std::endl; p-&gt;age = 20; std::cout &lt;&lt; \"second time = \" &lt;&lt; p-&gt;age &lt;&lt; std::endl;&#125;// 引用类型，同一块内存上操作void printfB(Person &amp;p)&#123; std::cout &lt;&lt; \"first time = \" &lt;&lt; p.age &lt;&lt; std::endl; p.age = 30; std::cout &lt;&lt; \"second time = \" &lt;&lt; p.age &lt;&lt; std::endl;&#125;// 结构体类型，形参的修改在栈区上void printfC(Person p)&#123; std::cout &lt;&lt; \"first time = \" &lt;&lt; p.age &lt;&lt; std::endl; p.age = 40; std::cout &lt;&lt; \"second time = \" &lt;&lt; p.age &lt;&lt; std::endl;&#125; 123456789101112==========参数为指针类型==========first time = 18second time = 20third time = 20==========参数为引用类型==========first time = 20second time = 30third time = 30==========参数为结构体类型==========first time = 30second time = 40third time = 30 以上这段代码分别将一个结构体用指针形式、引用形式和结构体变量形式作为形参传入到函数中，并对其值进行修改，最后打印结果。 我们可以看到指针类型和引用类型对变量进行修改，因为它们都指向同一块内存。结构体类型，形参的修改在栈区上，并不会对堆区中的结构体变量本身产生影响。 注：关于堆区、栈区的内存四区概念有问题可以看 C语言学习笔记-指针01。 引用是一个变量的别名，有时候起到和指针一样的效果 引用比指针具有更强的可读性 引用作为函数返回值 若返回栈变量: 不能成为其它引用的初始值(不能作为左值使用) 若返回静态变量或全局变量: 可以成为其他引用的初始值(可作为右值使用,也可作为左值使用) 引用作为函数返回-值返回栈变量代码示例 引用作为函数返回-值返回静态变量代码示例 指针引用都在代码里了，关键代码如下，点击查看完整示例 12345678910111213141516171819202122232425262728293031int getTeacher(Teacher **teacher)&#123; Teacher *t = NULL; t = (Teacher*)malloc(sizeof(Teacher)); if(t == NULL) &#123; return -1; &#125; strcpy(t-&gt;name, \"Mary\"); t-&gt;age = 30; *teacher = t; return 0;&#125;int getTeacher02(Teacher *&amp;teacher)&#123; teacher = (Teacher *)malloc(sizeof(Teacher)); if(teacher == NULL) &#123; return -1; &#125; strcpy(teacher-&gt;name, \"Lucy\"); teacher-&gt;age = 20; return 0;&#125; const 引用const 最常见的作用就是在变量作为函数形参的时候加上 const 修饰，起到对形参的保护作用，加了 const 修饰的参数就不会在函数中被修改。 const 如果和引用放在一起会擦出什么火花？ 12345678const int a = 10;// int &amp;b = a; errconst int &amp;b = a;int c = 20;const int &amp;d = c;// d = 30; errc = 30; 第一组代码想说明的是 const 修饰的常量，如果我想用一直普通的引用指向它是不可以的。假设我们可以用一个普通的 b 做为 a 的引用，那就可以使用 b 去修改 a 所指代的内存，而 a 指向的内存是被 const 保护的，是不允许修改的。不过我们可以使用 const 修饰的引用去指向 a，此时他们的安全级别一样，都不会导致内存被修改的安全问题。 第二组代码想说明的 const 修饰的引用安全级别更高，可以用安全级别高的去引用安全级别低的。这时候造成的现象是 d 被 const 修饰不能修改，而 c 任然可以修改，虽然是同一片内存但是 d 也无法阻止 c 进行修改。 1234void printA(const Teacher &amp;teacher) &#123; //teacher.age = 33; printf(\"teacher.age:%d \\n\", teacher.age);&#125; 上面这段代码则表示 const 修饰的形参是只读的，不允许在函数中进行修改，起到了保护形参的作用。 总结一下： const 引用可以指向安全级别比其低的或者同级的 const 引用常在形参传递过程中起保护形参的作用","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-指针01","slug":"C:C++/C语言学习笔记-指针01","date":"2019-05-11T07:45:09.000Z","updated":"2019-05-14T13:47:23.915Z","comments":true,"path":"post/2f02c8ae.html","link":"","permalink":"http://shuiyujie.com/post/2f02c8ae.html","excerpt":"指针是 C 语言的灵魂，现在对于指针的掌握肯定是不透彻的，然学习是一个迭代的过程，姑且写出目前自己的理解。","text":"指针是 C 语言的灵魂，现在对于指针的掌握肯定是不透彻的，然学习是一个迭代的过程，姑且写出目前自己的理解。 指针的概念123456789101112131415161718192021#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;int main()&#123; int a = 10; int *p = NULL; p = (int *)malloc(sizeof(int)); p = &amp;a; printf(\"%d\\n\", p); printf(\"%d\\n\", *p); if(p != NULL) &#123; free(p); p = NULL; &#125;&#125; 先来看这样一段代码 p 是一个指针变量，*p 是指针指向的内存空间的保存的值。 &amp;a 表示取变量 a 所在内存的地址，p = &amp;a 表示将内存 a 所在的地址值赋值给 p，相当于指针 p 指向 a 所在的内存。 接下来分别打印 p 和 *p 来验证上面的说法 最后我们释放内存空间 明确几个概念： 指针是一个变量 注意区分指针变量和它指向的内存块 改变指针变量的值不会影响指针指向的内存区域 改变指针指向的内存区域也不会改变指针变量的值 内存四区先来看这样一段代码 123456789101112131415161718192021#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;char *get_str()&#123; char str[] = \"abcdefg\"; return str;&#125;int main()&#123; char buf[128] = &#123;0&#125;; strcpy(buf, get_str()); printf(\"buf = %s\\n\", buf); // char *p = NULL; // p = get_str(); // printf(\"p = %s\\n\", p); return 0;&#125; char *p 是一个 char 类型的指针变量，看代码的意思是希望通过 get_str() 方法给指针 p 赋值。如果你运行一下这段代码会发现它是会报错的，至于为什么会报错就要分析一下内存四区了。 区域 作用 栈区(stack) 由编译器自动分配释放，存放函数的参数值，局部变量值等。 堆区（heap） 一般由程序员分配释放（动态内存申请与释放），若程序员不释放，程序结束时可能由操作系统回收。 全局区（静态区） 全局变量和静态变量的存储是放在一起的，初始化的全局变量和静态变量在一块区域，未被初始化的全局变量和未初始化的静态变量在相邻的另一块区域，该区域在程序结束后由操作系统释放。 程序代码区 存放函数体的二进制代码。 现在通过内存四区的理论来分析上面这一段代码。 声明了一个名为 buf 的变量 调用 get_str() 声明一个数组 str ，它是一个局部变量所以再栈区 字符串 “abcdef…” 是字符串常量，所以再全局区（静态区） str = “abcdef” 会将全局区的字符串常量拷贝到 str 数组内存中 紧接着 get_str() 调用结束，与之相关的内存就会被销毁 strcpy() 希望将 get_str() 返回的字符串拷贝给 buf，但是因为这一块内存已经被销毁了，就会报错 注：有时候以上的操作可能会成功，这可能是因为没有及时销毁，可以用我上面注释掉的指针操作的代码做练习，用内存四区的方式分析，赋值是一定不会成功的，原理相同。 正确的赋值方式我们可以使用 malloc() 将变量保存在堆区，堆区的内存在函数执行完之后不会立即销毁，而是由我们自己销毁，或者再程序运行完之后由操作系统来销毁。 123456789101112131415161718192021222324252627282930#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;char *get_str2()&#123; char *tmp = (char *)malloc(100); if(tmp == NULL) &#123; return NULL; &#125; strcpy(tmp, \"adsaffa\"); return tmp;&#125;int main()&#123; char buf[128] = &#123;0&#125;; // strcpy(buf, get_str()); // printf(\"buf = %s\\n\", buf); char *p = NULL; p = get_str2(); if(p != NULL) &#123; printf(\"p = %s\\n\", p); free(p); p = NULL; &#125; return 0;&#125; 使用指针的几个注意事项这一小节属于使用指针过程中遇到的一些问题，以及如何避免，有个故事说：人如果知道自己会死在哪里，就死也不要去那里就行了。使用指针也一样，指针使用会遇到很多的坑，知道哪里有坑就要尽可能避免。 修改内存时要保证内存可写12345678910111213#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;int main()&#123; char *p = \"abcdef\"; char q[] = \"fdasfafdfas\"; p[2] = \"1”; // 会报错 err q[2] = \"1\"; printf(\"p: %s\\n\", p); printf(\"q: %s\\n\", q); return 0;&#125; 修改 p[2] 会报错，因为 abcdef 是字符串常量，保存在全局区（静态区），所以它是不能被修改的。使用指针的第一个事项就是修改内存的时候保证内存可写。 指针的步长指针的步长由指针指向的内存类型决定，比如说 int p 步长就是 4，chat p 步长就是 1。步长的意思就是每次 p++ 操作之后，地址移动的距离。可以分别打印 p 和 p++ 的值看一看就知道了。 由此还想讲一个问题就是如何计算一个动态数组的大小。比如 int a[] = {1, 2, 3, 4}，可以使用 sizeof(a)/sizeof(a[0])的方式计算数组大小。 不允许 NULL 或者未知非法地址拷贝内存12char *p =NULL;strcpy(p, “1232143”); // 报错 strcpy() 执行的操作时将字符串拷贝给指针 p 所指向的内存,此时指针 p 指向的内存是 NULL，就是说没有执行任何内存，字符串也就没地方拷贝，自然会发生错误。 如果我们让char* p = 0x11; 任意赋一个值，也会报错，向未知非法地址拷贝也是不被允许的。 正确的做法就是将 p 指向一个已经分配好内存的指针，比如 char *q = “fdsafas” 。 所以当我们给一个使用 malloc() 分配内存之后应该习惯性地加一个判断，比如int *p = (int *)malloc(sizeof(int));之后应该进行判断if(p == NULL)如果为空则说明内存分配失败，就应该对异常进行处理。一直对应的，释放内存是有个也应该讲指针赋值为空free(p); p = NULL;。 通过指针间接赋值我们定义一个函数会有返回值，返回值可能是一个结果、可能表示一种状态。比如说int findMax(int *a, int n);在这个函数中我们出入一个数组，然后返回这个数组中最大的值。 那现在如果我想返回这个数组的最大值、最小值该怎么办？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;int fun(int *a, int n, int *max, int *min)&#123; if(a == NULL || n &lt;= 0) &#123; return -1; &#125; int tmp_max, tmp_min, sum = *a; for(int i = 1; i &lt; n; i++) &#123; if(a[i] &lt; tmp_min) &#123; tmp_min = a[i]; &#125; if(a[i] &gt; tmp_max) &#123; tmp_max = a[i]; &#125; &#125; max = &amp;tmp_max; min = &amp;tmp_min; return 0;&#125;int main()&#123; int a[5] = &#123;2, 6, 9, 10, 1&#125;; int max = 0; int min = 0; int res = fun(a, 5, &amp;max, &amp;min); if(res == -1) &#123; perror(\"fun() err.\"); return -1; &#125; printf(\"max = %d, min = %d\\n\", max, min); return 0;&#125; 定义一个函数int fun(int *a, int n, int *max, int *min); 返回值表示成功或者失败 a 是数组指针 n 是数组大小 max 和 int 是 int 类型的指针 关键就在于 max 和 int 这两个指针，它们指向的两个内存分别保存数组的最大和最小值，而这两个指针是由主函数传入的，此时就完成你了最大值和最小值传递。 通过指针间接赋值是指针很强大的一个功能，我们可以看到很多函数都是这样定义的。","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-字符串","slug":"C:C++/C语言学习笔记-字符串","date":"2019-05-11T07:40:14.000Z","updated":"2019-05-18T00:51:26.182Z","comments":true,"path":"post/a99472bd.html","link":"","permalink":"http://shuiyujie.com/post/a99472bd.html","excerpt":"字符串初始化 访问字符串 字符串的拷贝","text":"字符串初始化 访问字符串 字符串的拷贝 字符串初始化C 语言没有字符串类型，利用字符类型来模拟，以 \\0 结尾。 12345678910111213141516171819202122232425262728#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;int main()&#123; // 如果用字符数组来初始化字符串，末尾不是\\0结尾后面的会乱码 char buf[] = &#123;'a', 'b', 'c'&#125;; printf(\"buf = %s\\n\", buf); // 指定数组长度未初始化的地方会补零，读到0就作为字符串结束标记，则不会乱码 char buf1[100] = &#123;'a', 'b', 'c'&#125;; printf(\"buf1 = %s\\n\", buf1); // '字符 0' ≠ 0 = '\\0' char buf2[100] = &#123;'a', 'b', 'c', '0', '1', '2'&#125;; printf(\"buf2 = %s\\n\", buf2); char buf3[100] = &#123;'a', 'b', 'c', 0, '1', '2'&#125;; printf(\"buf3 = %s\\n\", buf3); char buf4[100] = &#123;'a', 'b', 'c', '\\0', '1', '2'&#125;; printf(\"buf4 = %s\\n\", buf4); // 常用初始化字符串的方法 char str[] = \"safafasga\"; // strlen() 和 sizeof() 的区别: // 字符串末尾以\\0结尾，strlen() 计算字符串长短; sizeof()计算大小还会包含\\0 printf(\"strlen() = %d, sizeof() = %d\\n\", strlen(str), sizeof(str)); char str2[100] = \"safafasga\"; // 制定数组大小的情况下，未制定的位置将会自动补零 printf(\"strlen() = %d, sizeof() = %d\\n\", strlen(str2), sizeof(str2)); return 0;&#125; 访问字符串123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;string.h&gt;/*** 遍历字符串的方法**/int main()&#123; char *buf = \"fdsafasdgag\"; for(int i = 0; i &lt; strlen(buf); ++i) &#123; printf(\"%c\", buf[i]); &#125; printf(\"\\n\"); char *p = buf; for(int i = 0; i &lt; strlen(buf); i++) &#123; printf(\"%c\", p[i]); &#125; printf(\"\\n\"); for(int i = 0; i &lt; strlen(buf); i++) &#123; printf(\"%c\", *(p + i)); &#125; printf(\"\\n\"); for(int i = 0; i &lt; strlen(buf); i++) &#123; printf(\"%c\", *(buf + i)); &#125; printf(\"\\n\"); // p 和 buf 等价么 // buf 是常量，常量是不可变的 // p 是变量，可变 // p++; // buf++; // err return 0;&#125; 字符串的拷贝1234567891011121314151617181920212223242526272829303132333435363738#include &lt;stdio.h&gt;#include &lt;string.h&gt;/*** 1. 进行非空判断，避免异常* 2. 不要直接使用形参**/int my_strcpy(char *dst, char *src)&#123; // 非空判断 if(dst == NULL || src == NULL) return -1; // 不要直接使用形参 // 会改变数组的首地址位置 char *to = dst; char *from = src; while(*to++ == *from++); // 上面把形参结果来不起作用，只有这样才行，之后再看一下 // while(*dst++ == *src++); return 0;&#125;/*** 1. 实现字符串拷贝函数* 2. 写出健壮的代码的注意事项**/int main()&#123; char *src = \"dfagfadga\"; char dst[100] = &#123;0&#125;; int res = my_strcpy(dst, src); if(res != 0) &#123; fprintf(stderr,\"copy str error.\"); return -1; &#125; printf(\"dst: %s\\n\", dst); return 0;&#125;","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-const关键字","slug":"C:C++/C语言学习笔记-const关键字","date":"2019-05-11T07:32:21.000Z","updated":"2019-05-18T07:18:24.256Z","comments":true,"path":"post/5c7372a9.html","link":"","permalink":"http://shuiyujie.com/post/5c7372a9.html","excerpt":"c语言中 const 关键字使用示例 const 修饰的变量定义时要初始化，不初始化后面就没有办法赋值了 const 运用在指针上 c 语言中的 const 是个冒牌货，使用指针任然能够修改","text":"c语言中 const 关键字使用示例 const 修饰的变量定义时要初始化，不初始化后面就没有办法赋值了 const 运用在指针上 c 语言中的 const 是个冒牌货，使用指针任然能够修改 12345678910111213141516171819202122232425262728293031323334#include &lt;stdio.h&gt; int main()&#123; char buf[] = \"dfasfadsgafdg\"; const char *p = buf; // p[1] = \"2\"; err p = \"0x11\"; // const 此时修饰的是 *p，表示 p 指向的那块内存的内容 *p 是不可变的 // char const *p1 = buf; // p1[1] = '2; // 交换顺序这里 const char 与 char const 相同 char * const p1 = buf; p1[1] = '2'; // p1 = \"0x11\"; err // const 此时修饰的是 p，表示指针变量 p 本身是不可修改的，而指针变量指向的内容 *p 是可以修改的 const char * const p3 = buf; // p3[1] = '2; err // p3 = \"abc\"; err // 两个 const 分别修饰了 p 和 *p，此时无论是指针还是指针指向的内存都不可变，完成处于只读状态 const int a = 123; // a = 321; err int *q = NULL; q = &amp;a; *q = 321; printf(\"a: %d\\n\", a); // c 语言中的 const 是个冒牌货，使用指针任然能够修改 return 0;&#125;","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-结构体","slug":"C:C++/C语言学习笔记-结构体","date":"2019-05-11T01:40:29.000Z","updated":"2019-05-14T13:47:33.572Z","comments":true,"path":"post/e261fd69.html","link":"","permalink":"http://shuiyujie.com/post/e261fd69.html","excerpt":"结构体的类型和定义 结构体的赋值 结构体的数组 结构体嵌套一级指针 结构体嵌套二级指针 结构体作为函数参数 结构体深拷贝、浅拷贝","text":"结构体的类型和定义 结构体的赋值 结构体的数组 结构体嵌套一级指针 结构体嵌套二级指针 结构体作为函数参数 结构体深拷贝、浅拷贝 结构体的类型和定义结构体是一种数据类型，用 struct 关键字来修饰，定义一个结构体可以这样： 12345struct Teacher&#123; char name[50]; int age;&#125; 如果用 typedef 修饰，就可以直接使用 Teacher 1234567typedef struct Teacher&#123; char name[50]; int age;&#125;Teacher teacher = NULL; 为结构体申明变量有多种方式： 12345678910111213141516171819202122232425262728293031// 初始化结构体变量1: 定义类型的同时定义变量struct Teacher&#123; char name[50]; int age;&#125;t1, t2;// 初始化结构体变量2struct Student&#123; char name[50]; int age;&#125;s1 = &#123;\"Mike\", 15&#125;; // 初始化结构体变量3struct&#123; char name[50]; int age;&#125;dog = &#123;\"Luck\", 3&#125;; // 初始化结构体变量4struct Teacher t3 = &#123;\"Mary\", 21&#125;;// 初始化结构体变量5struct Teacher t4;struct Teacher *pTeacher = NULL;pTeacher = &amp;t4;strcpy(pTeacher-&gt;name, \"John\");pTeacher-&gt;age = 30; 结构体的赋值结构体赋值之前要先给结构体申请内存。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 结构体赋值 */#include &lt;stdio.h&gt;#include &lt;string.h&gt;struct Teacher&#123; char name[50]; int age;&#125;;typedef struct Teacher teacher_t;void show_teacher(teacher_t t);void copyTeacher(teacher_t *to, teacher_t *from);int main()&#123; teacher_t t1, t2, t3; // 结构体是一种数据类型，分配空间之后才能赋值 memset(&amp;t1, 0, sizeof(t1)); strcpy(t1.name, \"teacher\"); t1.age = 30; show_teacher(t1); // 直接赋值 t2 = t1; show_teacher(t2); // 作为参数传递到用于拷贝的函数 copyTeacher(&amp;t3, &amp;t1); show_teacher(t3); return 0;&#125;void copyTeacher(teacher_t *to, teacher_t *from)&#123; *to = *from;&#125;void show_teacher(teacher_t t)&#123; printf(\"teacher name = %s\\n\", t.name); printf(\"teacher age = %d\\n\", t.age);&#125; 结构体的数组指针数组可以分成静态结构体数组和动态结构体数组两种。 123456789101112131415161718// 静态结构体数组Teacher t1[3] = &#123;\"a\", 18, \"a\", 28, \"a\", 38&#125;;for(int i = 0; i &lt; 3; i++)&#123; printf(\"%s, %d\\n\", t1[i].name, t1[i].age);&#125;// 动态结构体数组Teacher *p = NULL;p = (Teacher *)malloc(3 * sizeof(Teacher));char buf[50];for(int i = 0; i &lt; 3; i++)&#123; sprintf(buf, \"name%d\", i); strcpy(p[i].name, buf); p[i].age = 20 + i;&#125; 结构体数组 结构体嵌套指针我们在定义结构体的时候，其中有一个属性使用了一级指针： 12345struct Teacher&#123; char *name; int age;&#125;; 如上所示char *name是一个指针类型的变量，指针类型的变量当我们为其赋值时要分配内存空间，释放结构体的时候也要先释放其中指针内存的变量。 示例代码，结构体嵌套一级指针 示例代码，结构体嵌套二级指针 结构体深拷贝、浅拷贝当结构体中嵌套了指针，采用浅拷贝的方式如下 12345678Student stu;stu.name = (char *)malloc(30);strcpy(stu.name, \"aaaaaa\");// 浅拷贝，stu 和 sut2 的name 指针指向同一块内存Student stu2;stu2 = stu;printf(\"name = %s\\n\", stu2.name); 此时 stu.name 和 stu2.name 指向同一块内存空间，当释放其中一个指针的时候，另一个 name 的值也就消失了。 深拷贝的方式指的就是 stu2.name 指向一块新的内存空间，这块内存空间拷贝 stu.name 指向的空间的内容，就像下面这样。 12345// 深拷贝，stu3 重新申请一块内存Student stu3;stu3.name = (char *)malloc(30);strcpy(stu3.name, stu.name);printf(\"name = %s\\n\", stu3.name); 示例代码，结构体的深拷贝和浅拷贝","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-文件操作","slug":"C:C++/C语言学习笔记-文件操作","date":"2019-05-11T01:11:29.000Z","updated":"2019-05-18T00:51:31.382Z","comments":true,"path":"post/fe52dfa7.html","link":"","permalink":"http://shuiyujie.com/post/fe52dfa7.html","excerpt":"C语言中我们使用一个指针变量指向一个文件，这个文件就是文件指针。这个文件指针就是 FILE 结构体，它被包含在头文件 “stdio.h” 中。拿到文件指针再结合文件操作的 API，我们就可以对文件进行读写操作。","text":"C语言中我们使用一个指针变量指向一个文件，这个文件就是文件指针。这个文件指针就是 FILE 结构体，它被包含在头文件 “stdio.h” 中。拿到文件指针再结合文件操作的 API，我们就可以对文件进行读写操作。 文件操作打开文件 fopen()12#include &lt;stdio.h&gt;FILE *fopen(const char *path, const char *mode); 文件的打开操作表示返回一个指向制定文件的 FILE 结构体。我们需要指定文件位置和操作方式，特别需要注意的是区分不同的文件操作方式将会产生的结果。 1234567891011r Open text file for reading. The stream is positioned at the beginning of the file.r+ Open for reading and writing. The stream is positioned at the beginning of the file.w Truncate file to zero length or create text file for writing. The stream is positioned at the beginning of the file.w+ Open for reading and writing. The file is created if it does not exist, otherwise it is truncated. The stream is positioned at the beginning of the file.a Open for appending (writing at end of file). The file is created if it does not exist. The stream is positioned at the end of the file.a+ Open for reading and appending (writing at end of file). The file is created if it does not exist. The initial file position for reading is at the beginning of the file, but output is always appended to the end of the file. 关闭文件 fclose()12#include &lt;stdio.h&gt;int fclose(FILE *stream); 文件操作完成后,必须要用fclose()函数进行关闭,这是因为对打开的文件进行写入时,若文件缓冲区的空间未被写入的内容填满,这些内容不会写到打开的文件中去而丢失。只有对打开的文件进行关闭操作时,停留在文件缓冲区的内容才能写到该文件中去,从而使文件完整。再者一旦关闭了文件,该文件对应的FILE结构将被释放,从而使关闭的文件得到保护,因为这时对该文件的存取操作将不会进行。文件的关闭也意味着释放了该文件的缓冲区。 它表示该函数将关闭FILE指针对应的文件,并返回一个整数值。若成功地关闭了文件,则返回一个0值,否则返回一个非0值。 fgetc() 和 fputc() 按字符读写文件首先用 fopen() 读入一个文件FILE *fp = **fopen**(&quot;../data/file01.txt&quot;, &quot;r+&quot;); 指定文件位置以及读写格式，r+表示打开文件进行读写操作。 写操作使用 fputc() 逐个字符写入： 1234567char text[] = \"This is a text for test.\";int length = strlen(text);for(int i = 0; i &lt; length; i++)&#123; fputc(text[i], fp);&#125; 读操作使用 fgetc() 逐个字符读取，当读到结束标志位 EOF 时停止 123456789101112// 方法一while( (ch = fgetc(fp)) != EOF )&#123; printf(&quot;%c&quot;, ch);&#125;// 方法二while(!feof(fp))&#123; ch = fgetc(fp); printf(&quot;%c&quot;, ch);&#125; 逐个字符读写文件，示例代码 fgets() 和 fputs() 按行读写文件123456789101112131415161718192021222324252627282930/* 按行读写 */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main()&#123; FILE *fp = fopen(\"../data/file02.txt\", \"r+\"); if(fp == NULL) &#123; perror(\"fopen(): file is NULL.\"); &#125; char tmp[100] = \"aaaaaaaaa\"; fputs(tmp, fp); char buf[100]; while(!feof(fp)) &#123; char *p = fgets(buf, sizeof(buf), fp); if(p != NULL) &#123; printf(\"buf = %s\", buf); printf(\"p = %s\", p); &#125; &#125; return 0;&#125; fwrite() 和 fread() 按块读写文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#define _CRT_SECURE_NO_WARNINGS #include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#define FILE_NAME \"../data/file03.txt\"struct teacher&#123; int age; int id; char *name; int name_len;&#125;;int main(void)&#123; FILE *fp = NULL;#if 0 int write_ret = 0; fp = fopen(FILE_NAME, \"wb+\"); if (fp == NULL) &#123; fprintf(stderr, \"fopen error\\n\"); return -1; &#125; struct teacher t1; char *name = \"zhang3\"; t1.age = 10; t1.id = 20; t1.name = malloc(64); memset(t1.name, 0, 64); strcpy(t1.name, name); t1.name_len = strlen(name); write_ret = fwrite(&amp;t1, sizeof(struct teacher), 1, fp); if (write_ret &lt; 0) &#123; fprintf(stderr, \"write error\\n\"); return -1; &#125; write_ret = fwrite(t1.name, t1.name_len, 1, fp); if (write_ret &lt; 0) &#123; fprintf(stderr, \"write error\\n\"); return -1; &#125; if (fp != NULL) &#123; fclose(fp); &#125;#endif// #if 0 struct teacher t2 = &#123; 0 &#125;; int read_ret = 0; fp = fopen(FILE_NAME, \"rb+\"); if (fp == NULL) &#123; fprintf(stderr, \"fopen r+error\\n\"); return -1; &#125; read_ret = fread(&amp;t2, sizeof(struct teacher), 1, fp); if (read_ret &lt; 0) &#123; fprintf(stderr, \"fread error\\n\"); fclose(fp); return -1; &#125; t2.name = malloc(t2.name_len + 1); memset(t2.name, 0, t2.name_len + 1); read_ret = fread(t2.name, t2.name_len, 1, fp); if (read_ret &lt; 0) &#123; fprintf(stderr, \"fread error\\n\"); fclose(fp); return -1; &#125; printf(\"id : %d, age : %d, name:%s, name_len :%d\\n\", t2.id, t2.age, t2.name, t2.name_len); if (fp != NULL) &#123; fclose(fp); &#125;// #endif return 0;&#125;","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-写一个最简单的makefile","slug":"C:C++/C语言学习笔记-写一个最简单的makefile","date":"2019-05-11T00:58:12.000Z","updated":"2019-05-14T13:45:56.076Z","comments":true,"path":"post/24de7431.html","link":"","permalink":"http://shuiyujie.com/post/24de7431.html","excerpt":"示例代码","text":"示例代码 最简单的 makefile12app: main.c add.c mul.c sub.c gcc main.c add.c mul.c sub.c -o app 目标：生产名为 app 的可执行文件 依赖：可执行文件通过 .c 文件生成 命令：通过 gcc 命令生成 第一个版本的问题在于每次都需要编译所有的 .c 文件，如果想要修改哪个文件就只编译修改过的文件就可以这样那样写： 1234567891011121314app:main.o add.o mul.o sub.o gcc main.o add.o mul.o sub.o -o appmain.o:main.c gcc -c main.cadd.o:add.c gcc -c add.cmul.o:mul.c gcc -c mul.csub.o:sub.c gcc -c sub.c 目标：生成名为 app 的可执行文件 依赖：预编译完成的 .o 文件 命令：系列 gcc 命令 makefile的工作原理makefile中的变量12345678obj=main.o add.o mul.o sub.otarget=appCC=gcc$(target):$(obj) gcc $(obj) -o $(target) %.o:%.c $(CC) -c $&lt; -o $@ %.o:%.c模式匹配 自动变量，只能在规则中的命令中使用 $&lt;规则中第一个依赖 $@规则中的目标 $^规则中所有依赖 makefile中的函数1234567891011121314target=appsrc=$(wildcard ./*.c)obj=$(patsubst ./%.c, ./%.o, $(src))CC=gccCPPFLAGS=-I$(target):$(obj) $(CC) $(obj) -o $(target) %.o:%.c $(CC) -c $&lt; -o $@ .PHONY:cleanclean: rm -f $(obj) $(target)","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"C语言学习笔记-静态库、动态库的制作和使用","slug":"C:C++/C语言学习笔记-静态库、动态库的制作和使用","date":"2019-05-02T08:22:20.000Z","updated":"2019-05-14T13:47:38.544Z","comments":true,"path":"post/62b06016.html","link":"","permalink":"http://shuiyujie.com/post/62b06016.html","excerpt":"本文将介绍 C 语言静态库和动态库制作和使用的过程，系统环境是 Ubuntu16.04，其他系统制作方法可能略有差别。 示例代码","text":"本文将介绍 C 语言静态库和动态库制作和使用的过程，系统环境是 Ubuntu16.04，其他系统制作方法可能略有差别。 示例代码 目录结构12345678910.├── README.md├── include│ └── head.h├── lib├── main.c└── src ├── add.c ├── mul.c ├── sub.c include - 头文件目录 lib - 库文件目录 src - 源代码目录 静态库本节介绍如何将代码编译成静态库。当我们希望程序响应更快，或者不想提供源代码，我们可以使用静态库。 编译静态库123456789101112131415# 将 .c 文件编译成 .o 文件➜ lsadd.c mul.c sub.c➜ gcc *.c -c -I ../include➜ lsadd.c add.o mul.c mul.o sub.c sub.o# 将 .o 文件打包，生成 .a 文件➜ ar rcs libMyCalc.a *.o➜ lsadd.c libMyCalc.a mul.o sub.oadd.o mul.c sub.c# 将 .a 文件移动到 lib 目录下➜ mv libMyCalc.a ../lib 使用静态库1234567# 方式 1gcc main.c lib/libMyCalc.a -I include -o sum./sum# 方式 2gcc main.c -Iinclude -L lib -l MyCalc -o myapp./myapp 动态库 编译动态库123456789101112131415# .c 编译为 .o➜ src git:(master) ✗ lsadd.c mul.c sub.c➜ src git:(master) ✗ gcc -fPIC -c *.c -I ../include➜ src git:(master) ✗ lsadd.c add.o mul.c mul.o sub.c sub.o# .o 打包为 .so➜ src git:(master) ✗ gcc -shared -o libMyCalc.so *.o -Iinclude➜ src git:(master) ✗ lsadd.c libMyCalc.so mul.o sub.oadd.o mul.c sub.c# .so 移动到 lib➜ src git:(master) ✗ mv libMyCalc.so ../lib 使用动态库1234567# 方式 1gcc main.c lib/libMyCalc.so -o app -Iinclude./app# 方式 2gcc main.c -Iinclude -L lib -l MyCalc -o myapp./myapp 如果在使用动态库的过程中遇到了问题，请看下一小结。 动态库链接失败问题动态库是由动态连机器加载的，通过ldd app我们可以查看可执行文件执行的时候依赖的所有动态库。交由动态连接器管理的动态库都能被正确加载，比如说/lib目录下的库文件。我们自定义的动态库没有被加载就是因为没有告知动态连接器我们的动态库在哪里。 方法一 临时配置要想汤动态连机器知道我们的动态库在哪，一种方式是直接把自定义的动态库丢到/lib目录下，这样自然就能加载到了，当然我想没有人会想这么做的。另一种方式就是在配置文件中配置动态库的位置，比如像下面的这样： 1export LD_LIBRARY_PATH=我们的动态库位置 LD_LIBRARY_PATH我们可以将动态库的位置保存在这里，此时就能顺利运行程序，但是重启终端将会失效，临时测试的时候使用很方便。 方法二 永久配置1231. vim /etc/ld.so.conf2. 将动态库的路径配置在里面3. sudo ldconfig -v","categories":[{"name":"C/C++","slug":"C-C","permalink":"http://shuiyujie.com/categories/C-C/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://shuiyujie.com/tags/c/"},{"name":"c","slug":"c","permalink":"http://shuiyujie.com/tags/c/"}]},{"title":"消息队列的连环炮","slug":"分布式/消息队列/消息队列的连环炮","date":"2019-03-23T15:26:49.000Z","updated":"2019-10-04T03:01:35.491Z","comments":true,"path":"post/b63a0941.html","link":"","permalink":"http://shuiyujie.com/post/b63a0941.html","excerpt":"1. 引子消息队列分布式系统中重要的组件，一种存放消息的容器，主要作用有解耦、异步、削锋，是大型分布式系统不可缺少的中间件。 常见的消息队列有 ActiveMQ，RabbitMQ，RocketMQ，Kafka。 简历中涉及到了消息队列，面试官先问了这样几个问题： 你们系统里为什么要使用消息队列？ 既然使用了消息队列，说说他还有什么使用场景？ 消息队列的优缺点是什么？","text":"1. 引子消息队列分布式系统中重要的组件，一种存放消息的容器，主要作用有解耦、异步、削锋，是大型分布式系统不可缺少的中间件。 常见的消息队列有 ActiveMQ，RabbitMQ，RocketMQ，Kafka。 简历中涉及到了消息队列，面试官先问了这样几个问题： 你们系统里为什么要使用消息队列？ 既然使用了消息队列，说说他还有什么使用场景？ 消息队列的优缺点是什么？ 2. 为什么使用消息队列？我的回答：甲方提供 EOS 充值服务，我方进行调用。出于解耦的目的，引入了消息队列。 一个类似应试的回答方法，就是思考面试官问这个问题是出于什么目的，想获得的是什么样的答案？ 当问到为什么使用消息队列时，面试官期望的回答是公司的 xxx 业务遇到了挑战，不用 MQ 会有麻烦，使用 MQ 之后带来了好处。 通过一个问题就能看出是为了用而用，还是经过思考之后使用。 3. 消息队列的使用场景？问消息队列的使用场景，和问消息队列有什么优点，消息队列有什么作用是等价的。 消息队列的作用主要有三个解耦、异步、削峰。 解耦 B,C,D 系统需要使用 A 系统产生的关键数据。 无消息队列时 系统 A 为系统 B、C、D 等提供各自的接口，导致系统 A 与它们紧密耦合 添加系统 E 又需要接口，删除 B 系统原接口又没用了 有消息队列时 系统 A 作为生产者，将消息发送到消息队列 系统 B、C、D 作为消费者订阅消息 新增消费者只需订阅消息，对原系统和业务没有影响 异步 用户请求数据时，系统的响应时间是保证用户体验很重要的一部分。 无消息队列时 用户请求 A 系统，A 系统需要等待 BCD 执行完成之后响应 用户收到响应用时近 1 秒 用消息队列时 用户请求 A 系统，A 系统将请求推到消息队列中，B、C、D 异步执行 用户收到响应用时 200 毫秒 削峰 秒杀场景下，每秒有 5000 个请求，Mysql 每秒最大处理 2000 条 sql。 无消息队列时 用户请求数据直接写入数据库，高并发时数据库压力剧增，甚至奔溃 Mysql 宕机，整个系统都不能用了 有消息队列时系统 B、C、D 用户请求数据先存入 MQ 中 系统 A 每秒读取 2000 条数据进行处理 每秒多出 3000 条未处理数据按场景稍后处理 4. 消息队列有什么缺点？优点前面已经说过了，还需要讨论一下缺点。 为什么要问缺点是什么？凡事都有两面性，如果只是考虑到消息队列的优点，而没有考虑缺点，这就是一个潘多拉的魔盒。打开魔盒，接踵而来的会是一系列的意外。 推广到引入其他技术亦然，只有考虑到缺点之后才可以采取额外的技术方案或者架构来规避这些缺点。 系统可用性降低 系统引入的外部依赖越多，宕机的可能性就越大 系统引入消息队列，就要考虑消息队列的可靠性 比如原本只需要考虑 A,B,C,D 四个系统 引入消息队列之后就需要考虑 A,B,C,D 四个系统外加消息队列 系统复杂度提高 消息重复消费问题 消息丢失问题 消息传递顺序问题 一致性问题 A 系统处理完返回成功，即认为请求成功 但是也存在 BC 系统写入成功，而 D 系统写入失败的情况 这样的情况就是数据不一致 总结面试官问到 MQ 的时候，希望考察我们在使用 MQ 的时候是否有过自己的思考。没有完美的技术，任何技术都具有两面性，要考虑它的使用场景，并且对可能遇到的风险做到心中有数，提前预防。 思考引入消息队列之后： 如何保证高可用？ 如何避免消息的重复消费和消息丢失？ 如何保证消息的顺序执行？ 下一篇文章一起讨论。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"消息队列","slug":"分布式/消息队列","permalink":"http://shuiyujie.com/categories/分布式/消息队列/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/tags/分布式/"},{"name":"消息队列","slug":"消息队列","permalink":"http://shuiyujie.com/tags/消息队列/"}]},{"title":"各种消息队列的对比","slug":"分布式/消息队列/各种消息队列的对比","date":"2019-03-23T14:24:19.000Z","updated":"2019-10-04T11:45:19.464Z","comments":true,"path":"post/59dbbfed.html","link":"","permalink":"http://shuiyujie.com/post/59dbbfed.html","excerpt":"我们选择 MQ 的时候主要对比了 4 种常用的 MQ，分别是 Kafka、ActiveMQ、RabbitMQ 和 RocketMQ。 首先淘汰的 ActiveMQ，最早的时候很多人用 RabbitMQ，但是现在它的单机吞吐量只有万级，社区也没怎么维护了，有种日落西山的意味。 Kafka 主要特点是基于 Pull 的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，更加适合大数据量的数据收集服务，我们的当时的系统能到吞吐量能到万级撑死了，也就没选 Kafka。 RabbitMQ 虽然它是用 erlang 语言开发，不适合我们进行二次开发，但是它的单机吞吐量也有十万，社区比较活跃，功能也比较完备，比较适合我们公司数据量不是很大的场景。 RocketMQ 其实也挺好的，阿里开源的消息队列，经过双十一的考验。不过我们的 Leader 觉得 RocketMQ 的社区没有 RabbitMQ 的稳，RabbitMQ 功能也足够完备。 最终就选择了 RabbitMQ 作为我们使用的消息中间件。","text":"我们选择 MQ 的时候主要对比了 4 种常用的 MQ，分别是 Kafka、ActiveMQ、RabbitMQ 和 RocketMQ。 首先淘汰的 ActiveMQ，最早的时候很多人用 RabbitMQ，但是现在它的单机吞吐量只有万级，社区也没怎么维护了，有种日落西山的意味。 Kafka 主要特点是基于 Pull 的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，更加适合大数据量的数据收集服务，我们的当时的系统能到吞吐量能到万级撑死了，也就没选 Kafka。 RabbitMQ 虽然它是用 erlang 语言开发，不适合我们进行二次开发，但是它的单机吞吐量也有十万，社区比较活跃，功能也比较完备，比较适合我们公司数据量不是很大的场景。 RocketMQ 其实也挺好的，阿里开源的消息队列，经过双十一的考验。不过我们的 Leader 觉得 RocketMQ 的社区没有 RabbitMQ 的稳，RabbitMQ 功能也足够完备。 最终就选择了 RabbitMQ 作为我们使用的消息中间件。 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"消息队列","slug":"分布式/消息队列","permalink":"http://shuiyujie.com/categories/分布式/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://shuiyujie.com/tags/消息队列/"}]},{"title":"如何使用Jupyter notebook","slug":"Python/如何使用Jupyter-notebook","date":"2019-03-21T23:51:30.000Z","updated":"2019-07-15T14:11:45.989Z","comments":true,"path":"post/80ec4f16.html","link":"","permalink":"http://shuiyujie.com/post/80ec4f16.html","excerpt":"Notebook 已迅速成为处理数据的必备工具。其已知用途包括数据清理和探索、可视化、机器学习和大数据分析。GitHub 上面也会自动提供 notebook。借助此出色的功能，你可以轻松共享工作。http://nbviewer.jupyter.org/ 也会提供 GitHub 代码库中的 notebook 或存储在其他地方的 notebook。","text":"Notebook 已迅速成为处理数据的必备工具。其已知用途包括数据清理和探索、可视化、机器学习和大数据分析。GitHub 上面也会自动提供 notebook。借助此出色的功能，你可以轻松共享工作。http://nbviewer.jupyter.org/ 也会提供 GitHub 代码库中的 notebook 或存储在其他地方的 notebook。 Notebook 运行的核心是 notebook 服务器。你通过浏览器连接到该服务器，而 notebook 呈现为 Web 应用。你在 Web 应用中编写的代码通过该服务器发送给内核。内核运行代码并将代码发送回该服务器，之后，任何输出都会返回到浏览器中。保存 notebook 时，它作为 JSON 文件（文件扩展名为 .ipynb）写入到该服务器中。 使用 Jupyter notebook安装 notebook安装 Jupyter 的最简单方法是使用 Anaconda。该发行版自动附带了 Jupyter notebook。你能够在默认环境下使用 notebook。 要在 conda 环境中安装 Jupyter notebook，请使用 conda install jupyter notebook。 也可以通过 pip 使用 pip install jupyter notebook 来获得 Jupyter notebook。 查看如何使用Anaconda了解如何使用Anaconda。 启动 notebook 服务器要启动 notebook 服务器，请在终端或控制台中输入 jupyter notebook。服务器会在你运行此命令的目录中启动。这意味着任何 notebook 文件都会保存在该目录中。你通常希望在 notebook 所在的目录中启动服务器。不过，你可以在文件系统中导航到 notebook 所在的位置。 运行此命令时（请自己试一下！），服务器主页会在浏览器中打开。默认情况下，notebook 服务器的运行地址是 http://localhost:8888。如果启动其他服务器，新服务器会尝试使用端口 8888，但由于此端口已被占用，因此新服务器会在端口 8889 上运行。之后，可以通过 http://localhost:8889 连接到新服务器。每台额外的 notebook 服务器都会像这样增大端口号。 在右侧，你可以点击“New”（新建），创建新的 notebook、文本文件、文件夹或终端。“Notebooks”下的列表显示了你已安装的内核。由于我在 Python 3 环境中运行服务器，因此列出了 Python 3 内核。 顶部的选项卡是 Files（文件）、Running（运行）和 Cluster（聚类）。Files（文件）显示当前目录中的所有文件和文件夹。点击 Running（运行）选项卡会列出所有正在运行的 notebook。可以在该选项卡中管理这些 notebook。 关闭 notebook通过在服务器主页上选中 notebook 旁边的复选框，然后点击“Shutdown”（关闭），你可以关闭各个 notebook。但是，在这样做之前，请确保你保存了工作！否则，在你上次保存后所做的任何更改都会丢失。下次运行 notebook 时，你还需要重新运行代码。 通过在终端中按两次 Ctrl + C，可以关闭整个服务器。再次提醒，这会立即关闭所有运行中的 notebook，因此，请确保你保存了工作！ 远程运行 notebook如果你想在本地或者远程的机器上安装Jupyter Notebook，可以参考下面的两个文档。 安装：https://jupyter.org/install.html 运行：https://jupyter.readthedocs.io/en/latest/running.html#running 后台运行使用 jupyter notebook --allow-root &gt; jupyter.log 2&gt;&amp;1 &amp;或者 nohup jupyter notebook --allow-root &gt; jupyter.log 2&gt;&amp;1 &amp;。 用&amp;让命令后台运行, 并把标准输出写入 jupyter.log 中。nohup表示no hang up, 就是不挂起, 于是这个命令执行后即使终端退出, 也不会停止运行. Notebook 界面Cell新建的一个 notebook 之后，默认就会有一个 cell，也就是下图中蓝色的小框。 Cell 可以称为单元格。单元格是你编写和运行代码的地方。 工具栏从左侧开始，工具栏上的其他控件是： 落伍的软盘符号，表示“保存”。请记得保存 notebook！ + 按钮用于创建新的单元格 然后是用于剪切、复制和粘贴单元格的按钮。 运行、停止、重新启动内核 单元格类型：代码、Markdown、原始文本和标题 命令面板（见下文） 单元格工具栏，提供不同的单元格选项（例如将单元格用作幻灯片） 命令面板小键盘符号代表命令面板。点击它会弹出一个带有搜索栏的面板，供你搜索不同的命令。这能切实帮助你加快工作速度，因为你无需使用鼠标翻查各个菜单。你只需打开命令面板，然后键入要执行的操作。 生成目录 conda install -c conda-forge jupyter_contrib_nbextensions 在 Nbexyensions 选项卡中开启 Table of Contents(2) 选项，即可在侧边栏生成目录。 快捷键Notebook 提供了许多快捷键，我们可以使用shift + command + P呼出命令行面板，在输入keyboard就可以查看快捷键列表。 键入Esc进入命令模式，即可使用这些快捷键。 Magic 关键字Magic 关键字是可以在单元格中运行的特殊命令，能让你控制 notebook 本身或执行系统调用（例如更改目录）。例如，可以使用 %matplotlib 将 matplotlib 设置为以交互方式在 notebook 中工作。 Magic 命令的前面带有一个或两个百分号（% 或 %%），分别对应行 Magic 命令和单元格 Magic 命令。行 Magic 命令仅应用于编写 Magic 命令时所在的行，而单元格 Magic 命令应用于整个单元格。 注意：这些 Magic 关键字是特定于普通 Python 内核的关键字。如果使用其他内核，这些关键字很有可能无效。 代码计时有时候，你可能要花些精力优化代码，让代码运行得更快。在此优化过程中，必须对代码的运行速度进行计时。可以使用 Magic 命令 timeit 测算单元格中代买运行时间和函数的运行时间。 测试整个单元格代码运行时间的时候可以添加%%timeit 1234%%timeitsum = 0for i in range(100): sum += i就会输出 13.97 µs ± 150 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) 测试整个某个函数代码运行时间的时候可以添加%timeit 1234def sum(): sum = 0 for i in range(100): sum += i 1%timeit sum() 14.06 µs ± 180 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) 在 notebook 中进行调试对于 Python 内核，可以使用 Magic 命令 %pdb 开启交互式调试器。出错时，你能检查当前命名空间中的变量。 要详细了解 pdb，请阅读此文档。要退出调试器，在提示符中输入 q 即可。 补充读物Magic 命令还有很多，我只是介绍了你将会用得最多的一些命令。要了解更多信息，请查看此列表，它列出了所有可用的 Magic 命令。 转换 notebookNotebook 只是扩展名为 .ipynb 的大型 JSON 文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&#123; \"cells\": [ &#123; \"cell_type\": \"code\", \"execution_count\": 8, \"metadata\": &#123;&#125;, \"outputs\": [ &#123; \"name\": \"stdout\", \"output_type\": \"stream\", \"text\": [ \"3.97 µs ± 150 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\n\" ] &#125; ], \"source\": [ \"%%timeit\\n\", \"sum = 0\\n\", \"for i in range(100):\\n\", \" sum += i\" ] &#125;, &#123; \"cell_type\": \"code\", \"execution_count\": 9, \"metadata\": &#123; \"collapsed\": true &#125;, \"outputs\": [], \"source\": [ \"def sum():\\n\", \" sum = 0\\n\", \" for i in range(100):\\n\", \" sum += i\" ] &#125;, &#123; \"cell_type\": \"code\", \"execution_count\": 10, \"metadata\": &#123;&#125;, \"outputs\": [ &#123; \"name\": \"stdout\", \"output_type\": \"stream\", \"text\": [ \"4.06 µs ± 180 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\\n\" ] &#125; ], \"source\": [ \"%timeit sum()\" ] &#125;, &#123; \"cell_type\": \"code\", \"execution_count\": null, \"metadata\": &#123; \"collapsed\": true &#125;, \"outputs\": [], \"source\": [] &#125; ], \"metadata\": &#123; \"kernelspec\": &#123; \"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\" &#125;, \"language_info\": &#123; \"codemirror_mode\": &#123; \"name\": \"ipython\", \"version\": 3 &#125;, \"file_extension\": \".py\", \"mimetype\": \"text/x-python\", \"name\": \"python\", \"nbconvert_exporter\": \"python\", \"pygments_lexer\": \"ipython3\", \"version\": \"3.6.5\" &#125; &#125;, \"nbformat\": 4, \"nbformat_minor\": 2&#125; 由于 notebook 是 JSON 文件，因此，可以轻松将其转换为其他格式。Jupyter 附带了一个名为 nbconvert 的实用程序，可将 notebook 转换为 HTML、Markdown、幻灯片等格式。 例如，要将 notebook 转换为 HTML 文件，请在终端中使用 1jupyter nbconvert --to html notebook.ipynb 要将 notebook 与不使用 notebook 的其他人共享，转换为 HTML 很有用。而要在博客和其他接受 Markdown 格式化的文本编辑器中加入 notebook，Markdown 很合适。 像平常一样，要详细了解 nbconvert，请阅读相关文档。 更好地使用现在的技术趋势，则是彻底云端化了，例如Jupyter官方的Binder平台（介绍文 档：https://mybinder.readthedocs.io/en/latest/index.html）和Google提供的 Google Colab环境（介 绍：https://colab.research.google.com/notebooks/welcome.ipynb）。它们让Jupyter Notebook变得和石墨文档、Google Doc在线文档一样，在浏览器点开链接就能运行。 所以，现在当你用Binder打开一份GitHub上的Jupyter Notebook时，你不需要安装任何软件，直接在浏览器 打开一份代码，就能在云端运行。 比如这样一个GitHub文件。在Binder中，你只要输入其对应的GitHub Repository的名字或者URL，就能在 云端打开整个Repository，选择你需要的notebook。 另外，还有下面这些 Jupyter Notebook，可以作为实践的第一站。 第一个是Jupyter官方：https://mybinder.org/v2/gh/binder-examples/matplotlib-versions/mpl-v2.0/?filepath=matplotlib_versions_demo.ipynb 第二个是Google Research提供的Colab环境，尤其适合机器学习的实践应 用：https://colab.research.google.com/notebooks/basic_features_overview.ipynb","categories":[{"name":"Python","slug":"Python","permalink":"http://shuiyujie.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://shuiyujie.com/tags/python/"}]},{"title":"如何使用Anaconda","slug":"Python/如何使用Anaconda","date":"2019-03-20T12:11:03.000Z","updated":"2019-07-04T14:19:02.011Z","comments":true,"path":"post/9d29b615.html","link":"","permalink":"http://shuiyujie.com/post/9d29b615.html","excerpt":"Anaconda 能让你轻松安装在数据科学工作中经常使用的包。你还将使用它创建虚拟环境，以便更轻松地处理多个项目。Anaconda 简化了我的工作流程，并且解决了我在处理包和多个 Python 版本时遇到的大量问题。","text":"Anaconda 能让你轻松安装在数据科学工作中经常使用的包。你还将使用它创建虚拟环境，以便更轻松地处理多个项目。Anaconda 简化了我的工作流程，并且解决了我在处理包和多个 Python 版本时遇到的大量问题。 Anaconda 实际上是一个软件发行版，它附带了 conda、Python 和 150 多个科学包及其依赖项。应用程序 conda 是包和环境管理器。Anaconda 的下载文件比较大（约 500 MB），因为它附带了 Python 中最常用的数据科学包。 conda 是一种只能通过命令行来使用的程序。conda 与 pip 相似，不同之处是可用的包以数据科学包为主，而 pip 适合一般用途。但是，conda 并非像 pip 那样专门适用于 Python，它也可以安装非 Python 的包。它是适用于任何软件堆栈的包管理器。也就是说，并非所有的 Python 库都能通过 Anaconda 发行版和 conda 获得。在使用 conda 的同时，你仍可以并且仍将使用 pip 来安装包。 Conda 安装了预编译的包。例如，Anaconda 发行版附带了使用 MKL 库编译的 Numpy、Scipy 和 Scikit-learn，从而加快了各种数学运算的速度。这些包由发行版的贡献者维护，这意味着它们通常滞后于新版本。但是，由于有人需要为许多系统构建这些包，因此，它们往往更为稳定，而且更便于你使用。 除了管理包之外，conda 还是虚拟环境管理器。它类似于另外两个很流行的环境管理器，即 virtualenv 和 pyenv。 环境能让你分隔你要用于不同项目的包。你常常要使用依赖于某个库的不同版本的代码。例如，你的代码可能使用了 Numpy 中的新功能，或者使用了已删除的旧功能。实际上，不可能同时安装两个 Numpy 版本。你要做的应该是，为每个 Numpy 版本创建一个环境，然后在适用于项目的环境中工作。 安装 AnacondaAnaconda 可用于 Windows、Mac OS X 和 Linux。可以在 https://docs.anaconda.com/anaconda/install/ 上找到安装程序和安装说明。 此外，和我一样使用 Ubuntu 的朋友参考这两篇优秀的文章：How To Install the Anaconda Python Distribution on Ubuntu 16.04 和 How to Install Anaconda on Ubuntu 18.04。 完成安装后，会自动进入默认的 conda 环境，而且所有包均已安装完毕，如下面所示。可以在终端或命令提示符中键入 conda list，以查看你安装的内容。 初次安装下的软件包版本一般都比较老旧，因此提前更新可以避免未来不必要的问题，初次安装可以键入以下命令更新所有的软件包： conda upgrade –all Anaconda 可以使用命令行的方式使用，也可以使用 GUI，下面将介绍用命令行的方式使用。 conda 切换为国内源1234567conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/conda config --set show_channel_urls yes 123456channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: true 关于停止Anaconda镜像服务的通知。 管理包安装了 Anaconda 之后，管理包是相当简单的。要安装包，请在终端中键入 conda install package_name。例如，要安装 numpy，请键入 conda install numpy。 你还可以同时安装多个包。类似 conda install numpy scipy pandas 的命令会同时安装所有这些包。还可以通过添加版本号（例如 conda install numpy=1.10）来指定所需的包版本。 Conda 还会自动为你安装依赖项。例如，scipy 依赖于 numpy，因为它使用并需要 numpy。如果你只安装 scipy (conda install scipy)，则 conda 还会安装 numpy（如果尚未安装的话）。 大多数命令都是很直观的。要卸载包，请使用 conda remove package_name。要更新包，请使用 conda update package_name。如果想更新环境中的所有包（这样做常常很有用），请使用 conda update --all。最后，要列出已安装的包，请使用前面提过的 conda list。 如果不知道要找的包的确切名称，可以尝试使用 conda search search_term 进行搜索。例如，我知道我想安装 Beautiful Soup，但我不清楚确切的包名称。因此，我尝试执行 conda search beautifulsoup。 它返回可用的 Beautiful Soup 包的列表，并列出了相应的包名称 beautifulsoup4。 管理环境列出环境可以使用 conda env list 列出你创建的所有环境。你会看到环境的列表，而且你当前所在环境的旁边会有一个星号。默认的环境名为 root。 创建环境如前所述，conda 是虚拟环境管理器，可以使用 conda 创建环境以隔离项目。 要创建环境，请在终端中使用 conda create -n env_name list of packages。在这里，-n env_name 设置环境的名称（-n 是指名称），而 list of packages 是要安装在环境中的包的列表。例如，要创建名为 my_env 的环境并在其中安装 numpy，请键入 conda create -n my_env numpy。 创建环境时，可以指定要安装在环境中的 Python 版本。这在你同时使用 Python 2.x 和 Python 3.x 中的代码时很有用。要创建具有特定 Python 版本的环境，请键入类似于 conda create -n py3 python=3 或 conda create -n py2 python=2 的命令。实际上，我在我的个人计算机上创建了这两个环境。我将它们用作与任何特定项目均无关的通用环境，以处理普通的工作（可轻松使用每个 Python 版本）。这些命令将分别安装 Python 3 和 2 的最新版本。要安装特定版本（例如 Python 3.3），请使用 conda create -n py python=3.3。 进入环境创建了环境后，在 OSX/Linux 上使用 source activate my_env 进入环境。在 Windows 上，请使用 activate my_env。 进入环境后，你会在终端提示符中看到环境名称，它类似于 (my_env) ~ $。环境中只安装了几个默认的包，以及你在创建它时安装的包。可以使用 conda list 检查这一点。在环境中安装包的命令与前面一样：conda install package_name。不过，这次你安装的特定包仅在你进入环境后才可用。要离开环境，请键入 source deactivate（在 OSX/Linux 上）。在 Windows 上，请使用 deactivate。 保存和加载环境共享环境这项功能确实很有用，它能让其他人安装你的代码中使用的所有包，并确保这些包的版本正确。可以使用 conda env export &gt; environment.yaml 将包保存为 YAML。第一部分 conda env export 输出环境中的所有包的名称（包括 Python 版本）。 上图可以看到列出了环境的名称和所有依赖项及其版本。导出命令的第二部分 &gt; environment.yaml 将导出的文本写入到 YAML 文件 environment.yaml 中。现在可以共享此文件，而且其他人能够创建和你用于项目相同的环境。 要通过环境文件创建环境，请使用 conda env create -f environment.yaml。这会创建一个新环境，而且它具有在 environment.yaml 中列出的同样的库。 删除环境如果你不再使用某些环境，可以使用 conda env remove -n env_name 删除指定的环境（在这里名为 env_name）。 最佳实践使用环境对我帮助很大的一点是，我的 Python 2 和 Python 3 具有独立的环境。我使用了 conda create -n py2 python=2 和 conda create -n py3 python=3 创建两个独立的环境，即 py2 和 py3。现在，我的每个 Python 版本都有一个通用环境。在所有这些环境中，我都安装了大多数标准的数据科学包（numpy、scipy、pandas 等）。 我还发现，为我从事的每个项目创建环境很有用。这对于与数据不相关的项目（例如使用 Flask 开发的 Web 应用）也很有用 共享环境在 GitHub 上共享代码时，最好同样创建环境文件并将其包括在代码库中。这能让其他人更轻松地安装你的代码的所有依赖项。对于不使用 conda 的人，我通常还会使用 pip freeze（在此处了解详情）将一个 pip requirements.txt 文件包括在内。 了解更多信息要详细了解 conda 和它如何融入到 Python 生态系统中，请查看这篇由 Jake Vanderplas 撰写的文章：Conda myths and misconceptions（有关 conda 的迷思和误解）。此外，有空也可以参考这篇 conda 文档。","categories":[{"name":"Python","slug":"Python","permalink":"http://shuiyujie.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://shuiyujie.com/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"http://shuiyujie.com/tags/anaconda/"}]},{"title":"Docker容器发展历史","slug":"docker&k8s/Docker容器发展历史","date":"2019-03-19T14:21:11.000Z","updated":"2019-09-20T14:50:06.339Z","comments":true,"path":"post/9da65e04.html","link":"","permalink":"http://shuiyujie.com/post/9da65e04.html","excerpt":"容器技术的兴起源于 PaaS 技术的普及； Docker 公司发布的 Docker 项目具有里程碑式的意义； Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。 容器本身没有价值，有价值的是“容器编排”。","text":"容器技术的兴起源于 PaaS 技术的普及； Docker 公司发布的 Docker 项目具有里程碑式的意义； Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。 容器本身没有价值，有价值的是“容器编排”。 Docker 解决 Paas 应用打包困难的问题2013~2014 年，以 Cloud Foundry 为代表的 PaaS 项目，逐渐完成了教育用户和开拓市场的艰巨任务，也正是在这个将概念逐渐落地的过程中，应用“打包”困难这个问题，成了整个后端技术圈子的一块心病。 出现这个问题的根本原因是，一旦用上了 PaaS，用户就必须为每种语言、每种框架，甚至每个版本的应用维护一个打好的包。这个打包过程，没有任何章法可循，更麻烦的是，明明在本地运行得好好的应用，却需要做很多修改和配置工作才能在 PaaS 里运行起来。而这些修改和配置，并没有什么经验可以借鉴，基本上得靠不断试错，直到你摸清楚了本地应用和远端 PaaS 匹配的“脾气”才能够搞定。 Docker 项目的出现，则为这个根本性的问题提供了一个近乎完美的解决方案。这正是 Docker 项目刚刚开源不久，就能够带领一家原本默默无闻的 PaaS 创业公司脱颖而出，然后迅速占领了所有云计算领域头条的技术原因。 Docker 项目给 PaaS 世界带来的“降维打击”，其实是提供了一种非常便利的打包机制。这种机制直接打包了应用运行所需要的整个操作系统，从而保证了本地环境和云端环境的高度一致，避免了用户通过“试错”来匹配两种不同运行环境之间差异的痛苦过程。 Docker 与开发者紧密结合并迅速崛起 Docker 镜像通过技术手段解决了 PaaS 的根本性问题； Docker 容器同开发者之间有着与生俱来的密切关系； PaaS 概念已经深入人心的完美契机。 相比于其他正在企业级市场里厮杀得头破血流的经典 PaaS 项目们，Docker 项目的推广策略从一开始就呈现出一副“憨态可掬”的亲人姿态，把每一位后端技术人员（而不是他们的老板）作为主要的传播对象。 简洁的 UI，有趣的 demo，“1 分钟部署一个 WordPress 网站”“3 分钟部署一个 Nginx 集群”，这种同开发者之间与生俱来的亲近关系，使 Docker 项目迅速成为了全世界 Meetup 上最受欢迎的一颗新星。 Docker 一家独大企图进一步商业化实际上，Docker 项目一日千里的发展势头，一直伴随着公司管理层和股东们的阵阵担忧。他们心里明白，虽然 Docker 项目备受追捧，但用户们最终要部署的，还是他们的网站、服务、数据库，甚至是云计算业务。 Docker 项目希望提供更多平台层能力，发布了 Docker Swarm、Fig（后来的 Compose）项目，向 Paas 进化。 Docker 也和 CoreOS 公司、RedHat 等产生利益冲突，同时拒绝了微软的天价收购，为后来 Kubernetes 项目的诞生埋下了伏笔。 Kubernetes 推出，容器技术尘埃落定Docker 公司在 Docker 开源项目的发展上，始终保持着绝对的权威和发言权，并在多个场合用实际行动挑战到了其他玩家（比如，CoreOS、RedHat，甚至谷歌和微软）的切身利益。 于是，2015 年 6 月 22 日，由 Docker 公司牵头，CoreOS、Google、RedHat 等公司共同宣布，Docker 公司将 Libcontainer 捐出，并改名为 RunC 项目，交由一个完全中立的基金会管理，然后以 RunC 为依据，大家共同制定一套容器和镜像的标准和规范。 这套标准和规范，就是 OCI（ Open Container Initiative ）。OCI 的提出，意在将容器运行时和镜像的实现从 Docker 项目中完全剥离出来。这样做，一方面可以改善 Docker 公司在容器技术上一家独大的现状，另一方面也为其他玩家不依赖于 Docker 项目构建各自的平台层能力提供了可能。 所以这次，Google、RedHat 等开源基础设施领域玩家们，共同牵头发起了一个名为 CNCF（Cloud Native Computing Foundation）的基金会。这个基金会的目的其实很容易理解：它希望，以 Kubernetes 项目为基础，建立一个由开源基础设施领域厂商主导的、按照独立基金会方式运营的平台级社区，来对抗以 Docker 公司为核心的容器商业生态。 而为了打造出这样一个围绕 Kubernetes 项目的“护城河”，CNCF 社区就需要至少确保两件事情： Kubernetes 项目必须能够在容器编排领域取得足够大的竞争优势； CNCF 社区必须以 Kubernetes 项目为核心，覆盖足够多的场景。 2017 年 10 月，Docker 公司出人意料地宣布，将在自己的主打产品 Docker 企业版中内置 Kubernetes 项目，这标志着持续了近两年之久的“编排之争”至此落下帷幕。 2018 年 1 月 30 日，RedHat 宣布斥资 2.5 亿美元收购 CoreOS。 2018 年 3 月 28 日，这一切纷争的始作俑者，Docker 公司的 CTO Solomon Hykes 宣布辞职，曾经纷纷扰扰的容器技术圈子，到此尘埃落定。 参考01 | 预习篇 · 小鲸鱼大事记（一）：初出茅庐 02 | 预习篇 · 小鲸鱼大事记（二）：崭露头角 03 | 预习篇 · 小鲸鱼大事记（三）：群雄并起 04 | 预习篇 · 小鲸鱼大事记（四）：尘埃落定","categories":[{"name":"Docker&K8s","slug":"Docker-K8s","permalink":"http://shuiyujie.com/categories/Docker-K8s/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://shuiyujie.com/tags/docker/"},{"name":"k8s","slug":"k8s","permalink":"http://shuiyujie.com/tags/k8s/"}]},{"title":"Ubuntu16.04配置OpenCV环境","slug":"OpenCV/Ubuntu16-04配置OpenCV环境","date":"2019-03-19T11:42:00.000Z","updated":"2019-07-03T13:28:51.541Z","comments":true,"path":"post/5359e313.html","link":"","permalink":"http://shuiyujie.com/post/5359e313.html","excerpt":"本文介绍在 Ubuntu16.04 中配置 OpenCV 的环境 Linux 环境: Ubuntu16.04 LST 软件版本: opencv-3.4.0 、opencv_contrib-3.4.0 (安装包在文末下载) 参考文档：Installation in Linux","text":"本文介绍在 Ubuntu16.04 中配置 OpenCV 的环境 Linux 环境: Ubuntu16.04 LST 软件版本: opencv-3.4.0 、opencv_contrib-3.4.0 (安装包在文末下载) 参考文档：Installation in Linux 依赖安装执行以下命令 12345[compiler] sudo apt-get install build-essential[required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev cmake-gui[optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev 下载以下两个文件，注意版本 opencv-3.4.0 、opencv_contrib-3.4.0 解压文件，并新建一个build文件夹作为编译目录，如下 1234opencv/├── build├── opencv-3.4.0└── opencv_contrib-3.4.0 使用CMake-gui进行编译使用cmake-gui生成Makefile,并进行编译 生成 Makefile 点击 Browse_Source… 引入 opencv-3.4.0 文件夹 点击 Browse_Build… 引入 build 文件夹 不想编译 protobuf 可以选择 -D BUILD_PROTOBUF=OFF PROTOBUF_UPDATE_FILES=ON dnn 也会依赖 protobuf 也可以选择不编译 cuda 相关的用不到也不可以不编译 点击 Configure 点击 Generate 弹出如下对话框，选择使用Makefile生成的系统平台 123注：1. 第一次会弹出`CMakeSetup`选择`default`那个就可以了2. 开始的时候没有上图红色部分 之后我们勾选下面这几个选项 编译成静态库，取消选择 BUILD_SHARED_LIBS 勾选 OPENCV_ENABLE_NOFREE 选择 OPENCV_EXTRA_MODULES_PATH 引入 opencv_contrib-3.4.0 下的 modules 文件夹 点击 Configure 点击 Generate 编译进入 build 文件夹路径 make -j7 # 7 表示7个并发 修改 opencv.pv 文件 cd /build/unix_install vim opencv.pc 12345678910111213# Package Information for pkg-configprefix=/usr/localexec_prefix=$&#123;prefix&#125;libdir=$&#123;exec_prefix&#125;/libincludedir_old=$&#123;prefix&#125;/include/opencvincludedir_new=$&#123;prefix&#125;/includeName: OpenCVDescription: Open Source Computer Vision LibraryVersion: 3.4.0Libs: -L$&#123;exec_prefix&#125;/lib -lopencv_stitching -lopencv_superres -lopencv_videostab -lopencv_aruco -lopencv_bgsegm -lopencv_bioinspired -lopencv_ccalib -lopencv_dpm -lopencv_face -lopencv_photo -lopencv_freetype -lopencv_fuzzy -lopencv_img_hash -lopencv_line_descriptor -lopencv_optflow -lopencv_reg -lopencv_rgbd -lopencv_saliency -lopencv_stereo -lopencv_structured_light -lopencv_phase_unwrapping -lopencv_surface_matching -lopencv_tracking -lopencv_datasets -lopencv_text -lopencv_dnn -lopencv_plot -lopencv_xfeatures2d -lopencv_shape -lopencv_video -lopencv_ml -lopencv_ximgproc -lopencv_calib3d -lopencv_features2d -lopencv_highgui -lopencv_videoio -lopencv_flann -lopencv_xobjdetect -lopencv_imgcodecs -lopencv_objdetect -lopencv_xphoto -lopencv_imgproc -lopencv_coreLibs.private: -ldl -lm -lpthread -lrtCflags: -I$&#123;includedir_old&#125; -I$&#123;includedir_new&#125; 将文件中的Libs.private:删除 sudo make install sudo updatedb sudo ldconfig 运行写一段测试程序，可以使用命令行的方式或者 CMake 进行编译 测试程序12345678910111213141516171819202122232425262728293031323334353637#include \"opencv2/highgui/highgui.hpp\"#include \"opencv2/imgproc/imgproc.hpp\"#include \"opencv2/core/core.hpp\"#include \"opencv2/video/video.hpp\"#include &lt;iostream&gt;using namespace cv;int main(int argc, char **argv)&#123; // 从命令行参数读取图片路径 char *imageName = argv[1]; Mat image; image = imread(imageName, IMREAD_COLOR); if (argc != 2 || !image.data) &#123; std::cout &lt;&lt; \"No image data\" &lt;&lt; std::endl; return -1; &#125; Mat gray_image; cvtColor(image, gray_image, COLOR_BGR2GRAY); // 保存转换之后的灰度图片 imwrite(\"Gray_Image.jpg\", gray_image); namedWindow(imageName, WINDOW_AUTOSIZE); namedWindow(\"Gray image\", WINDOW_AUTOSIZE); imshow(imageName, image); imshow(\"Gray image\", gray_image); waitKey(0); return 0;&#125; 命令行方式编译 g++ ModifyImage.cpp pkg-config --cflags --libs opencv -o ModifyImage CMake 方式编译新建一个CMakeLists.txt 123456cmake_minimum_required(VERSION 2.8)project( ModifyImage )find_package( OpenCV REQUIRED )include_directories( $&#123;OpenCV_INCLUDE_DIRS&#125; )add_executable( ModifyImage ModifyImage.cpp )target_link_libraries( ModifyImage $&#123;OpenCV_LIBS&#125; ) 输入以下命令 cmake . make 通过以上两种方式都会生成一个可执行文件，我们可以执行它 ./ModifyImage","categories":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://shuiyujie.com/categories/OpenCV/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://shuiyujie.com/tags/ubuntu/"},{"name":"opencv","slug":"opencv","permalink":"http://shuiyujie.com/tags/opencv/"}]},{"title":"Ubuntu OpenSSH Server","slug":"Linux/Ubuntu-OpenSSH-Server","date":"2019-03-16T10:41:24.000Z","updated":"2019-05-14T13:52:24.357Z","comments":true,"path":"post/cbbc9ca0.html","link":"","permalink":"http://shuiyujie.com/post/cbbc9ca0.html","excerpt":"本文介绍如何使用OpenSSH实现计算机之间的远程控制和数据交换。你将了解到OpenSSH的一些配置以及如何在Ubuntu中修改这些配置。 Ubuntu16.04是目前Ubuntu较为稳定的版本，本文使用该版本进行说明。","text":"本文介绍如何使用OpenSSH实现计算机之间的远程控制和数据交换。你将了解到OpenSSH的一些配置以及如何在Ubuntu中修改这些配置。 Ubuntu16.04是目前Ubuntu较为稳定的版本，本文使用该版本进行说明。 介绍OpenSSH基于SSH协议，是用于实现计算机之间远程控制和数据交换的工作的工具。 传统的工具实现远程登录(telnet)和rcp等功能的方式不安全的，他们会用明文的方式交换用户密码。OpenSSH用后台进程和客户端工具来提高安全性，对远程控制和数据交换操作进行加密，比其他传统工具更加高效。 OpenSSH使用sshd持续地监听来自各个客户端程序的连接。当客户端发出连接请求，ssh根据客户端的连接类型来判断是否建立连接。比如，如果远程计算机是一个ssh客户端程序，OpenSSH将会在认证之后建立一个控制会话。如果远程用户使用scp进行连接，OpenSSH在认证之后会与客户端建立连接，并在后台初始化一个安全的文件拷贝。 OpenSSH可以使用密码、公钥和 Kerberos 等多种方式进行认证。 安装OpenSSH的安装非常简单，分别安装OpenSSH Sever和OpenSSH Client。 sudo apt install openssh-clientsudo apt install openssh-server 配置OpenSSH的配置文件是/etc/ssh/sshd_config，查看详细配置可以使用 man sshd_config 在修改配置文件之前我们应当对配置文件进行备份 sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.originalsudo chmod a-w /etc/ssh/sshd_config.original 我们可以通过修改配置文件做这些事情： 将OpenSSH监听的默认TCP端口从2222改为默认端口22，可以修改Port 2222 允许运行使用公式登录，可以使用PubkeyAuthentication yes 修改完成之后重启服务使其生效 sudo systemctl restart sshd.service SSH Keys配置了ssh key允许主机之间直接通信而不用输入密码。 首选生成ssh key，使用一下命令并一路回车 ssh-keygen -t rsa 此时会在~/.ssh文件夹在生产一个密钥文件和一个公钥文件，我们将公钥拷贝给远程的主机 ssh-copy-id username@remotehost 之后用相同的方式将远程主机的公钥拷贝给本机，就可以实现双方免密登录。 最后我们还要注意，认证用户需要对用于认证的文件有读写的权限。 chmod 600 .ssh/authorized_keys 引用OpenSSH Server","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://shuiyujie.com/tags/ubuntu/"}]},{"title":"更多搜索引擎","slug":"善用佳软/更多搜索引擎","date":"2019-03-16T06:59:31.000Z","updated":"2019-09-10T14:30:41.740Z","comments":true,"path":"post/9a276542.html","link":"","permalink":"http://shuiyujie.com/post/9a276542.html","excerpt":"善用搜索引擎，程序员提升的一大步。","text":"善用搜索引擎，程序员提升的一大步。 谷歌 全球最大搜索引擎公司。 必应 微软旗下，比较出名了，国内可访问。 Bird.so 技术问题的聚合，国内也可访问，是我谷歌替代方案。 DuckDuckGo 注重隐私安全，不记录用户数据是卖点，注重隐私的人的上好选择。 Wikipedia 维基百科就像是一本参考书，有很高的权威性。 SemanticScholar Semantic Scholar 是由微软联合创始人 Paul Allen 做的免费学术搜索引擎，其检索结果来自于期刊、学术会议资料或者是学术机构的文献。 MEZW MEZW 搜索服务可以汇集国内外网页的搜索结果，登录帐号，更可自定义的屏蔽掉不希望展示在搜索结果中的网站。","categories":[{"name":"善用佳软","slug":"善用佳软","permalink":"http://shuiyujie.com/categories/善用佳软/"}],"tags":[{"name":"网站","slug":"网站","permalink":"http://shuiyujie.com/tags/网站/"},{"name":"工具","slug":"工具","permalink":"http://shuiyujie.com/tags/工具/"}]},{"title":"SpringBoot 系列文章","slug":"Java/Spring Boot/SpringBoot 系列文章","date":"2019-02-21T03:03:47.000Z","updated":"2019-10-19T10:47:07.522Z","comments":true,"path":"post/ee9b8bc4.html","link":"","permalink":"http://shuiyujie.com/post/ee9b8bc4.html","excerpt":"","text":"SpringBoot 配置文件及其读取 SpringBoot 错误处理页 SpringBoot 模板 Thymeleaf 的使用 SpringBoot 文件上传 SpringBoot 基础拦截器 SpringBoot 整合 MyBatis SpringBoot 整合消息服务 SpringBoot 整合Redis数据库 SpringBoot 整合Restful SpringBoot 整合Shiro验证框架","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://shuiyujie.com/tags/springboot/"}]},{"title":"SpringBoot 模板 Thymeleaf 的使用","slug":"Java/Spring Boot/SpringBoot 模板 Thymeleaf 的使用","date":"2019-02-19T03:10:47.000Z","updated":"2019-10-06T07:53:01.591Z","comments":true,"path":"post/41479a53.html","link":"","permalink":"http://shuiyujie.com/post/41479a53.html","excerpt":"Java 开发行业有三种常用显示模板 FreeMarker Velocity Thymeleaf（推荐使用） 本项目是使用 Thymeleaf 模板的简单 Demo 点击查看源码","text":"Java 开发行业有三种常用显示模板 FreeMarker Velocity Thymeleaf（推荐使用） 本项目是使用 Thymeleaf 模板的简单 Demo 点击查看源码 SpringBoot 模板 Thymeleaf 的使用pom.xml 中添加依赖 12345&lt;!-- 配置使用 thymeleaf 模板--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 控制层进行信息显示 12345678910111213@Controllerpublic class MessageController extends AbstractBaseController &#123; @RequestMapping(value = \"/show\", method = RequestMethod.GET) public String show(String mid, Model model) &#123; // request属性传递包装 model.addAttribute(\"url\", \"www.shuiyujie.com\"); // request属性传递包装 model.addAttribute(\"mid\", mid); // 此处只返回一个路径， 该路径没有设置后缀，后缀默认是*.html return \"message/message_show\"; &#125;&#125; 由于我们使用的是@Controller注解，所以此时return &quot;message/message_show&quot;将进行一次路由，路由到的文件就是 Thymeleaf 模板文件。 Thymeleaf 模板文件位置配置文件位置的配置很重要，要按照规范进行配置。 首先我们要建立一个Resources类型目录，Resources目录就是源代码目录，这个是可以通过 IDE 进行设置。 我是这样设置的： src/main/view 目录下存放页面 src/main/view/static 目录下存放 js,css,images 等文件 src/main/view/templates 目录下存放 html 页面 注: static 静态目录下的文件可以直接访问，而不需要通过控制器进行路由。 http://localhost:8080/show通过路由访问 message_show.html 页面 http://localhost:8080/message_index.html直接访问 message_index.html 页面","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://shuiyujie.com/tags/springboot/"}]},{"title":"SpringBoot 错误处理页","slug":"Java/Spring Boot/SpringBoot 错误处理页","date":"2019-02-19T03:03:47.000Z","updated":"2019-10-06T07:53:05.584Z","comments":true,"path":"post/3c98ab20.html","link":"","permalink":"http://shuiyujie.com/post/3c98ab20.html","excerpt":"SpringBoot 错误处理有三种情况 数据验证错误 错误页指派 全局异常处理 比如说在提交表单信息的时候，有些信息没填，有些信息有规范的格式，如果不符合要求就是数据校验错误。 我们需要对这些数据的格式进行校验，不符合要求不能接受且要提示错误信息。 点击查看源码","text":"SpringBoot 错误处理有三种情况 数据验证错误 错误页指派 全局异常处理 比如说在提交表单信息的时候，有些信息没填，有些信息有规范的格式，如果不符合要求就是数据校验错误。 我们需要对这些数据的格式进行校验，不符合要求不能接受且要提示错误信息。 点击查看源码 springboot 错误处理，数据校验错误SpringBoot 有数据校验的默认支持，该支持由 Hibernate 开发框架提供。 错误信息统一配置在ValidationMessages.properties文件中。 这种方式每个 VO 类在ValidationMessages.properties文件中配置对应的错误信息，并且在 VO 类的属性上添加上错误注解，过程繁琐。不推荐使用，更推荐使用反射和拦截器的方式处理错误信息。 但是我们还是要会使用这种方式处理数据验证错误，写了一个小 Demo，再接下来简单记录一下使用过程。 添加错误信息配置文件src/main/resources目录中建立ValidationMessages.properties文件。文件中配置错误信息 12345678member.mid.notnull.error=邮箱不允许为空member.mid.email.error=邮箱格式错误member.mid.length.error=邮箱长度错误member.age.notnull.error=年龄不允许为空member.age.digits.error=年龄格式错误member.salary.notnull.error=工资不允许为空member.salary.digits.error=工资格式错误member.birthday.notnull.error=生日不允许为空 VO 类添加注解12345678910111213141516SuppressWarnings(\"serial\")public class Member implements Serializable &#123; @NotNull(message=\"&#123;member.mid.notnull.error&#125;\") @Email(message=\"&#123;member.mid.email.error&#125;\") @Length(min=6,message=\"&#123;member.mid.length.error&#125;\") private String mid ; @NotNull(message=\"&#123;member.age.notnull.error&#125;\") @Digits(integer=3,fraction=0,message=\"&#123;member.age.digits.error&#125;\") private Integer age ; @NotNull(message=\"&#123;member.salary.notnull.error&#125;\") @Digits(integer=20,fraction=2,message=\"&#123;member.salary.digits.error&#125;\") private Double salary ; @NotNull(message=\"&#123;member.birthday.notnull.error&#125;\") private Date birthday ;&#125; 控制器配置校验1234567891011121314@RequestMapping(value = \"/add\", method = RequestMethod.POST)@ResponseBodypublic Object add(@Valid Member vo, BindingResult result) &#123; if (result.hasErrors()) &#123; Iterator&lt;ObjectError&gt; iterator = result.getAllErrors().iterator(); while (iterator.hasNext()) &#123; ObjectError error = iterator.next(); System.out.println(\"【错误信息】code = \" + error.getCode() + \"，message = \" + error.getDefaultMessage()); &#125; return result.getAllErrors(); &#125; else &#123; return vo; &#125;&#125; springboot 错误处理，配置错误页面错误页面配置在src/main/view/static目录下，比如叫 error-404.html SringBoot 1.x 这样处理12345678910111213141516171819202122232425262728293031323334package com.shuiyujie.config;import org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer;import org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer;import org.springframework.boot.web.servlet.ErrorPage;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.http.HttpStatus;/** * @author shui * @create 2019-02-12 **/@Configurationpublic class ErrorPageConfig &#123; @Bean public EmbeddedServletContainerCustomizer containerCustomizer() &#123; return new EmbeddedServletContainerCustomizer() &#123; @Override public void customize( ConfigurableEmbeddedServletContainer container) &#123; ErrorPage errorPage400 = new ErrorPage(HttpStatus.BAD_REQUEST, \"/error-400.html\"); ErrorPage errorPage404 = new ErrorPage(HttpStatus.NOT_FOUND, \"/error-404.html\"); ErrorPage errorPage500 = new ErrorPage( HttpStatus.INTERNAL_SERVER_ERROR, \"/error-500.html\"); container.addErrorPages(errorPage400, errorPage404, errorPage500); &#125; &#125;; &#125;&#125; SringBoot 2.x 这样处理在SpringBoot2中没有EmbeddedServletContainerCustomizer这个类了，要这样处理 1234567891011121314151617181920212223ackage com.shuiyujie.config;import org.springframework.boot.web.server.ErrorPage;import org.springframework.boot.web.server.ErrorPageRegistrar;import org.springframework.boot.web.server.ErrorPageRegistry;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;/** * @author shui * @create 2019-02-12 **/@Componentpublic class ErrorPageConfig implements ErrorPageRegistrar &#123; @Override public void registerErrorPages(ErrorPageRegistry registry) &#123; ErrorPage[] errorPages = new ErrorPage[3]; errorPages[0] = new ErrorPage(HttpStatus.NOT_FOUND, \"/error-400.html\"); errorPages[1] = new ErrorPage(HttpStatus.NOT_FOUND, \"/error-404.html\"); errorPages[2] = new ErrorPage(HttpStatus.INTERNAL_SERVER_ERROR, \"/error-500.html\"); registry.addErrorPages(errorPages); &#125;&#125; SpringBoot 全局错误处理之前设置了错误页面，发生错误将会跳转到错误页面。 1234567891011/** * 演示 500 错误 * * @return */@RequestMapping(value=\"/get\")@ResponseBodypublic String get() &#123; System.out.println(\"除法计算：\" + (10 / 0)); return \"hello world\" ;&#125; 比如这样一个 10/0 的错误控制器跳转到 500 页面，这样就看不到详细的报错信息了。我们想看到类似控制台里面更加详细的报错信息。 定义一个错误信息处理的页面新建一个错误界面src/main/view/templates/error.html 123456789101112!DOCTYPE HTML&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;title&gt;SpringBoot模版渲染&lt;/title&gt; &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"/images/mldn.ico\"/&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html;charset=UTF-8\"/&gt;&lt;/head&gt;&lt;body&gt; &lt;p th:text=\"$&#123;url&#125;\"/&gt; &lt;p th:text=\"$&#123;exception.message&#125;\"/&gt;&lt;/body&gt;&lt;/html&gt; 定义全局异常处理类123456789101112131415161718192021222324252627282930313233343536373839404142434445@RestControllerAdvicepublic class GlobalExceptionHandler &#123; // 定义错误显示页，error.html public static final String DEFAULT_ERROR_VIEW = \"error\"; // 所有的异常都是Exception子类 @ExceptionHandler(Exception.class) public Object defaultErrorHandler(HttpServletRequest request, Exception e) &#123; class ErrorInfo &#123; private Integer code; private String message; private String url; public Integer getCode() &#123; return code; &#125; public void setCode(Integer code) &#123; this.code = code; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; &#125; ErrorInfo info = new ErrorInfo(); // 标记一个错误信息类型 info.setCode(100); info.setMessage(e.getMessage()); info.setUrl(request.getRequestURL().toString()); return info; &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://shuiyujie.com/tags/springboot/"}]},{"title":"SpringBoot 配置文件及其读取","slug":"Java/Spring Boot/SpringBoot 配置文件及其读取","date":"2019-02-19T03:00:47.000Z","updated":"2019-10-06T07:52:57.580Z","comments":true,"path":"post/f0ba9e07.html","link":"","permalink":"http://shuiyujie.com/post/f0ba9e07.html","excerpt":"在配置文件中统一管理配置信息，使用配置文件 SpringBoot 有规范的方式。 src/main/resources 的 classpath 路径之中，创建application.properties配置文件。 配置文件也可以用 YAML 语言来写，创建application.yml配置文件 两个文件同时存在都会起作用，但当配置项冲突时，优先使用application.properties文件 点击获取项目源码","text":"在配置文件中统一管理配置信息，使用配置文件 SpringBoot 有规范的方式。 src/main/resources 的 classpath 路径之中，创建application.properties配置文件。 配置文件也可以用 YAML 语言来写，创建application.yml配置文件 两个文件同时存在都会起作用，但当配置项冲突时，优先使用application.properties文件 点击获取项目源码 资源文件的设置 资源文件统一放在src/main/resources/i18n目录中 建立 Messages.properties 12welcome.url=www.shuiyujie.comwelcome.msg=shuiyujie 建立 Pages.properties 12member.add.page=/pages/back/admin/member/member_add.jspmember.add.action=/pages/back/admin/member/member_add.action 配置文件 application.yml 中指定资源文件目录 12345spring: # 表示该配置直接为Spring容器负责处理 messages: # 表示进行资源配置 basename: i18n/Messages,i18n/Pages # 资源文件的名称server: port: 80 # 此处设置的服务的访问端口配置 资源文件的读取经过以上配置就会自动生成一个MessageSource资源文件对象，我们只要注入这个对象就能使用资源文件中配置的属性了。 我会建立一个控制器的父类在其中注入MessageSource，然后子类继承父类就能很方便地读取资源文件了 1234567public abstract class AbstractBaseController &#123; @Resource private MessageSource messageSource; // 自动注入此资源对象 public String getMessage(String key, String... args) &#123; return this.messageSource.getMessage(key, args, Locale.getDefault()); &#125;&#125; 12345678@RestControllerpublic class MessageController extends AbstractBaseController &#123; @RequestMapping(value = \"/echo\", method = RequestMethod.GET) public String echo(String mid) &#123; System.out.println(\"【*** 访问地址 ***】\" + super.getMessage(\"member.add.action\")); return super.getMessage(\"welcome.msg\", mid); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"http://shuiyujie.com/tags/springboot/"}]},{"title":"并发编程的优缺点","slug":"Java/Java 并发编程/【Java并发编程】并发编程的优缺点","date":"2019-01-26T03:03:47.000Z","updated":"2019-09-11T14:59:18.465Z","comments":true,"path":"post/579f9aa4.html","link":"","permalink":"http://shuiyujie.com/post/579f9aa4.html","excerpt":"Java 程序从 main() 函数开始开始执行，就是启动一个名称为 main 的线程。在程序顺序执行的过程中，看似没有其他线程参与，实际上有许多线程和 main 线程一起执行着。 Java 天生就是多线程的，为什么 Java 会设计成多线程的呢？并发编程是不是有它优点？然而事物都是有两面性的，并发编程的缺点是什么？应该怎么避免？","text":"Java 程序从 main() 函数开始开始执行，就是启动一个名称为 main 的线程。在程序顺序执行的过程中，看似没有其他线程参与，实际上有许多线程和 main 线程一起执行着。 Java 天生就是多线程的，为什么 Java 会设计成多线程的呢？并发编程是不是有它优点？然而事物都是有两面性的，并发编程的缺点是什么？应该怎么避免？ 1. 为什么要使用并发？（优点）1.1 更好地利用计算机处理器现在 CPU 都是多核的，而且核心越来越多。如果一个程序是单线程的，无论如何只能使用一个核心，CPU 的核心再多也无法提升机器的性能。 如果一个程序是多线程的，那么它就可以利用 CPU 的多个核心进行运算，CPU 的核心变得越多，运行速度也能越快。 1.2 更快的响应速度多线程可以让一系列的操作并发地执行，这样就可以提高程序响应的速度。比如用户下单，它包括插入订单数据、生成订单快照、发送邮件通知卖家和记录货品销售数量等。 等这些业务操作做完，用户要等 1 秒。把生成订单快照、发送邮件这样的操作给其他线程处理，用户可能就等 0.5 s。 1.3 更简单地进行开发Java 设计之初就考虑了并发编程，提供了一致的编程模型，使用 Java 的程序员可以把更多精力放在业务上，而不用考虑怎么使用多线程。 2. 并发编程的缺点及解决方案2.1 增加上下文切换，影响执行速度我们在阅读英文书籍的时候碰到了一个不认识的单词，这时候就去字典里查一下这个单词是什么意思，查完再回过头继续往下看书。这个一来一回的过程就相当于是上下文切换的过程。 线程之间上下文切换的过程是这样的。CPU 个每个线程分配一个时间片，一个时间片就表示一段很短的时间。CPU 执行线程时，时间片用完了就会换一个线程执行，就这样在各个线程之间切换。 查完字典再回过头继续阅读，这样会影响我们的阅读效率。上下文切换自然也会影响执行速度。 减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。 无锁并发编程。多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash算法取模分段，不同的线程处理不同段的数据。 CAS算法。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。 使用最少线程。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。 协程：在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 2.2 出现死锁，导致系统不可用1234567891011121314151617181920212223242526272829303132333435public class DeadLockDemo &#123; privat static String A = \"A\"; private static String B = \"B\"; public static void main(String[] args) &#123; new DeadLockDemo().deadLock(); &#125; private void deadLock() &#123; Thread t1 = new Thread(new Runnable() &#123; @Override publicvoid run() &#123; synchronized (A) &#123; try &#123; Thread.currentThread().sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (B) &#123; System.out.println(\"1\"); &#125; &#125; &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; @Override publicvoid run() &#123; synchronized (B) &#123; synchronized (A) &#123; System.out.println(\"2\"); &#125; &#125; &#125; &#125;); t1.start(); t2.start(); &#125;&#125; 比如t1拿到锁之后，因为一些异常情况没有释放锁（死循环）。又或者是t1拿到一个数据库锁，释放锁的时候抛出了异常，没释放掉。 一旦出现死锁，系统功能就不可用了，需要花许多精力去排查错误。 避免死锁的几个常见方法有： 避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用lock.tryLock（timeout）来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 2.3 资源限制影响性能资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。 比如带宽受限的情况下及时启动多个线程进行下载，下载速度永远无法超过带宽。 在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行。 但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。 对于硬件资源的限制，可以采用集群的方式，一台机器的资源有限那就用多台，比如部署 hadoop 集群。 对于软件资源限制可以采用连接池，提高资源的复用率。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"}]},{"title":"在ubuntu上安装maven","slug":"Java/在ubuntu上安装maven","date":"2018-09-19T14:04:24.000Z","updated":"2019-09-19T14:18:52.942Z","comments":true,"path":"post/9ec2f732.html","link":"","permalink":"http://shuiyujie.com/post/9ec2f732.html","excerpt":"本文介绍如何在 ubuntu16.04 中安装 maven。","text":"本文介绍如何在 ubuntu16.04 中安装 maven。 在 Maven 官网下载安装包，然后解压。 解压之后拷贝到到 /opt 目录下，并建立软链接： 12mv apache-maven-3.3.9 /optln -s apache-maven-3.3.9 maven 配置环境变量 1234567vim ~/.bashrcexport M2_HOME=/opt/mavenexport M2=$M2_HOME/binexport PATH=$M2:$PATHsource ~/.bashrc 测试安装是否成功 1mvn -v 修改 maven 的源 12cp /opt/maven/conf/settings.xml ~/.m2vim ~/.m2/settings.xml 在 &lt;mirrors&gt;中进行如下配置 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://shuiyujie.com/tags/maven/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://shuiyujie.com/tags/ubuntu/"}]},{"title":"装饰者模式","slug":"cs/设计模式/装饰者模式","date":"2018-09-19T00:42:21.000Z","updated":"2019-10-06T11:04:30.777Z","comments":true,"path":"post/a708a60d.html","link":"","permalink":"http://shuiyujie.com/post/a708a60d.html","excerpt":"定义与类型 定义：在不改变原有对象的基础上，将功能附加到对象上 提供了比继承更有弹性的替代方案（扩展原有对象功能） 类型：结构型 适用场景 扩展一个类的功能或给一个类添加附加职责 动态的给一个对象添加功能，这些功能可以再动态地撤销 优点 继承的有力补充，比继承灵活，不改变原有对象的情况下给一个对象扩展功能 通过使用不同装饰类以及这些装饰类的排列组合，可以实现不同效果 符合开闭原则 缺点 会出现更多的代码，更多的类，增加程序复杂性 动态装饰时，多层装饰时会更负责 相关设计模式 装饰者模式和代理模式 装饰者模式和适配器模式","text":"定义与类型 定义：在不改变原有对象的基础上，将功能附加到对象上 提供了比继承更有弹性的替代方案（扩展原有对象功能） 类型：结构型 适用场景 扩展一个类的功能或给一个类添加附加职责 动态的给一个对象添加功能，这些功能可以再动态地撤销 优点 继承的有力补充，比继承灵活，不改变原有对象的情况下给一个对象扩展功能 通过使用不同装饰类以及这些装饰类的排列组合，可以实现不同效果 符合开闭原则 缺点 会出现更多的代码，更多的类，增加程序复杂性 动态装饰时，多层装饰时会更负责 相关设计模式 装饰者模式和代理模式 装饰者模式和适配器模式 现在有一个场景，煎饼馃子的老板卖煎饼，煎饼可以加蛋或者香肠。这个类怎么实现呢？ 一种方式就是使用继承：我有一个煎饼馃子的类，要加蛋的话添加一个加蛋的类，要加香肠的话添加一个香肠的类。类图如下所示： 通过继承的方式来扩展有什么弊端呢？第一个缺点是容易导致「类爆炸」，就是我加什么都得创建一个新的类；第二个缺点是我想要加两个蛋怎么加，也得重新创建一个类。 那怎么办呢？可以用装饰者模式。 ABattercake 是一个抽象类，其中包括计算价格和添加描述的方法； AbstractDecorator 也是一个抽象类，它就是一个抽象的装饰者，我们可以用一个构造器注入一个 ABattercake； 12345678910111213141516171819public abstract class AbstractDecorator extends ABattercake &#123; private ABattercake aBattercake; public AbstractDecorator(ABattercake aBattercake) &#123; this.aBattercake = aBattercake; &#125; protected abstract void doSomething(); @Override protected String getDesc() &#123; return this.aBattercake.getDesc(); &#125; @Override protected int cost() &#123; return this.aBattercake.cost(); &#125;&#125; Battercake 和 EggDecorator 是 AbstractDecorator 具体实现 1234567891011121314151617181920public class EggDecorator extends AbstractDecorator &#123; public EggDecorator(ABattercake aBattercake) &#123; super(aBattercake); &#125; @Override protected void doSomething() &#123; &#125; @Override protected String getDesc() &#123; return super.getDesc()+\" 加一个鸡蛋\"; &#125; @Override protected int cost() &#123; return super.cost()+1; &#125;&#125; 这样我想要几个蛋就加创建几个蛋对象，想要几个香肠创建几个香肠对象。 1234567891011public class Test &#123; public static void main(String[] args) &#123; ABattercake aBattercake; aBattercake = new Battercake(); aBattercake = new EggDecorator(aBattercake); aBattercake = new EggDecorator(aBattercake); aBattercake = new SausageDecorator(aBattercake); System.out.println(aBattercake.getDesc()+\" 销售价格:\"+aBattercake.cost()); &#125;&#125; 装饰者模式同样会产生「类爆炸」的问题，但是比继承的方式好一点。同时装饰者模式需要创建更多的对象，程序的复杂性也提高了。","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"设计模式","slug":"computer-science/设计模式","permalink":"http://shuiyujie.com/categories/computer-science/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://shuiyujie.com/tags/设计模式/"}]},{"title":"UML","slug":"cs/设计模式/UML","date":"2018-09-18T15:42:44.000Z","updated":"2019-09-22T07:43:28.694Z","comments":true,"path":"post/af8b09ba.html","link":"","permalink":"http://shuiyujie.com/post/af8b09ba.html","excerpt":"临时笔记，待补充。","text":"临时笔记，待补充。 UML 的定义 统一建模语言（Unified Modeling Language） 非专利的第三代建模和规约语言 UML 特点 UML 是一种开发的方法 用于说明、可视化、构建和编写一个正在开发的面向对象爱你过的、软件密集系统的制品的开放方法 UML 展现了一系列最佳工程实践。这些最佳实践在对大规模，复杂系统进行建模方面，特别是在软件架构层次已经被验证有效 UML2.2分类 结构式图形：强调的是系统式的建模 静态图（类图、对象图、包图） 实现图（组件图、部署图） 剖面图 复合结构图 行为式图形：强调系统模型中触发的事件 活动图 状态图 用例图 交互式图形：属于行为式图形子集合，强调系统模型中资料流程 通信图 交互概述图（UML2.0） 时序图（UML2.0） 时间图（UML2.0）","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"设计模式","slug":"computer-science/设计模式","permalink":"http://shuiyujie.com/categories/computer-science/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://shuiyujie.com/tags/设计模式/"}]},{"title":"工厂相关设计模式","slug":"cs/设计模式/工厂相关设计模式","date":"2018-09-18T15:42:36.000Z","updated":"2019-09-22T07:43:28.693Z","comments":true,"path":"post/6d867605.html","link":"","permalink":"http://shuiyujie.com/post/6d867605.html","excerpt":"","text":"简单工厂模式 定义：由一个工厂对象决定创建出哪一种产品类的实例 类型：创建型，但不属于 GOF 23 种类设计模式 适用场景： 工厂类负责创建的对象比较少 客户端(应用层)只知道传入工厂类的参数，对于创建对象对象(逻辑)不关心 优点：只需要传入一个正确的参数，就可以获取你所需要的对象，无需知道其创建细节 缺点：工厂类的职责相对过重，增加新的产品需要修改工厂类的判断逻辑，违背开闭原则 简单工厂模式中，应用层 Test 通过一个工厂类 VideoFactory 来获取一个对象。从应用层来看，这样做的优点是不需要单独创建一个 video 对象，而是采用工厂统一来创建对象，应用层只需要传入参数就可以了。 入参可能是字符串等，通过逻辑判断来返回相应的实例对象；入参也有可能是一个类，通过反射来获取对象。 工厂方法模式抽象工厂模式","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"设计模式","slug":"computer-science/设计模式","permalink":"http://shuiyujie.com/categories/computer-science/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://shuiyujie.com/tags/设计模式/"}]},{"title":"软件设计的7大原则","slug":"cs/设计模式/软件设计的7大原则","date":"2018-09-18T15:42:34.000Z","updated":"2019-09-21T13:49:34.332Z","comments":true,"path":"post/9c334df9.html","link":"","permalink":"http://shuiyujie.com/post/9c334df9.html","excerpt":"开闭原则、依赖倒置原则、单一职责原则、接口隔离原则、迪米特原则、里氏替换原则、合成复用原则。","text":"开闭原则、依赖倒置原则、单一职责原则、接口隔离原则、迪米特原则、里氏替换原则、合成复用原则。 开闭原则 what：一个软件实体如类、模块和函数应该对扩展开发，对修改关闭。 why：提高软件系统的可复用性及可维护性 how: 版本更新尽量不更改源代码，但是可以添加新功能； example: 弹性工作时间，每天工作8小时不变，早点来早点走，晚点来晚点走 面向抽象编程，继承、多态机制 左图表示我们有一个订单接口，现在要在其基础上扩展一个功能，能够实现打折的功能。如何才能实现打折功能呢？ 在 IOrder 接口中添加一个 getDiscountPrice()，并在其实现类中实现这个方法。这样可行么？如果这样做就需要在所有实现这个方法的类中，添加 getDiscountPrice()。 修改 getPrice() 的实现，这样原价去哪里获取呢？也不是很好 我们可以写一个类 ShoeDiscoutOrder 继承 ShoeOrder 然后 @Override getPrice() 来实现打折功能，而原价则使用 getOriginPrice()，并在其中使用 super.getPrice() 调用父类方法。 这是开闭原则的简单应用：扩展是开启的，但是对接口和基类的修改是关闭的。 依赖倒置原则 what: 高层模块不应该依赖低层模块，二者都应该依赖其抽象 why: 减少雷剑的耦合性，提高系统稳定性，提高代码可读性和可维护性，降低修改程序所造成的风险 how：抽象不应该依赖细节；细节应该依赖抽象。针对接口编程，不要针对实现编程。 example：对比扩展类中的方法编程，以及面向接口编程的区别。 现在有一个 course 类，类中有上各种课程的方法，我在应用层调用各种上课的方法，就需要在 course 中新增相应的方法。此时就造成了一种局面：高层模块依赖于低层模块。代码是这样的： 123456789101112131415161718192021// 应用层（高层）调用低层的方法public class Test &#123; public static void main(String[] args) &#123; Geely geely = new Geely(); geely.studyJavaCourse(); geely.studyFECourse(); &#125;&#125;// 低层想要扩展就要加方法public class Geely &#123; public void studyJava()&#123; // 业务代码 &#125; public void studyPython()&#123; // 业务代码 &#125; // 更多课程都要添加新的方法&#125; 我们希望高层模块不依赖低层模块，并且让它们依赖其抽象。具体来说是什么意思呢？就是说我有一个接口实现了 stydyCouse() 这一个方法，具体什么课，怎么学都由该接口的实现类负责。我从高层传入对象，就可以调用相应的类。 12345678910111213141516171819202122232425// 高层调用低层，只要注入具体的实现类public class Test &#123; public static void main(String[] args) &#123; Geely geely = new Geely(); geely.setiCourse(new JavaCourse()); geely.studyImoocCourse(); geely.setiCourse(new FECourse()); geely.studyImoocCourse(); &#125;&#125;// 低层调用方法无需变动public class Geely &#123; private ICourse iCourse; public void setiCourse(ICourse iCourse) &#123; this.iCourse = iCourse; &#125; public void studyImoocCourse()&#123; iCourse.studyCourse(); &#125;&#125; 依赖倒置的核心是面向接口编程。 单一职责原则 what: 不要存在多余一个导致类变更的原则 why: 一个类有多个职责会导致它变更，修改某一个职责可能导致其他职责出错 how: 一个类/接口/方法只负责一个职责 example：类、接口和方法级别的单一职责实现。 类级别现在有一个 bird 类，有的鸟靠飞，有的鸟靠走。我们可能会用一个逻辑判断来实现。如果说遵循单一职责原则，就应该将其拆成两个类 FlyBird 和 WalkBird ，然后由应用层来判断是哪种鸟。 123456789public class Bird &#123; public void mainMoveMode(String birdName)&#123; if(\"鸵鸟\".equals(birdName))&#123; System.out.println(birdName+\"用脚走\"); &#125;else&#123; System.out.println(birdName+\"用翅膀飞\"); &#125; &#125;&#125; 接口级别比如说有个 ICourse 接口，接口中有若干方法，一些方法负责获取课程信息，一些方法负责管理可能。比如说，有个方法是退订这门课，那就不能获取课程信息了对不对。总之，这样一个接口现在负责了两种职责，我们就应该将其拆成两组接口。 123456789public interface ICourse &#123; // 职责一：负责获取课程信息 String getCourseName(); byte[] getCourseVideo(); // 职责二：负责管理课程 void studyCourse(); void refundCourse();&#125; 方法级别方法中做逻辑判断，负责多个任务，实际上是可以拆分开的。 1234567891011121314151617181920private void updateUsername(String userName)&#123; userName = \"geely\";&#125;private void updateUserAddress(String address)&#123; address = \"beijing\";&#125;// 多个职责，条件判断，拆！private void updateUserInfo(String userName, String address,boolean bool)&#123; if(bool)&#123; //todo something1 &#125;else&#123; //todo something2 &#125; userName = \"geely\"; address = \"beijing\";&#125; 接口隔离原则 what: 用多个专门的接口，而不使用单一的总接口，客户端不应该依赖它不需要的接口 why: 符合高内聚低耦合的设计思想，从而使得类具有很好的可读性、可扩展性和可维护性 how： 一类对一个类的依赖应该建立在最小的接口上 建立单一接口，不要建立庞大臃肿的接口 尽量细化接口，接口中的方法尽量少 注意适度原则，一定要适度 example: 一个关于动物的接口承载太多的方法，将其拆分的例子 同样是动物但是这个动物能做的事儿，其他动物可能不能做。如果动物都实现同样的接口，那么有些方法的实现就要空着了。所以要拆分。 12345public interface IAnimalAction &#123; void eat(); void fly(); void swim();&#125; 拆分前，拆分后如下所示： 接口隔离原则看着简单，但是把握好接口隔离的粒度还是需要仔细考量的。 迪米特原则 what: 一个对象应该对其他对象保持最少的了解，又叫最少知道原则 why: 降低类之间的耦合 how: 尽量降低类和类之间的耦合 example： Boss 类只需要和 TeamLeader 打交道，而不需要和 course 打交道。 修改之后如下所示： 迪米特原则的关键就是梳理出这个类应该和哪些类打交道，不应该和哪些类打交道，做到尽可能合理。 里氏替换原则合成复用原则","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"设计模式","slug":"computer-science/设计模式","permalink":"http://shuiyujie.com/categories/computer-science/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://shuiyujie.com/tags/设计模式/"}]},{"title":"Linux三剑客——Git","slug":"Linux/Linux三剑客——Git","date":"2018-08-31T04:50:15.000Z","updated":"2019-08-31T04:53:51.955Z","comments":true,"path":"post/fd2c899.html","link":"","permalink":"http://shuiyujie.com/post/fd2c899.html","excerpt":"","text":"","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"},{"name":"git","slug":"git","permalink":"http://shuiyujie.com/tags/git/"}]},{"title":"Http状态码","slug":"cs/计算机网络/Http状态码","date":"2018-08-24T05:37:28.000Z","updated":"2019-10-03T14:06:55.817Z","comments":true,"path":"post/140645a5.html","link":"","permalink":"http://shuiyujie.com/post/140645a5.html","excerpt":"类别 原因短语 1xx Informational（信息性状态码） 接收的请求正在处理 2xx Success（成功状态码） 请求正常处理完毕 3xx Redirection（重定向状态码） 需要进行附加操作以完成请求 4xx Client Error（客户端错误状态码） 服务器无法处理请求 5xx Server Error（服务器错误状态码） 服务器处理请求出错","text":"类别 原因短语 1xx Informational（信息性状态码） 接收的请求正在处理 2xx Success（成功状态码） 请求正常处理完毕 3xx Redirection（重定向状态码） 需要进行附加操作以完成请求 4xx Client Error（客户端错误状态码） 服务器无法处理请求 5xx Server Error（服务器错误状态码） 服务器处理请求出错 1XX 信息 100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2XX 成功 200 OK 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 3XX 重定向 301 Moved Permanently ：永久性重定向； 302 Found ：临时性重定向； 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4XX 客户端错误 400 Bad Request ：请求报文中存在语法错误。 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。 403 Forbidden ：请求被拒绝。 404 Not Found 5XX 服务器错误 500 Internal Server Error ：服务器正在执行请求时发生错误； 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"http","slug":"http","permalink":"http://shuiyujie.com/tags/http/"}]},{"title":"TCP协议和UDP协议","slug":"cs/计算机网络/TCP协议和UDP协议","date":"2018-08-24T04:48:34.000Z","updated":"2019-10-03T13:32:54.891Z","comments":true,"path":"post/57bfe272.html","link":"","permalink":"http://shuiyujie.com/post/57bfe272.html","excerpt":"传输层的作用是分段及封装应用层送来的数据，提供端到端的传输服务，在发送主机与接收主机之间构建逻辑通信。传输层包括两个协议 TCP 协议、UDP 协议。","text":"传输层的作用是分段及封装应用层送来的数据，提供端到端的传输服务，在发送主机与接收主机之间构建逻辑通信。传输层包括两个协议 TCP 协议、UDP 协议。 传输层协议分段、封装、端到端分段的主要目的就是提高传输效率。应用层到传输层的数据是一个个字节，每一个字节都会有一个编号，这个标号就叫做字节号。如果传输层接收到一个个字节，然后再一个个传输到网络层，这样的效率没有将多个字节组合起来效率高。这里可以想象我们在运西瓜，一个个送西瓜没有把西瓜装成一车运送的效率高。所以传输层会将字节组成和报文段，报文段也有编号，在这里这些编号叫做序号。 封装即表示加上相应的头部信息，将数据封装起来。 如上图所示，我们在应用层有两个应用进程 AP1 和 AP2。假设它们是 Telnet 和 Http 服务，那么它们的端口就是 23 端口和 80 端口。与之对应的 AP3 和 AP4 同样也使用 23 端口和 80 端口。 此时，传输层就会通过端口建立端到端的连接，使对应的应用程序进行通信。这里需要注意的是，此处的端到端连接，并不是指在传输层直接建立链路，而是指传输层通过端口建立了逻辑上的连接。 总结：由应用程序产生应用进程，应用进程产生进程端口号，由端口号提供相关的服务。 两台主机进程间通信条件 本地主机（IP地址定义） 本地进程（端口定义） 远程主机（IP地址定义） 远程进程（端口定义） 端口范围 熟知端口(著名端口)：0-1023，由ICANN指派和控制 注册端口：1024-49151,IANA不指派也不控制，但须注册 动态端口(短暂端口)：49152-65535，IANA不指派也不控制，无须注册 传输层的分用和复用 传输层协议和网络层协议的主要区别 两台主机之间要进行通信，在应用层的应用程序会启动应用进程，应用进程会开放相应的端口。两台主机通过因特网进行互联，其中网络层 IP 协议的作用是提高主机之间的逻辑通信，而传输层的 TCP 协议和 UDP 协议则提供进程之间的逻辑通信。 TCP 协议传输控制协议 TCP 简介： 面向连接的、可靠的、基于字节流的传输层通信协议 将应用层的数据流分割成报文段并发送给目标节点的 TCP 层 数据包都有序号，对方收到则发送 ACK 确认，未收到则重传 使用校验和来检验数据在传输过程中是否有误 核心问题 TCP 包头很复杂，但是主要关注五个问题，顺序问题，丢包问题，连接维护，流量控制，拥塞控制； 连接的建立是经过三次握手，断开的时候四次挥手，一定要掌握的我画的那个状态图。 TCP 协议基本概念 套接字地址 TCP 使用“连接”(而不仅仅是“端口”)作为最基本的抽象，同时将 TCP 连接的端点称为插口(socket)，或套接字、套接口。 插口和端口、IP 地址的关系是：ip地址+端口=套接字 在传出层向网络层发送数据时要以分组为单位，而不是以字节流来发送的。TCP 协议把若干字节构成一个分组，我们可以把这样的分组称为报文段（Segment），这种报文段并不一定都一样长，可以几个字节，也可以是几千个字节。 缓存：数据流向的每一个方向上都有两种缓存，发送缓存和接收缓存。 字节号：应用层向传输层发送的是一个个字节，在传输层会对这些字节进行编号，对字节的编号被称为字节号。字节号的范围为 0~$$(2^{32}-1)$$，它不是从 0 开始进行编号的，而是随机从某个编号开始顺序往下进行编号。 序号：传输层向网络层发送的是一个个报文段，一个或者多个字节组合起来形成报文段，给报文段的编号就叫做序号。每个报文段的序号就是这个报文段中第一个字节数据的字节号。 TCP 协议报文段格式详解 源端口和目的端口字段——各占 2 字节。端口是传输层与应用层的服务接口。传输层的复用和分用功能都要通过端口才能实现。 序号字段——占 4 字节。TCP 连接中传送的数据流中的每一个字节都编上一个序 号。序号字段的值则指的是本报文段所发送的数据的第一个字节的序号。 确认号字段——占 4 字节，是期望收到对方的下一个报文段的数据的第一个字节的序号。 首部长度——占 4 bit，它指出 TCP 首部共有多少个 4 字节，首部长度可以在 20~60 字节之间 。因此，这个字段值可以在5（5X4=20）至15（15X5=60）之间。 保留字段——占 6 bit，保留为今后使用，但目前应置为 0。 紧急比特 URG —— 当 URG=1 时，表明紧急指针字段有效。它告诉系统此报 文段中有紧急数据，应尽快传送(相当于高优先级的数据)。 确认比特 ACK —— 只有当ACK=1 时确认号字段才有效。当 ACK=0时，确认号无效。 推送比特 PSH —— 接收 TCP 收到 PSH=1 的报文段，就尽快地交付给接收应用进程，而不再等到整个缓存都填满了后再向上交付。 复位比特 RST —— 当 RST=1 时，表明 TCP 连接中出现严重差错（如由于主机崩溃或其他原因），必须释放连接，然后再重新建立运输连接。 同步比特 SYN —— 同步比特 SYN=1，就表示这是一个连接请求或连接接受报文。 终止比特 FIN —— 用来释放一个连接。当FIN=1 时，表明此报文段的发 送端的数据已发送完毕，并要求释放运输连接。 窗口字段 —— 占 2 字节。窗口字段用来控制对方发送的数据量，单位为字节。TCP 连接的一端根据设置的缓存空间大小确定自己的接收窗口大小，然后通知对方以确定对方的发送窗口的上限。 检验和 —— 占 2 字节。检验和字段检验的范围包括首部和数据这两部分。在计 算检验和时，要在 TCP 报文段的前面加上 12 字节的伪首部。 紧急指针字段 —— 占 16 bit。紧急指针指出在本报文段中的紧急数据的最后一个 字节的序号。 选项字段 —— 长度可变 。 TCP 只规定了一种选项 ， 即 最大报文段长度 MSS (Maximum Segment Size)。MSS 告诉对方 TCP：“我的缓存所能接收的报文段 的数据字段的最大长度是 MSS 个字节。” 填充字段 —— 这是为了使整个首部长度是 4 字节的整数倍 三次握手 第一次握手：客户端发起一个连接请求，传输 SYN 表示希望建立连接，传输随机产生报文段序号的SEQ=x 第二次握手：服务器端收到客户端发起的连接请求，传输 SYN+ACK 表示确认连接，并且传输服务器端的报文段 SEQ=y，同时用 ACK=x+1表明希望客户端下一次传过来的报文序号为 x+1。 第三次握手：客户端收到服务器端的确认请求，根据服务器端的要求传输 SEQ=x+1 的报文，并且发送 ACK=y+1 确认服务器端的确认请求。 第一次握手：SYN, SEQ=x 第二次握手：SYN+ACK,SEQ=y,ACK=x+1 第三次握手：ACK,SEQ=x+1,ACK=y+1 三次握手建立连接的要点就是，我们通过 TCP 报文段中的 SYN 和 ACK 标志位来确认连接。同时需要注意报文的顺序问题，SEQ 在最开始是随机产生的，但是在建立连接的过程中要按照次序发送。 四次挥手 第一次挥手：A 的 TCP 报文中 FIN=1，表示我这里的工作完成了，要和你断开连接。同时，带着自己的报文序号 SEQ=x 第二次挥手：B 收到 A 的断开连接的请求，用 ACK=x+1表示我知道你要断开了，带上自己的报文 SEQ=y。此时连接处于半断开状态，也就是说 A 和 B 单方面表示要断开了，但是 B 这边可能还有一些事情要处理，没有正式通知 A 它这边已经准备好断开了。 第三次挥手：这是时候 B 的事情处理完了，它告诉 A 我这边一切 ok 了，我也准备断开了。所以 B 和 A 断开连接的请求一样，发送 FIN=1 同时发送 SEQ 和 ACK 第四次挥手：此时按说 A 收到 B 也说可以断开，彻底断开就完了。但是事情还没有结束。因为 A 如果不告诉 B 我已经收到你的断开请求，B 无法确认 A 是不是知道可以彻底断开了，B 就会一直发送请求，知道 A 进行回复。所以 A 要发送确认 B 断开请求的请求，即 ACK=y+1,SEQ=x+1。 这次挥手结束我们会发现 A 处于 TIME_WAIT 状态，这是什么原因呢？其实也很简单，A 发送确认请求之后无法确认这个请求会到达 B，所以就多等 2MSL 的时间，这段时间如果收到 B 的 FIN 请求，则还有机会再进行确认。如果 A 直接进行 CLOSED 状态，很有可能 B 收不到这个确认请求。 UDP 协议UDP 是无连接、不可靠的协议。UDP 协议的责任是创建进程到进程间的通信（由端口号完成），以及有限的差错控制，出现差错悄悄丢弃报文分组。 TCP 协议与 UDP 协议的对比 TCP UDP TCP 提供可靠交付。通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达。 UDP 继承了 IP 包的特性，不保证不丢失，不保证按顺序到达。 TCP 是面向字节流的。发送的时候发的是一个流，没头没尾。 UDP 继承了 IP 的特性，基于数据报的，一个一个地发，一个一个地收。 TCP 是可以有拥塞控制的。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。 UDP 就不会，应用让我发，我就发，管它洪水滔天 TCP 其实是一个有状态服务，是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。 UDP 则是无状态服务。** 通俗地说是没脑子的，天真无邪的，发出去就发出去了。 TCP 协议和 UDP 协议都是传输层的通信协议。 协议特点：TCP 协议是面向连接的、可靠的、基于字节流的的传输层通信协议。UDP 是无连接、不可靠、基于数据报文段的通信协议； 应用场景：TCP 相对于 UDP 来说首部开销更大，传输速率较慢，但是可靠性更高。所以，TCP 协议适用于对通信数据可靠性要求高的场景，比如说文件传输、邮件传输等；UDP 协议适合通信速度高的场景，比如说域名转换、视频直播等。 TCP 和 UDP 分别对应的常见应用层协议TCP 对应的应用层协议 FTP：定义了文件传输协议，使用 21 端口。常说某某计算机开了 FTP 服务便是启动了文件传输服务。下载文件，上传主页，都要用到 FTP 服务。 Telnet：它是一种用于远程登陆的端口，用户可以以自己的身份远程连接到计算机上，通过这种端口可以提供一种基于 DOS 模式下的通信服务。如以前的 BBS 是-纯字符界面的，支持 BBS 的服务器将 23 端口打开，对外提供服务。 SMTP：定义了简单邮件传送协议，现在很多邮件服务器都用的是这个协议，用于发送邮件。如常见的免费邮件服务中用的就是这个邮件服务端口，所以在电子邮件设置-中常看到有这么 SMTP 端口设置这个栏，服务器开放的是 25 号端口。 POP3：它是和 SMTP 对应，POP3 用于接收邮件。通常情况下，POP3 协议所用的是 110 端口。也是说，只要你有相应的使用 POP3 协议的程序（例如 Fo-xmail 或 Outlook），就可以不以 Web 方式登陆进邮箱界面，直接用邮件程序就可以收到邮件（如是163 邮箱就没有必要先进入网易网站，再进入自己的邮-箱来收信）。 HTTP：从 Web 服务器传输超文本到本地浏览器的传送协议。 UDP 对应的应用层协议 DNS：用于域名解析服务，将域名地址转换为 IP 地址。DNS 用的是 53 号端口。 SNMP：简单网络管理协议，使用 161 号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。 TFTP(Trival File Transfer Protocal)：简单文件传输协议，该协议在熟知端口 69 上使用 UDP 服务。","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"tcp","slug":"tcp","permalink":"http://shuiyujie.com/tags/tcp/"},{"name":"udp","slug":"udp","permalink":"http://shuiyujie.com/tags/udp/"}]},{"title":"ARP协议与RARP协议","slug":"cs/计算机网络/ARP协议与RARP协议","date":"2018-08-24T02:48:34.000Z","updated":"2019-10-02T13:19:38.381Z","comments":true,"path":"post/c82e686d.html","link":"","permalink":"http://shuiyujie.com/post/c82e686d.html","excerpt":"上一章讲解了 IP 协议，IP 协议具有定位功能。通过 IP 协议，我们就能够在广域网中找到主机的位置。ARP 协议则是已知 IP 地址，找到 MAC 地址的协议。在我们知道 MAC 地址之后就可以在网络的链路上传送数据帧。RARP 被称为反地址解析协议，即已知 MAC 地址，找到 IP 地址，现在已经用得比较少了。","text":"上一章讲解了 IP 协议，IP 协议具有定位功能。通过 IP 协议，我们就能够在广域网中找到主机的位置。ARP 协议则是已知 IP 地址，找到 MAC 地址的协议。在我们知道 MAC 地址之后就可以在网络的链路上传送数据帧。RARP 被称为反地址解析协议，即已知 MAC 地址，找到 IP 地址，现在已经用得比较少了。 ARP（地址解析协议）ARP（AddressResolutionProtocol）地址解析协议用于将计算机的网络 IP 地址转化为物理 MAC 地址。ARP协议的基本功能就是通过目标设备的IP地址，查询目标设备的MAC地址，以保证通信的顺利进行。在每台安装有TCP/IP 协议的电脑里都有一 个 ARP 缓存表，表里的 IP 地址与 MAC 地址是一一对应的。 不管网络层使用的是什么协议，在实际网络的链路上传送数据帧时，最终还是必须使用硬件地址。 ARP：即通过 IP 地址找 MAC 地址的协议，电脑中会有一张 ARP 缓存表。 逻辑地址 IP 物理地址 MAC 网络级 物理级 全局唯一性 本地唯一性 软件实现 硬件实现 32位 48位 每一个主机都设有一个 ARP 高速缓存(ARP cache)，里面有所在的局域网上的各主机和路由器的 IP 地址到硬件地址的映射表。 当主机 A 欲向本局域网上的某个主机 B 发送 IP 数据报时，就先在其 ARP 高速缓存中查看有无主机 B 的 IP 地址。如有，就可查出其对应的硬件地址，再将此硬件地址写入 MAC 帧，然后通过局域网将该 MAC 帧发往此硬件地址。 ARP 高速缓存的作用 为了减少网络上的通信量，主机 A 在发送其 ARP 请求分组时，就将 自己的 IP 地址到硬件地址的映射写入 ARP 请求分组。 当主机 B 收到 A 的 ARP 请求分组时，就将主机 A 的这一地址映射 写入主机 B 自己的 ARP 高速缓存中。这对主机 B 以后向 A 发送数 据报时就更方便了。 应该注意的问题 ARP 是解决同一个局域网上的主机或路由器的 IP 地址和硬件地址的映射问题。 如果所要找的主机和源主机不在同一个局域网上，那么就要通过 ARP 找到一个位于本局域网上的某个路由器的硬件地址，然后把分组发送给这个路由器，让这个路由器把分组转发给下一个网络。剩下 的工作就由下一个网络来做。 从 IP 地址到硬件地址的解析是自动进行的，主机的用户对这种地址 解析过程是不知道的 只要主机或路由器要和本网络上的另一个已知 IP 地址的主机或路由 器进行通信，ARP 协议就会自动地将该 IP 地址解析为链路层所需要 的硬件地址 默认情况下ARP缓存的超时时限是两分钟 ARP 相关命令 arp –d清除本机arp缓存表 arp –a查看本机当前arp表 arp –s绑定arp地址（注意：本次机器生效，下次重启后会全部失效） RARP（反向地址解析协议） 逆地址解析协议 RARP 使只知道自己硬件地址的主机能够知道 其 IP 地址。这种主机往往是无盘工作站。 因此 RARP协议目前已很少使用。","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"network","slug":"network","permalink":"http://shuiyujie.com/tags/network/"},{"name":"arp","slug":"arp","permalink":"http://shuiyujie.com/tags/arp/"}]},{"title":"IP协议","slug":"cs/计算机网络/IP协议","date":"2018-08-24T00:48:34.000Z","updated":"2019-10-02T09:44:28.618Z","comments":true,"path":"post/4a388439.html","link":"","permalink":"http://shuiyujie.com/post/4a388439.html","excerpt":"网络层协议主要包含：IP协议、ARP（地址解析协议）、RARP（反向地址解析协议）和 ICMP（互联网控制消息协议）。今天将会讲解 IP 协议的功能，IP 数据报的解析以及 IP 分片。","text":"网络层协议主要包含：IP协议、ARP（地址解析协议）、RARP（反向地址解析协议）和 ICMP（互联网控制消息协议）。今天将会讲解 IP 协议的功能，IP 数据报的解析以及 IP 分片。 IP 协议IP协议功能 寻址和路由 传递服务 不可靠，可靠性由上层协议提供 无连接 数据报分段和重组 寻址和路由：比如要从北京往南京发一封邮件，找到南京这个地方就叫寻址；北京到南京有好多条路线，选择走哪条路就叫路由。 传递服务：IP 协议是不可靠的，其可靠性由上层协议来保证，比如用 TCP 协议来保证。IP 协议就是尽最大努力来做事，不保证一定能做到。 数据报的分段是充足：从北京往南京发货物，货物比较大，一车装不下。那就要拆成好几个部分发送。到达目的地再把各个部件组合起来。 IP 数据报由首部和数据部分组成。 字段 比特 作用 版本 4 bit IP协议的版本，一般为 4，即 IPv4 首部长度 4 bit 1111 可表示的最大值为 15 单位， 1个单位4个字节，所以首部长度的最大值是60字节 服务类型 8 bit 用来获得更好的服务，以前一直没有被人们使用 总长度 16 bit 首部长度 + 数据长度，单位为字节，因此数据报的最大长度为 2^16-1=65535 字节。总长度 &lt;= 最大传送单元 MTU 标识 16 bit 一个计数器，用来产生数据报的标识，同一段数据即使分片其标识任然是相同的 标志 3 bit 第1位，无意义当 DF = 0 时，表示允许分片当 MF = 1 时，表示后面“还有分片”。MF = 0 表示最后一个分片。 片偏移 13 bit 较长的分组在分片后，某片在原分组中的相对位置。片偏移以 8 个字节为偏移单位。 生存时间 8 bit TTL (Time To Live)表示数据报在网络中的生存时间，一般以以「跳数」为单位 协议 8 bit 字段指出此数据报携带的数据使用何种协议。以便目的主机的 IP 层将数据部分上交给哪个处理过程。 首部检验和 16 bit 字段只检验数据报的首部不包括数据部分。这里不采用 CRC 检验码而采用简单的计算方法。 源地址 4 bit 发送数据的机器的 IP 地址 目的地址 4 bit 接收数据的机器的 IP 地址 注：其中标识、标志和片偏移和 IP分片相关。 设置 TTL 可以避免发生环路，避免网络资源的浪费。最开始 TTL 的计时单位是秒，现在一般使用跳数来表示，每经过一个路由器，这个值就会减少1，减到0的时候就会将这个报文丢弃。 IP 数据报首部的可变部分 IP 首部的可变部分就是一个选项字段，用来支持排错、测量以及安全 等措施，内容很丰富 选项字段的长度可变，从 1 个比特到 32个比特不等，取决于所选择 的项目 增加首部的可变部分是为了增加 IP 数据报的功能，但这同时也使得 IP 数据报的首部长度成为可变的。这就增加了每一个路由器处理数据报的开销 实际上这些选项很少被使用 IP 分片 IP 数据报由首部和数据部分组成。数据部分存储我们要传输的数据，数据体积如果太大，就需要将其拆分开来，分成多次传输。这就是 IP 分片。 在 TCP/IP 分层中，数据链路层用 MTU（Maximum Transmission Unit，最大传输单元）来限制所能传输的数据包大小，MTU是指一次传送的数据最大长度，不包括数据链路层数据帧的帧头，如以太网的MTU为1500字节，实际上数据帧的最大长度为1512字节，其中以太网数据帧的帧头为12字节。 当发送的 IP 数据报的大小超过了 MTU 时，IP 层就需要对数据进行分片，否则数据将无法发送成功。 比如说我们用火车从北京向南京送一批苹果，这批苹果就相当于是 IP 数据报中的数据部分。 但是火车的承载容量有限，火车最多能装多少就相当于 MTU。 一车装不下，我们把这一批苹果分装到不同的火车上，这就相当于 IP 分片。 比如说上图中的数据大小为 3800 字节，将其分成三个数据包片：0~1399、1400~2799、2800~3799。 我们分片之后，我们还需要将其按照序号组装起来，如何组装就要通过 IP 数据报的首部来进行标记了。 在 IP 首部中有 4 个字节是和 IP 分片相关的： 标识(identification) : 占 16 bit，其本质是一个计数器，用来产生数据报的标识，同一个数据报的各个分片的标识是一样的。 标志(flag)：占 3 bit，其中有1位用来表示是否有更多的分片，如果是最后一个分片，该标志位为0，否则为1 R：目前只有后两个比特有意义。 DF：Don’t Fragment,“不分片”位，DF=1表示不分片； MF：More Fragment,“更多的片”，除了最后一片外，其它每个组成数据报的片都要把比特置1 片偏移：占13 bit，某片在原分组中的相对位置。片偏移以 8 个字节为偏移单位，所以要除以8 参考： IP分片浅析 TCP层的分段和IP层的分片之间的关系 &amp; MTU和MSS之间的关系","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"network","slug":"network","permalink":"http://shuiyujie.com/tags/network/"},{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://shuiyujie.com/tags/tcp-ip/"}]},{"title":"ISO/OSI参考模型","slug":"cs/计算机网络/ISO-OSI参考模型","date":"2018-08-22T18:48:34.000Z","updated":"2019-10-01T09:18:17.744Z","comments":true,"path":"post/38134907.html","link":"","permalink":"http://shuiyujie.com/post/38134907.html","excerpt":"Open System Interconnection，简称 ISO/OSI RM 是一个逻辑结构，并非一个具体的计算机设备或网络。任何两个遵守协议的标准的系统都可以互连通信，描述的是通信软件的结构。","text":"Open System Interconnection，简称 ISO/OSI RM 是一个逻辑结构，并非一个具体的计算机设备或网络。任何两个遵守协议的标准的系统都可以互连通信，描述的是通信软件的结构。 ISO（国际标准化组织）ISO是一个代表了130个国家的标准组织的集体，它的总部设在瑞士 的日内瓦。ISO的目标是制定国际技术标准以促进全球信息交换和无 障碍贸易。 ISO的权威性不仅限于信息处理和通信工业，它还适用于纺织品业、 包装业、货物分发、能源生产和利用、造船业，以及银行业务和金融 服务。事实上，在ISO的大约12,000个标准中，仅有大约500个应用 于计算机相关的产品和功能中。国际电子与电气工程标准是由一个相 似的国际标准组织IEC（国际电子技术协会）单独制定的。ISO所有的 信息技术标准设计与IEC相一致。 OSI 标准只要遵循 OSI 标准，一个系统就可以和位于世界上任何地方的 、也遵循这同一标准的其他任何系统进行通信。 在市场化方面 OSI 却失败了。 OSI 的专家们在完成 OSI 标准时没有商业驱动力； OSI 的协议实现起来过分复杂，且运行效率很低； OSI 标准的制定周期太长，因而使得按 OSI 标准生产的设备无法及 时进入市场； OSI 的层次划分并也不太合理，有些功能在多个层次中重复出现。 网络分层体系结构 网络中的任何一个系统都 是按照层次结构来组织的 同一网络中，任意两个端 系统必须具有相同的层次 每层使用其下层提供的服 务，并向其上层提供服务 通信只在对等层间进行（ 间接的、逻辑的、虚拟的 ），非对等层之间不能互 相“通信” 实际的物理通信只在最底 层完成 Pn：第n层协议，即第 n 层对等实体间通信时必须 遵循的规则或约定 OSI 七层模型的作用： 应用层：提供应用程序运行的环境、负责管理和执行应用程序 表示层：数据传输前，对数据进行格式化、加密和压缩的规则 会话层：解除或建立对话双方的联系 传输层： 传输层可以提供在不同系统之间的进程间数据交互的可靠服务，在网络内两个实体之间建立端到端的通信信道，用来传输信息或报文分组，本层提供两端点可靠、透明的数据传输 可以为会话层提供与网络类型无关的可靠信息传输机制，对会话层屏蔽了下层网络操作细节 网络层：负责建立、保持和终止中间设备的连接，同时负责通信子网内路径选择和拥挤控制 数据链路层： 如何将数据组装成帧，帧是数据链路层基本的传输单位 如何处理数据在传输过程中出现的差错 如何调节发送速率使之与接收方匹配 在两个网络实体之间提供数据链路的建立、维持和释放管理 物理层： 物理层包括设备之间物理接口和用户设备与网络终端设备之间的传输规则 物理层有四个重要的特性： 机械特性：规定了物理连接时对插头和插座的几何尺寸、插针或插孔芯数及排列方式 电气特性：规定了信号状态的电压、电流的识别，最大传输速率等 功能特性：规定了接口信号的来源、作用及与其他信号之间的关系 过程特性：规定了使用交换电路进行数据交换的控制步骤，这些控制步骤应用便利比特流出阿叔得以顺利完成 OSI七层模型的使用","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"network","slug":"network","permalink":"http://shuiyujie.com/tags/network/"},{"name":"通信协议","slug":"通信协议","permalink":"http://shuiyujie.com/tags/通信协议/"}]},{"title":"通信协议的概念","slug":"cs/计算机网络/通信协议的概念","date":"2018-08-22T17:48:34.000Z","updated":"2019-10-01T09:18:24.755Z","comments":true,"path":"post/92ea9430.html","link":"","permalink":"http://shuiyujie.com/post/92ea9430.html","excerpt":"占据两个山顶的蓝军与驻扎在这山谷的白军作战。力量对比是：一个山顶上的蓝军打不过白军，但两个山顶的蓝军协同作战就可战胜白军 。一个山顶上的蓝军拟于次日正午向白军发起攻击。于是发送电文给 另一山顶上的友军。但通信线路很不好，电文出错的可能性很大。因此要求收到电文的友军必须发送确认电文。但确认电文也可能出错。 试问能否设计出一种协议，使得蓝军能实现协同作战因而一定(即100 %)取得胜利？","text":"占据两个山顶的蓝军与驻扎在这山谷的白军作战。力量对比是：一个山顶上的蓝军打不过白军，但两个山顶的蓝军协同作战就可战胜白军 。一个山顶上的蓝军拟于次日正午向白军发起攻击。于是发送电文给 另一山顶上的友军。但通信线路很不好，电文出错的可能性很大。因此要求收到电文的友军必须发送确认电文。但确认电文也可能出错。 试问能否设计出一种协议，使得蓝军能实现协同作战因而一定(即100 %)取得胜利？ 这样无限循环下去，两边的蓝军都始终无法确认自己最后发出的电文对方是否已经收到。没有一种协议能够蓝军能 100% 获胜。 这个例子告诉我们，看似非常简单的协议，设计起来要考虑的问题还是比较多的。 网络协议的比喻网络协议是比较抽象的概念，这里将其余军营中的职级对比着思考。小时候我爱玩军旗，军旗里面有军长、师长、旅长、团长、营长、连长、排长。 Q: 为什么军营里面要分出这么多级别，也相当于网络中要分出这么多层？ 其实原因很直接，复杂的系统都要分层。军营是一个复杂的系统，不分层没法打仗；计算机的世界分层的思想无时不刻都在提现。所以，复杂的系统都要分层，计算机网络是一个复杂的结构，它需要分层。 Q: 那么军营中的人是怎么沟通的？也相当于网络是怎么通信的？ 一般是这样的： 军长和军长商量着办，也就是对等实体的相互通信；相当于网络中同一层之间通信 下一级为上一级服务的，比如说排长为连长服务，连长为营长服务；相当于网络中的下一层为上一层服务 军长和师长在哪儿商量军情，参谋部啊，参谋部就相当于网络中的 SPI（访问点） 网络协议的概念 计算机网络协议 负责在网络上建立通信通道和控制通过通道的信息流的规则 协议依赖于网络体系结构，由硬件和软件协同实现 计算机网络协议的组成 组成 说明 类比 语法 用于确定协议元素的格式，即数据与控制信息的结构和格式 类似于主谓宾、定状补，按照一个规章来沟通 语义 用于确定协议元素的类型，即规定了通信双方需要发出何种控制信息，完成何种动作，以及做出何种应答 满足语法的条件下，说出来的话要有含义，不是疯言疯语 定时 用于确定通信速度的匹配和时序，即对事件实现顺序的详细说明 说话不能太快，也不能太难，这样别人才能听得清楚 协议必须将各种不利的条件事先都估计到，而不能假定一切情 况都是很理想和很顺利的。 必须非常仔细地检查所设计协议能否应付所有的不利情况。 应当注意：事实上难免有极个别的不利情况在设计协议时并没有预计 到。在出现这种情况时，协议就会失败。因此实际上协议往往只能应付绝大多数的不利情况。 协议的开发是一个很复杂的过程。这里涉及到：考虑数据通信 时的各种利弊条件、考虑数据在通信线路上传输的形式，并且 不能与现有的协议互相冲突、考虑协议不能过分冗余、协议不 能不安全，需要强加密，考虑硬件是否能够实现，考虑编程过 程中有哪些漏洞存在，是否适应现今的各种协议…… 1974年，美国IBM公司宣布它研制的系统网络体系结构 SNA.DEC公司研制了他的数据网络体系结构DNA。俩种体系互相不兼容.不能通信.问题日益严重。新的结构开发千钧一发","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"network","slug":"network","permalink":"http://shuiyujie.com/tags/network/"}]},{"title":"全球公共 DNS 解析服务推荐","slug":"cs/计算机网络/全球公共-DNS-解析服务推荐","date":"2018-08-22T16:48:34.000Z","updated":"2019-10-01T09:18:27.004Z","comments":true,"path":"post/142fbc59.html","link":"","permalink":"http://shuiyujie.com/post/142fbc59.html","excerpt":"DNS （Domain Name System 的缩写）的作用非常简单，就是根据域名查出IP地址，你可以把它想象成一本巨大的电话本。原理可见下图： 今天就给大家推荐一些优秀的 DNS 解析服务，方便在不同的场景中使用，解决无法上网的问题和起到加速 / 防劫持的作用 。? 以下列表中包含符号 「★」 的 DNS 服务较为优质。 https://medium.com/censorship/public-dns-servers-ip-collect-66beb622aea8","text":"DNS （Domain Name System 的缩写）的作用非常简单，就是根据域名查出IP地址，你可以把它想象成一本巨大的电话本。原理可见下图： 今天就给大家推荐一些优秀的 DNS 解析服务，方便在不同的场景中使用，解决无法上网的问题和起到加速 / 防劫持的作用 。? 以下列表中包含符号 「★」 的 DNS 服务较为优质。 https://medium.com/censorship/public-dns-servers-ip-collect-66beb622aea8 国内同一个 DNS 服务器在不同国家不同地区的表现并不完全相同，解析速度、响应时间等都有所区别，所以建议大家多多对比。 ★ DNSPod Public DNS+ Public DNS+ 是属于 腾讯云旗下的公共 DNS 服务。拥有 80 多条国内线路和 4 条海外线路，有 BGP Anycast 技术，也是国内首家支持谷歌 ECS (edns-client-subnet) 协议的公共 DNS 解析服务。它只使用一个 IP，但有三地集群容灾和秒级自动故障切换，在国内大多数地方的测速数据都非常好，值得推荐。 IPv4 地址 首选：119.29.29.29 ★ AliDNS 阿里公共 DNS 解析服务 阿里公共 DNS 是 阿里云 推出的免费 DNS 递归解析系统，宣称全球数百台服务器组成的集群，拥有充足的带宽资源，目标是成为国内互联网基础设施的组成部分，支持 BGP Anycast 以及 ECS 技术。 IPv4 地址 首选：223.5.5.5 备用：223.6.6.6 ★ 114 DNS 114 DNS 在国内的用户量相当巨大，其 DNS 解析成功率高，与 ISP 的 DNS 相比，能访问更多的国内外网站；号称纯净、无劫持、无需再忍受被强插广告或粗俗网站之痛苦；114DNS 做得比较早，有一定的技术积累，稳定性不错，尽管速度比前两者差一点点，但也都能让人满意。它还有一个特色，就是根据不同用途可以选择不同的 DNS 组。推荐，笔者正在是使用。 常规公共 DNS (干净无劫持) 首选：114.114.114.114 备选：114.114.115.115 拦截钓鱼病毒木马网站 (保护上网安全) 首选：114.114.114.119 备用：114.114.115.119 拦截色情网站 (保护儿童) 首选：114.114.114.110 备用：114.114.115.110 ★ 百度 BaiduDNS 百度 DNS 公共解析服务，支持 ipv4 和 ipv6。作为中国最大的搜索引擎，百度拥有一流的基础设施和强大技术实力，国内速度相当快！该服务快速稳定无劫持，智能拦截恶意网站，支持 BGP Anycast 和 ECS 技术。 IPv4 地址：180.76.76.76 IPv6 地址：2400:da00::6666 360 DNS 派 （DNSpai Public DNS） DNS 派是由 360 出品的免费公众 DNS 解析服务。它可以让网上冲浪更加稳定、快速、安全；为家庭拦截钓鱼网站，过滤非法网站，建立一个绿色健康的网上环境；为域名拼写自动纠错等。 首选（电信 / 移动 / 铁通）：101.226.4.6 备选（电信 / 移动 / 铁通）：218.30.118.6 首选（联通）：123.125.81.6 备选（联通）：140.207.198.6 CNNIC sDNS sDNS (SecureDNS，简称 sDNS) 是由中国互联网络信息中心 CNNIC 与国内外电信运营商合作推出的免费公共云解析服务，旨在为用户提供高速、安全、智能的上网接入解析服务。sDNS 递归云解析服务采用 IP Anycast+BGP 技术跨区域、跨运营商的分布式异构部署，比运营商提供的 DNS 更快更稳定。 IPv4 地址 首选：1.2.4.8 备用：210.2.4.8 OneDNS OneDNS 是一个安全、快速、免费的小众 DNS 服务。它能屏蔽恶意网站、摆脱无良 ISP 的 DNS 污染与劫持。同时横跨南北的高速线路加速您的网络连接。 IPv4 地址 首选：117.50.11.11 备用：117.50.22.22 国外顾名思义，国外的 DNS 当然是适合海外用户使用咯。而对于国内用户来说，尽管这些 DNS 服务器在国内访问速度不算快，但作用也不少，比如买了国外的 VPS 来搭建番 · 羽 · 土 · 啬相关的应用时，可以配置它使用国外的 DNS 等等。 ★ Google Public DNS (8.8.8.8) 来自 Google 提供的免费全球公共 DNS 服务，主要为了改进网络浏览速度、改善网络用户的浏览体验。这个基本上不用多做什么介绍了，可能它也是目前全球范围内使用量最大的公共 DNS 了，老牌、稳定、技术强劲。 IPv4 地址 首选：8.8.8.8 备用：8.8.4.4 IPv6 地址 首选：2001:4860:4860::8888 备用：2001:4860:4860::8844 ★ CloudFlare DNS (1.1.1.1) CloudFlare DNS 是号称全球最快的 DNS 服务 (当然天朝不算在全球范围内&gt;_&lt;)， CloudFlare 是全球最大的 CDN / DDOS 防护服务提供商之一 (在国内与百度云加速合作)，其遍布全球的基础设施资源极其丰富，资金和技术实力相当雄厚。在国外实测速度相当强劲，目前我已将海外的 VPS 全部换到此 DNS，解析速度非常快，相当值得推荐！ IPv4 地址 首选：1.1.1.1 备用：1.0.0.1 IPv6 地址： 首选：2606:4700:4700::1111 备用：2606:4700:4700::1001 IBM Quad9 (9.9.9.9) IBM 、Global Cyber Alliance 和 Packet Clearing House 合作推出的免费 Quad9 公共 DNS 服务 (9.9.9.9)，主打安全，它会智能屏蔽恶意网址、僵尸网络、钓鱼攻击和其它恶意主机相关联的域名，而且更注重隐私保护。对安全有需求的朋友可以使用这组 DNS。 IPv4 地址 首选：9.9.9.9 备用：149.112.112.112 IPv6 地址 首选：2620:fe::fe 备用：2620:fe::9 Cisco OpenDNS OpenDNS 是一个老牌的免费公共 DNS 提供商，后来被 Cisco (思科) 全资收购。 IPv4 地址 首选：208.67.222.222 备用：208.67.220.220 IPv6 地址 首选：2620:0:ccc::2 备用：2620:0:ccd::2 Hurricane Electric Public DNS （HE） IPv4 地址 首选：74.82.42.42 备用：66.220.18.42 IPv6 地址 首选：2001:470:20::2 备用：2001:470:0:9d::2 科摩多 Comodo SecureDNS IPv4 地址 首选：8.26.56.26 备用：8.20.247.20 Verisign Public DNS IPv4 地址 首选：64.6.64.6 备用：64.6.65.6 IPv6 地址 首选：2620:74:1b::1:1 备用：2620:74:1c::2:2 Neustar Recursive DNS IPv4 地址 首选：156.154.70.1 备用：156.154.71.1 IPv6 地址 首选：2610:a1:1018::1 备用：2610:a1:1019::1 ORACLE Dyn Public DNS IPv4 地址 首选：216.146.36.36 备用：216.146.35.35 Level3 Public DNS IPv4 地址 首选：209.244.0.3 备用：209.244.0.4 Alternate DNS IPv4 地址 首选：23.253.163.53 备用：198.101.242.72 香港 香港宽频 / HKBN IPv4 地址 首选：203.80.96.10 和记环球电讯 DNS IPv4 地址 首选：202.45.84.58 备用：202.45.84.59 Pacific SuperNet DNS IPv4 地址 首选：202.14.67.4 备用：202.14.67.14 台湾 中华电信 / HiNet IPv4 地址 首选：168.95.1.1 备用：168.95.192.1 IPv6 地址 首选：2001:b000:168::1 备用：2001:b000:168::2 数位联合电信 / Seednet IPv4 地址 首选：139.175.252.16 备用：139.175.55.244 台湾网路资讯 / TWNIC Quad101 Public DNS IPv4 地址 首选：101.101.101.101 备用：101.102.103.104 IPv6 地址 首选：2001:de4::101 备用：2001:de4::102 韩国 KT olleh IPv4 地址： 首选：168.126.63.1 备用：168.126.63.2 SK Broadband 首选：210.220.163.82 备用：219.250.36.130 LG U+ 首选：164.124.101.2 备用：203.248.252.2 首选：164.124.107.9 备用：203.248.242.2 其他国家 俄罗斯 Yandex Public DNS IPv4 地址 首选：77.88.8.8 备用：77.88.8.1 IPv6 地址 首选：2a02:6b8::feed:0ff 备用：2a02:6b8:0:1::feed:0ff 俄罗斯 SafeDNS IPv4 地址 首选：195.46.39.39 备用：195.46.39.40 德国 DNS.WATCH Public DNS IPv4 地址 首选：84.200.69.80 备用：84.200.70.40 IPv6 地址 首选：2001:1608:10:25::1c04:b12f 备用：2001:1608:10:25::9249:d69b 瑞士 xiala.net Public DNS IPv4 地址 首选：77.109.148.136 备用：77.109.148.137 IPv6 地址 首选：2001:1620:2078:136:: 备用：2001:1620:2078:137:: 丹麦 UncensoredDNS IPv4 地址 首选：91.239.100.100 备用：89.233.43.71 IPv6 地址 首选：2001:67c:28a4:: 备用：2a01:3a0:53:53:: 荷兰 Freenom World Public DNS IPv4 地址 首选：80.80.80.80 备用：80.80.81.81 写在后面 你可以在路由器上设置公共 DNS，这将应用到你全部的设备。也可以根据需要只在某台电脑、手机上独立进行设置。Win、macOS、Linux 等系统的配置方法可以很容易搜索得到。 而且不仅仅是电脑和手机，像 XBox、PS4 等游戏机或者任何网络设备，基本都可以设置 DNS 的。如果你玩游戏、下载游戏速度太慢太卡，也不妨试试更换一个 DNS 看看效果。","categories":[{"name":"computer science","slug":"computer-science","permalink":"http://shuiyujie.com/categories/computer-science/"},{"name":"计算机网络","slug":"computer-science/计算机网络","permalink":"http://shuiyujie.com/categories/computer-science/计算机网络/"}],"tags":[{"name":"network","slug":"network","permalink":"http://shuiyujie.com/tags/network/"},{"name":"dns","slug":"dns","permalink":"http://shuiyujie.com/tags/dns/"}]},{"title":"数据库三范式","slug":"数据库/数据库三范式","date":"2018-05-30T12:39:39.000Z","updated":"2019-10-05T11:53:53.274Z","comments":true,"path":"post/f413b470.html","link":"","permalink":"http://shuiyujie.com/post/f413b470.html","excerpt":"第一范式(确保每列保持原子性) 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。 第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。 上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式(确保表中的每列都和主键相关) 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键。 第三范式(确保每列都和主键列直接相关,而不是间接相关) 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。","text":"第一范式(确保每列保持原子性) 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。 第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。 上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式(确保表中的每列都和主键相关) 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键。 第三范式(确保每列都和主键列直接相关,而不是间接相关) 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://shuiyujie.com/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://shuiyujie.com/tags/mysql/"}]},{"title":"MySQL中的事务","slug":"数据库/MySQL中的事务","date":"2018-05-29T15:57:28.000Z","updated":"2019-10-03T14:25:33.501Z","comments":true,"path":"post/ca802349.html","link":"","permalink":"http://shuiyujie.com/post/ca802349.html","excerpt":"事务是不可再分的工作单元，一个事务对应一个完整的业务，一个业务由一条或者多条 DML 语句构成。 比如说张三向李四转账，这是一个业务。转账的操作需要扣除张三的余额，增加李四的余额，这就是两条 update 语句，共同构成了转账这个业务。只执行一条 update 语句代表事务执行失败，需要回滚；两条都执行成功，一个事务才算执行成功，就会把数据写到 MySQL 底层数据文件中。 本文主要介绍事务的概念、事务的四大特性（ACID）、事务的隔离级别以及如何设置和使用事务。","text":"事务是不可再分的工作单元，一个事务对应一个完整的业务，一个业务由一条或者多条 DML 语句构成。 比如说张三向李四转账，这是一个业务。转账的操作需要扣除张三的余额，增加李四的余额，这就是两条 update 语句，共同构成了转账这个业务。只执行一条 update 语句代表事务执行失败，需要回滚；两条都执行成功，一个事务才算执行成功，就会把数据写到 MySQL 底层数据文件中。 本文主要介绍事务的概念、事务的四大特性（ACID）、事务的隔离级别以及如何设置和使用事务。 事务 一个最小的不可再分的工作单元 通常一个事务对应的是一个完整的业务 一个完整的业务需要批量的 DML 语句共同完成 事务只和 DML 语句有关系，或者说 DML 语句才有事务 在事务进行过程中，在结束之前，DML 语句是不会更改底层数据库文件中的数据的。只有在事务成功结束的时候，才会修改底层硬盘中的数据。 事务开启的标志是什么？事务结束的标志是什么？ 开启的标志： 任何一条 DML 语句（insert update delete）执行，标志事务的启动 结束的标志： 提交：成功的结束，将所有的 DML 语句操作历史记录和底层硬盘文件中的数据一次同步 回滚：失败的结束，将所有的 DML 语句操作历史记录全部清空 关于一些术语 开启事务：Start Transaction 事务结束：End Transaction 提交事务：Commit Transaction 回滚事务：Rollback Transaction 和事务有短的两条重要的 SQL 语句【TCL】： commit 提交 rollback 回滚 DML （Data Manipulation Language 数据操作语言） insert 将记录插入到数据库 update 修改数据库的记录 delete 删除数据库的记录 commit 提交数据 savepoint 设置回滚点 rollback 回滚 当执行DML命令如果没有提交，将不会被其他会话看到。除非在DML命令之后执行了DDL命令或DCL命令，或用户退出会话，或终止实例，此时系统会自动发出commit命令，使未提交的DML命令提交。 事务四大特性（ACID）原子性（Atomicity）:事务是最小的工作单元，不可再分 一致性（Consistency）:事务要求所有的 DML 语句操作的时候，必须保证同时成功或者同时失败 隔离性（Isolation）:事务A和事务B之间具有隔离 持久性（Durability）:是事务结束的保证，事务终结的标志 事务的提交和回滚在 MySQL 数据库管理系统中，默认情况下，事务是自动提交的。也就是说，只要执行一条 DML 语句，开启了事务，并且提交了事务 这种在动提交机制是可以关闭的： 关闭方式一： 1234567891011121314# 手动开启事务start transaction;# 执行 DML 语句insert into t_user(name) values(&apos;zhangsan&apos;);# 【事务成功的结束】手动提交事务commit;# 手动开启事务start transaction;# 执行 DML 语句insert into t_user(name) values(&apos;zhangsan&apos;);# 【事务失败的结束】手动回滚事务rollback; 关闭方式二： 123456789# 只对当前会话有效SET AUTOCOMMIT = OFF;SET AUTOCOMMIT = ON;# 或者SET SESSION AUTOCOMMIT = OFF;SET SESSION AUTOCOMMIT = ON;# 查看变量状态show variables like %auto%; 事务的隔离级别事务的四大特性之一：隔离性（isolation），即事务 A 和事务 B 之间具有一定的隔离级别。 脏读、不可重复读、幻读 读未提交 READ_UNCOMMITTED: 事务 A 未提交的数据，事务 B 可以读取到，可能会导致脏读、幻读或不可重复读；这种隔离级别是最低级别，这种隔离级别一般在理论上存在，数据库默认的隔离级别一般都高于这个级别。 读已提交 READ_COMMITTED: 事务 A 提交了数据，事务 B 才能读取到。换句话说，对方事务提交之后的数据，我当前事务才能读取到。这种隔离级别可以避免脏读，但是这种隔离级别会导致“不可重复读”。 可重复读 REPEATABLE_READ: 事务 A 提交之后的数据，事务 B 读取不到，事务 B 是可以重复读取数据的。可以阻止脏读和不可重复读，但幻读仍有可能发生；MySQL 默认的隔离级别就是可重复读。 串行化 SERIALIZABLE: 最高的隔离级别，完全服从 ACID 的隔离级别。事务 A 在操作数据表中数据时，事务 B 需要排队等待。该级别可以防止脏读、不可重复读以及幻读。但是由于无法并发，将严重影响程序的性能。通常情况下也不会用到该级别。 隔离级别 脏读 不可重复读 幻读 读未提交 ✔︎ ✔︎ ✔︎ 读已提交 ✘ ✔︎ ✔︎ 可重复读 ✘ ✘ 对 InnoDB 不可能 串行化 ✘ ✘ ✘ 脏读、不可重复读、幻读 脏读：：表示一个事务能够读取另一个事务中还未提交的数据，即事务 A 未提交，事务 B 能读到 A 未提交的数据。 不可重复读 ：是指在一个事务内，多次读同一数据。这个解释可重复读更容易理解一点，可重复读只的是事务 A 此时读取到的状态，和事务 B 提交事务修改数据之后的状态一样。 幻读：：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。 设置服务器缺省隔离级别通过修改配置文件设置可以再 my.ini 文件中使用 transaction-isolation 选项来设置服务器的缺省事务隔离级别 12345678– READ-UNCOMMITTED– READ-COMMITTED– REPEATABLE-READ– SERIALIZABLE• 例如：[mysqld]transaction-isolation = READ-COMMITTED 通过命令动态设置隔离级别 隔离级别也可以在运行的服务器中动态设置，应使用SET TRANSACTION ISOLATION LEVEL语句 其语法模式为： SET [GLOBAL | SESSION] TRANSACTION ISOLATION LEVEL &lt;isolation-level&gt; 其中的&lt;isolation-level&gt;可以是： READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE 例如： SET TRANSACTION ISOLATION LEVEL REPEATABLE READ; 隔离级别的作用范围 事务隔离级别的作用范围分为两种： 全局级：对所有的会话有效 会话级：只对当前的会话有效 例如，设置会话级隔离级别为 READ COMMITTED ： 12SET TRANSACTION ISOLATION LEVEL READ COMMITTED；SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED； 设置全局级隔离级别为READ COMMITTED ： 1SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED； 查看隔离级别 服务器变量tx_isolation（包括会话级和全局级两个变量）中保存着当前的会话隔离级别。 为了查看当前隔离级别，可访问tx_isolation变量： 查看会话级的当前隔离级别： 12SELECT @@tx_isolation;SELECT @@session.tx_isolation; 查看全局级的当前隔离级别： 1SELECT @@global.tx_isolation;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://shuiyujie.com/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://shuiyujie.com/tags/mysql/"},{"name":"事务","slug":"事务","permalink":"http://shuiyujie.com/tags/事务/"}]},{"title":"MySQL中的存储引擎","slug":"数据库/MySQL中的存储引擎","date":"2018-05-29T14:57:28.000Z","updated":"2019-10-02T15:29:12.039Z","comments":true,"path":"post/1c3a568a.html","link":"","permalink":"http://shuiyujie.com/post/1c3a568a.html","excerpt":"MySQL中常用的三种存储引擎分别是： MyISAM存储引擎、innoDB存储引擎、MEMORY存储引擎。本文将对这三种存储引擎作出重点介绍，最后比较它们的使用场景。","text":"MySQL中常用的三种存储引擎分别是： MyISAM存储引擎、innoDB存储引擎、MEMORY存储引擎。本文将对这三种存储引擎作出重点介绍，最后比较它们的使用场景。 存储引擎的使用 数据库中的各表（在创建表时）指定的存储引擎来处理 服务器可用的引擎依赖于以下因素 MySQL 版本 服务器在开发时如何被配置 启动选项 怎么看表用的什么存储引擎。 MyISAM 存储引擎 MyISAM 存储引擎是 MySQL 最常用的引擎 它管理的表具有以下特征 使用三个文件表示表 格式文件 —— 存储表结构的定义（mytable.frm） 数据文件 —— 存储表行的结果（mytable.MYD） 索引文件 —— 存储表上的索引（mytable.MYI） 灵活的 AUTO_INCREMENT 字段处理 可被转换为压缩、只读表来节省空间 InnoDB InnoDB 存储引擎是 MySQL 默认的存储引擎 它管理的表具有以下特征 每个 InnoDB 表在数据库目录中以 .frm 格式文件表示 InnoDB 表空间 tablespace 被用于存储表的内容 提供一组用来记录事务性活动的日志文件 用 COMMIT、SAVEPOINT 及 ROLLBACK 支持事务处理 提供全 ACID 兼容 在 MySQL 服务器崩溃后提供自动恢复 多版本（MVCC）和行级锁定 支持外键及引用的完整性，包括级联删除和更新 MEMOEY 存储引擎 使用MEMORY存储引擎的表，其数据存储在内存中，且行的长度固定，这两个特点使得MEMORY存储引擎非常快 MEMORY 存储引擎具有有以下特征 在数据库目录内，每个表均以.frm格式的文件表示。 表数据及索引被存储在内存中。 表级锁机制。 不能包含TEXT或BLOB字段。 MEMORY存储引擎以前被称为HEAP引擎 选择合适的存储引擎当创建表时，应根据表的应用场景选择合适的存储引擎 MyISAM 表最适合于大量的数据读而少量数据更新的混合操作。MyISAM 表的另一种适用情形是使用压缩的只读表。 如果查询中包含较的数据更新操作，应使用 InnoDB。其行级锁机制和多版本的支持为数据读取和更新的混合操作提供了良好的并发机制。 使用 MEMORY 存储引擎存储非永久需要的数据，或者是能够从基于磁盘的表中重新生成的数据。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://shuiyujie.com/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://shuiyujie.com/tags/mysql/"},{"name":"存储引擎","slug":"存储引擎","permalink":"http://shuiyujie.com/tags/存储引擎/"}]},{"title":"Mysql版本升级","slug":"数据库/Mysql版本升级","date":"2018-05-29T12:59:41.000Z","updated":"2019-09-29T13:03:34.857Z","comments":true,"path":"post/813d855e.html","link":"","permalink":"http://shuiyujie.com/post/813d855e.html","excerpt":"MySQL 升级要考虑什么？ 如何进行升级？","text":"MySQL 升级要考虑什么？ 如何进行升级？ MySQL 升级要考虑什么？ 不能为了升级而升级，要有足够的理由。 在对 MySQL 进行升级前要考虑什么？ 升级可以给业务带来的益处 是否可以解决业务上某一方面的痛点 是否可以解决运维上某一方面的痛点 升级可能给业务带来的影响 对原业务程序的支持是否有影响 对原业务程序的性能是否有影响 数据库升级方案的制定 评估受影响的业务系统 升级的详细步骤 升级后的数据库环境检查 升级后的业务检查 升级失败的回滚方案 升级失败回滚的步骤 回滚后的数据库环境检查 回滚后的业务检查 如何进行升级？ MySQL 升级步骤？ 对待升级数据库进行备份 升级Slave服务器版本(关闭老版本，启动新版本) 手动进行主从切换 升级MASTER服务器版本 升级完成后进行业务检查","categories":[{"name":"数据库","slug":"数据库","permalink":"http://shuiyujie.com/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://shuiyujie.com/tags/mysql/"}]},{"title":"Mysql不同发行版的区别","slug":"数据库/Mysql不同发行版的区别","date":"2018-05-29T12:39:39.000Z","updated":"2019-09-29T12:59:05.087Z","comments":true,"path":"post/df0ea61.html","link":"","permalink":"http://shuiyujie.com/post/df0ea61.html","excerpt":"","text":"MySQL 常见的发行版本 MySQL 官方版本（社区版、企业版） Percona MySQL：在 MySQL 官方发行版上进行二次开发 MariaDB 各个发行版本之间的区别和优缺点 MySQL Percona MySQL MariaDB 服务器特性 开源 开源 开源 支持分区表 支持分区表 支持分区表 InnoDB XtraDB XtraDB 企业版监控工具社区版不提供 Percon Monitor 工具 Monyog 高可用特性 基于日志点复制 基于日志点复制 基于日志点复制 基于 Gtid 复制 基于 Gtid 复制 基于 Gtid 复制，但 Gtid 同 MySQL 不兼容 MGR MGR &amp; PXC Galera Cluster MySQL Router Proxy SQL MaxScale 安全特性 企业级防火墙 ProxySQL FireWall MaxScale FireWall 企业版用户审计 审计日志 审计日志 用户密码生命周期 用户密码生命周期 sha256_passwordcaching_sha_password sha256_passwordcaching_sha_pass_word ed25519sha256_password 开发及管理 窗口函数(8.0) 窗口函数(8.0) 窗口函数(10.2) - - 支持基于日志回滚 - - 支持记在表中记录修改 Super read_only Super read_only","categories":[{"name":"数据库","slug":"数据库","permalink":"http://shuiyujie.com/categories/数据库/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://shuiyujie.com/tags/mysql/"}]},{"title":"静态代理、JDK动态代理、Cglib动态代理","slug":"Java/JavaSE/静态代理、JDK动态代理、Cglib动态代理","date":"2018-03-05T11:07:44.000Z","updated":"2019-10-07T14:05:10.572Z","comments":true,"path":"post/90a3864c.html","link":"","permalink":"http://shuiyujie.com/post/90a3864c.html","excerpt":"","text":"本文将介绍代理模式，进而介绍代理模式的两种实现方式静态代理和动态代理的概念。接着用代码模拟静态代理，以及动态代理。动态代理有 JDK 实现的动态代理，CGlib 动态代理两种。Spring AOP 即是通过动态代理来实现的，所以在本文最后会介绍 Spring AOP 的源码。 代理、静态代理、动态代理代理模式(Proxy Pattern) why： 根据开闭原则，我们不应该直接修改一个类，然而我们想要为其增减一些功能 类定义在 jar 中，甚至无法修改 what：给某一个对象提供一个代理，并由代理对象控制对原对象的引用。 how：根据代理类创建时期不同，可以将其分为静态代理、动态代理。 how good：通过代理，可以在目标对象的基础上，减少或者增加功能。 静态代理前面介绍到实现代理模式有两种方法，静态代理和动态代理，两者的主要区别在于创建代理类的时间不同。静态代理类的 .class 文件在程序运行前就已经存在了；动态代理类的 .class 是在程序运行时创建的。 why：静态代理比动态代理更容易理解和实现，简单的代理可以使用静态代理完成 what：代理类在编译器就确定 how：静态代理即程序员实现编写好代理类，实现对目标对象的代理。目标对象与代理对象同时实现目标对象的接口，并在代理对象中用构造函数注入目标对象 how good： 通过代理对象来增加或减少目标对象的功能，控制目标对象原有的访问权限 用小对象代理大对象，减少内存占用，提高系统运行效率 动态代理用静态代理的方式来实现代理模式存在一些弊端：代理新的类就需要修改代理类，违背开闭原则；程序员需要写很多重复的代码。 动态代理中的代理类并不要求在编译期就确定，而是可以在运行期动态生成，从而实现对目标对象的代理功能。 why：动态代理可以解决静态代理的弊端，不侵入性地修改代码，用更高效的方式实现代理模式 what：代理类在运行期动态生成 how： JDK动态代理：java.lang.reflect 包中的 Proxy 类和 InvocationHandler 接口 Cglib动态代理：Cglib是第三方代码生成类库，运行时在内存中动态生成一个子类对象 how good：AOP、过滤器、拦截器都使用动态代理在运行期生成代理类 静态代理实现静态代理的实现包括目标对象接口、目标对象、代理对象三部分，其中目标对象和代理对象同时实现目标对象接口，并且在代理对象中用注入目标对象，实现对目标对象的增强。 123456789101112131415161718192021222324252627282930313233343536373839404142// 目标对象接口public interface HelloSerivice &#123; public void say();&#125;// 目标对象public class HelloSeriviceImpl implements HelloSerivice&#123; @Override public void say() &#123; System.out.println(\"hello world\"); &#125;&#125;// 代理对象public class HelloSeriviceProxy implements HelloSerivice&#123; private HelloSerivice target; public HelloSeriviceProxy(HelloSerivice target) &#123; this.target = target; &#125; @Override public void say() &#123; // 对目标对象的扩展 System.out.println(\"记录日志\"); target.say(); // 对目标对象的扩展 System.out.println(\"清理数据\"); &#125;&#125;public class Main &#123; @Test public void testProxy()&#123; //目标对象 HelloSerivice target = new HelloSeriviceImpl(); //代理对象 HelloSeriviceProxy proxy = new HelloSeriviceProxy(target); proxy.say(); &#125;&#125;// 记录日志// hello world// 清理数据 JDK 动态代理JDK动态代理：java.lang.reflect 包中的 Proxy 类和 InvocationHandler 接口 和静态代理一样，我们任然创建了一个接口，以及其具体实现，如下所示： 1234567891011public interface Target &#123; public String execute();&#125;public class TargetImpl implements Target &#123; public String execute() &#123; System.out.println(\"TargetImpl execute！\"); return \"execute\"; &#125;&#125; 与静态代理不同的是我们的代理类没有实现目标对象的接口，而是采用实现 InvocationHandler 接口，关键在于实现了 invoke() 接口，这个方法在后面说明： 123456789101112131415public class DynamicProxyHandler implements InvocationHandler &#123; private Target target; public DynamicProxyHandler(Target target) &#123; this.target = target; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"========before==========\"); Object result = method.invoke(target,args); System.out.println(\"========after===========\"); return result; &#125;&#125; 我们获取代理的方式则是采用 java.lang.reflect.Proxy 包下 Proxy 类的 newProxyInstance() 方法： 123456789public class Test &#123; public static void main(String[] args) &#123; Target target = new TargetImpl(); DynamicProxyHandler handler = new DynamicProxyHandler(target); Target proxySubject = (Target) Proxy.newProxyInstance(TargetImpl.class.getClassLoader(),TargetImpl.class.getInterfaces(),handler); String result = proxySubject.execute(); System.out.println(result); &#125;&#125; 这里的关键问题就在于 invoke()，Proxy.newProxyInstance()以及proxySubject.execute()。 InvocationHandler 接口java.lang.reflect 包中的 InvocationHandler 接口 1234567891011public interface InvocationHandler &#123; /** * Processes a method invocation on a proxy instance and returns * the result. This method will be invoked on an invocation handler * when a method is invoked on a proxy instance that it is * associated with. */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; 我们对于被代理的类的操作都会由该接口中的 invoke 方法实现，其中的参数的含义分别是: proxy：目标类的实例; method：调用目标类的方法； args：该方法需要的参数。 newProxyInstance 方法java.lang.reflect 包中的 Proxy 类中的 newProxyInstance 方法 1234public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException loader：被代理的类的类加载器； 2、interfaces：被代理类的接口数组； 3、invocationHandler：调用处理器类的对象实例。 该方法会返回一个被修改过的类的实例，从而可以自由的调用该实例的方法。 Cglib 动态代理Cglib动态代理：Cglib是第三方代码生成类库，运行时在内存中动态生成一个子类对象 123456789101112131415161718192021222324252627282930313233public class Target &#123; public String execute() &#123; String message = \"-----------test------------\"; System.out.println(message); return message; &#125;&#125;public class MyMethodInterceptor implements MethodInterceptor &#123; public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(\"&gt;&gt;&gt;&gt;MethodInterceptor start...\"); Object result = methodProxy.invokeSuper(o,objects); System.out.println(\"&gt;&gt;&gt;&gt;MethodInterceptor ending...\"); return \"result\"; &#125;&#125;public class CglibTest &#123; public static void main(String ... args) &#123; System.out.println(\"***************\"); CglibTest test = new CglibTest(); Target proxyTarget = (Target) test.createProxy(Target.class); String res = proxyTarget.execute(); System.out.println(res); &#125; public Object createProxy(Class targetClass) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(targetClass); enhancer.setCallback(new MyMethodInterceptor()); return enhancer.create(); &#125;&#125; 代理对象的生成过程由Enhancer类实现，大概步骤如下： 生成代理类 Class 的二进制字节码； 通过 Class.forName 加载二进制字节码，生成 Class 对象； 通过反射机制获取实例构造，并初始化代理类对象。 JAVA中的静态代理、动态代理以及CGLIB动态代理 所有和Java中代理有关的知识点都在这了。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"动态代理","slug":"动态代理","permalink":"http://shuiyujie.com/tags/动态代理/"}]},{"title":"Java反射机制","slug":"Java/JavaSE/Java反射机制","date":"2018-03-05T10:57:02.000Z","updated":"2019-10-07T15:00:19.324Z","comments":true,"path":"post/29f40393.html","link":"","permalink":"http://shuiyujie.com/post/29f40393.html","excerpt":"理解反射之前，我们应当知道 .java 文件将会被编译成 .class 文件。JVM 中的类加载器会加载 .class，一般是在类第一次使用的时候加载，也可以用Class.forName(&quot;com.mysql.jdbc.Driver&quot;)这种方式提前将类加载到 JVM 中，该方法会返回一个 Class 对象。 Java 反射机制在程序运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。这种 动态的获取信息 以及 动态调用对象的方法 的功能称为 java 的反射机制。 大白话说就是：反射要在运行时才知道操作哪个类，进而获得该类的完整构造，并调用对应的方法。 与反射密切相关的是 java.lang.reflect 包，以及 Class 类。java.lang.reflect 类库主要包含以下三个类： Field ：可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段； Method ：可以使用 invoke() 方法调用与 Method 对象关联的方法； Constructor ：可以用 Constructor 创建新的对象。","text":"理解反射之前，我们应当知道 .java 文件将会被编译成 .class 文件。JVM 中的类加载器会加载 .class，一般是在类第一次使用的时候加载，也可以用Class.forName(&quot;com.mysql.jdbc.Driver&quot;)这种方式提前将类加载到 JVM 中，该方法会返回一个 Class 对象。 Java 反射机制在程序运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。这种 动态的获取信息 以及 动态调用对象的方法 的功能称为 java 的反射机制。 大白话说就是：反射要在运行时才知道操作哪个类，进而获得该类的完整构造，并调用对应的方法。 与反射密切相关的是 java.lang.reflect 包，以及 Class 类。java.lang.reflect 类库主要包含以下三个类： Field ：可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段； Method ：可以使用 invoke() 方法调用与 Method 对象关联的方法； Constructor ：可以用 Constructor 创建新的对象。 如何使用反射获取对象 通过 Class.forName()、类型.class、对象.getClass()来获取 Class 对象 通过 Class 对象的 newInstance(),或者先获取构造器再用构造器调用 newInstance() 进一步获取它的属性、方法等信息 获取 Class 对象第一种，使用 Class.forName 静态方法 1Class clz = Class.forName(\"java.lang.String\"); 第二种，通过 类名.class 的方式，只适合在编译前就知道操作的 Class 1Class clz = String.class; 第三种，通过对象.getClass()的方式 12String str = new String(\"Hello\");Class clz = str.getClass(); 通过反射创建类对象通过反射创建类对象主要有两种方式：通过 Class 对象的 newInstance() 方法、通过 Constructor 对象的 newInstance() 方法。 第一种：通过 Class 对象的 newInstance() 方法。 12Class clz = Apple.class;Apple apple = (Apple)clz.newInstance(); 第二种：通过 Constructor 对象的 newInstance() 方法 123Class clz = Apple.class;Constructor constructor = clz.getConstructor();Apple apple = (Apple)constructor.newInstance(); 通过 Constructor 对象创建类对象可以选择特定构造方法，而通过 Class 对象则只能使用默认的无参数构造方法。下面的代码就调用了一个有参数的构造方法进行了类对象的初始化。 123Class clz = Apple.class;Constructor constructor = clz.getConstructor(String.class, int.class);Apple apple = (Apple)constructor.newInstance(\"红富士\", 15); 通过反射获取类属性、方法、构造器等 反射的优缺点优点可扩展性 ： 应用程序可以利用全限定名创建可扩展对象的实例，来使用来自外部的用户自 定义类； 类浏览器和可视化开发环境 ： 一个类浏览器需要可以枚举类的成员。可视化开发环境（如 IDE）可以从利用反 射中可用的类型信息中受益，以帮助程序员编写正确的代码。 调试器和测试工具 ： 调试器需要能够检查一个类里的私有成员。测试工具可以利用反射来 自动地调用类里定义的可被发现的 API 定义，以确保一组测试中有较高的代码覆盖率。 缺点尽管反射非常强大，但也不能滥用。如果一个功能可以不用反射完成，那么最好就不 用。在我们使用反射技术时，下面几条内容应该牢记于心。 性能开销 ： 反射涉及了动态类型的解析，所以 JVM 无法对这些代码进行优化。因此，反射操作的效率要比那些非反射操作低得多。我们应该避免在经常被执行的代码或对性能要 求很高的程序中使用反射。 安全限制 ： 使用反射技术要求程序必须在一个没有安全限制的环境中运行。如果一个程序必须在有安全限制的环境中运行，如 Applet，那么这就是个问题了。 内部暴露 ： 由于反射允许代码执行一些在正常情况下不被允许的操作（比如：访问私有的 属性和方法），所以使用反射可能会导致意料之外的副作用，这可能导致代码功能失调并 破坏可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可 能也随着变化。 反射的用途及实现待补充 参考资料Java 反射由浅入深 | 进阶必备 深入解析Java反射（1） - 基础 学习java应该如何理解反射？ 大白话说Java反射：入门、使用、原理","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"反射","slug":"反射","permalink":"http://shuiyujie.com/tags/反射/"}]},{"title":"AspectJ实现AOP","slug":"Java/Spring/AspectJ实现AOP","date":"2018-03-05T09:07:44.000Z","updated":"2019-10-06T07:53:32.714Z","comments":true,"path":"post/655d86b3.html","link":"","permalink":"http://shuiyujie.com/post/655d86b3.html","excerpt":"AOP 是面向切面编程的编程思想，AOP 解决日志、权限等于横切性问题，将这些问题与主业务逻辑分开，达到与主业务逻辑结构的目的。 当我们在编写程序的时候，需要完成一些与核心业务逻辑无关的功能。比如说日志功能、权限校验功能。这些操作与对象的核心功能无关，并且散落在各种业务逻辑之中。我们将日志、权限控制这样的功能抽离成独立的切面，并在程序中设置连接点使用这些功能，这就是 AOP 的思想。 AOP 概念是 OOP(Object Oriented Programming) 概念的补充，我们使用面向对象的思想将程序的操作单位从函数变成了对象。但是面向对象的思想，面对与对象的核心功能无关的操作时显得很无力。AOP 的思想就能很优雅地处理这些散落在业务代码中的功能。 总的来说 OOP 思想关注纵向的业务逻辑，AOP 思想关注横向的功能扩展。两者相辅相成，互为补充。","text":"AOP 是面向切面编程的编程思想，AOP 解决日志、权限等于横切性问题，将这些问题与主业务逻辑分开，达到与主业务逻辑结构的目的。 当我们在编写程序的时候，需要完成一些与核心业务逻辑无关的功能。比如说日志功能、权限校验功能。这些操作与对象的核心功能无关，并且散落在各种业务逻辑之中。我们将日志、权限控制这样的功能抽离成独立的切面，并在程序中设置连接点使用这些功能，这就是 AOP 的思想。 AOP 概念是 OOP(Object Oriented Programming) 概念的补充，我们使用面向对象的思想将程序的操作单位从函数变成了对象。但是面向对象的思想，面对与对象的核心功能无关的操作时显得很无力。AOP 的思想就能很优雅地处理这些散落在业务代码中的功能。 总的来说 OOP 思想关注纵向的业务逻辑，AOP 思想关注横向的功能扩展。两者相辅相成，互为补充。 补充：廖雪峰的教程中，关于 OOP 的描述，有一些比较经典。 OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。 面向过程的程序设计把计算机程序视为一系列的命令集合，即一组函数的顺序执行。为了简化程序设计，面向过程把函数继续切分为子函数，即把大块函数通过切割成小块函数来降低系统的复杂度。 而面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 … 面向对象的抽象程度又比函数要高，因为一个Class既包含数据，又包含操作数据的方法。 AOP 的应用场景 日志记录 权限验证 效率检查 事务管理 exception Spring Aop 和 AspectJ 的关系AOP 是一种概念 Spring Aop、AspectJ 都是 AOP 思想的具体实现，Spring Aop有自己的语法，但是语法复杂，所以 Spring Aop 借助了AspectJ的注解，但是底层实现还是自己的。 Spring AOP 提供两种编程风格： 利用 aspectj 的注解@AspectJ support xml aop:config 命名空间，Schema-based AOP support 本文第一章节演示的就是基于注解的方式。 AOP 相关概念 https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#aop-introduction-defn Aspect: 切面，@Componet、@Aspect 注解的类 Pointcut: 切点，是连接点（JoinPoint）的集合，是一个方法 Joinpoint: 连接点，目标对象的方法，也就是我们需要关注和增强的方法 @Pointcut(&quot;execution(* *..test*())&quot;) Advice：通知，位置 + 逻辑 Introduction： Target object：目标对象，原始对象，区别于经过 AOP 增强的对象 AOP prox：代理对象，包含了原始对象的代码和增加后的代码的那个对象 Weaving：织入，把代理逻辑加入到目标对象上的过程叫做织入 @AspectJ 实战 AOP12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 1. 申明是一个配置类： * 2. 配置扫描的包 * 3. 开启对 AspectJ 的支持 **/@Configuration@ComponentScan(\"com\")@EnableAspectJAutoProxypublic class AppConfig &#123;&#125;/** * 这是目标类，用 @Repository 将其交由 Spring 管理 **/@Repositorypublic class IndexDao &#123; public void test() &#123; System.out.println(\"test\"); &#125;&#125;/** * @Component 要将类交给 Spring 进行管理 * 其次要用 @Aspect 表明使用 AspectJ * 这个类就是一个 Aspect 切面 **/@Component@Aspectpublic class MyAspectJ &#123; /** * 在切面里面定义方法，这个方法就是——Pointcut 切点 * * 切点是由多有个 JoinPoint(连接点) 构成的 * * 连接点就用 pointcut expressions 来表明的 * 如：execution(* com.shuiyujie.dao...(..)) 具体可以去文档中找 */ @Pointcut(\"execution(* *..test*())\") public void pointCut()&#123; &#125; /** * 最后就是设置 Advice 通知了： * 通知包括两部分，location 通知的位置；logic 通知的内容 * @Before(\"pointCut()\") 表示 location，在这个切点之前的位置 * public void before() 这个方法就是logic，表示通知的内容 */ @Before(\"pointCut()\") public void before()&#123; System.out.println(\"before\"); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://shuiyujie.com/categories/Java/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://shuiyujie.com/tags/spring/"}]},{"title":"认识 Docker","slug":"Linux/认识-Docker","date":"2018-02-21T13:36:48.000Z","updated":"2019-05-14T13:52:32.783Z","comments":true,"path":"post/2a772a2b.html","link":"","permalink":"http://shuiyujie.com/post/2a772a2b.html","excerpt":"Docker 是一种虚拟化容器技术。他主要解决了配置环境问题。软件部署环节的第一步就是配置环境，使软件能够在机器上流畅地运行。常常听到的一句话是：“我的电脑上能运行啊。”可是换到生产环境就不行了，又需要重新配置环境。最好的解决方式是能够将原始环境的配置一模一样地复制一份，大家就能够时刻保持在相同的环境下运行软件。","text":"Docker 是一种虚拟化容器技术。他主要解决了配置环境问题。软件部署环节的第一步就是配置环境，使软件能够在机器上流畅地运行。常常听到的一句话是：“我的电脑上能运行啊。”可是换到生产环境就不行了，又需要重新配置环境。最好的解决方式是能够将原始环境的配置一模一样地复制一份，大家就能够时刻保持在相同的环境下运行软件。 认识 DockerDocker 是一种虚拟化容器技术。当我听到虚拟化容器最先想到的是虚拟机，docker 和虚拟机又有很大差别。 虚拟机可以在一种操作系统里面运行另一种操作系统，看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件可以随时删除。通过虚拟机我们可以在硬件上运行多个不同的操作系统，但是他有几个缺点： 资源占用多:虚拟机会独占一部分内存和硬盘空间。它运行的时候，其他程序就不能使用这些资源了; 冗余步骤多:虚拟机是完整的操作系统，一些系统级别的操作步骤，往往无法跳过，比如用户登录; 启动慢：启动操作系统需要多久，启动虚拟机就需要多久。 Docker 最初基于 Linux 中的一种容器轻量级虚拟化技术：Linux 容器（Linux Containers，缩写为 LXC）。LXC 则基于 Linux 内核调用 CGroups 和 Namespaces，同时提供用户态 API 接口。用户则可以通过 LXC 提供的资源限制和隔离功能，创建一套完整并且相互隔离的虚拟应用运行环境。 Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。或者说，在正常进程的外面套了一个保护层。对于容器里面的进程来说，它接触到的各种资源都是虚拟的，从而实现与底层系统的隔离。所以 Docker 是进程级别的管理，相对于虚拟机他就有以下几个优点： 启动快:容器里面的应用，直接就是底层系统的一个进程，而不是虚拟机内部的进程。 资源占用少:容器只占用需要的资源，不占用那些没有用到的资源；虚拟机由于是完整的操作系统，不可避免要占用所有资源。另外，多个容器可以共享资源，虚拟机都是独享资源。 体积小:容器只要包含用到的组件即可，而虚拟机是整个操作系统的打包，所以容器文件比虚拟机文件要小很多。 Docker 的历史Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目， 它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权 协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会， 并成立推动 开放容器联盟（OCI）。 Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目已经超过 4 万 6 千个星标和一 万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容 器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 Docker 的作用Docker 带来了行业的变革。首先是解决了云平台之间标准规范不统一，无法相互兼容对接的问题，有了 Docker 屏蔽了硬件层的差异，提供了统一的用户应用层；其次推进了软件开发的流程，各个部门采用相同的数据镜像之后可以将精力进一步集中在产品本身；也简化了软件运行生命周期中软件的日志管理和监控管理，因为 Docker 拥有统一的数据规范和接口规范，不需要对每一款产品进行定制化开发。 具体来说 Docker 的作用归纳为以下几点： 简化配置。将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度； 提供一次性的环境。通过作用一可以做到本地测试他人的软件、持续集成的时候提供单元测试和构建的环境； 代码流水线管理（Code Pipeline）。通过作用一，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少； 快速部署。在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。Docker通过为进程创建一个容器而无需启动一个操作系统，将这个过程缩短到了秒级； 调试能力。Docker提供了很多的工具，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug； 隔离应用，组建微服务架构。通过多个容器，一台机器可以跑多个服务，因此在本机就可以模拟出微服务架构； 提供弹性的云服务。因为 Docker 容器可以随开随关，很适合动态扩容和缩容。 Docker 三个基本概念镜像相比于传统的虚拟化中的 ISO 镜像，Docker 镜像要轻量化很多，它只是一个可定制的 rootfs。用户可以使用其他人创建的镜像，也可以通过 docker commit 这样的命令自己来创建镜像。 Docker 镜像是通过 Dockerfile 来创建，除了提供容器运行时所需的程序、库、资源、配置等文 件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 镜像基于联合文件系统的一种层式结构。镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后 一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除 前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看 到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 容器Docker 可以帮助我们构建和部署容器，只需要把应用程序或者服务打包放进容器即可，他们可以是 Web 服务器、数据库或者应用程序服务器等，docker 都用同样的方式将内容装载进容器中。我们就可以针对容器进行创建、启动、停止、删除、暂停等操作。 容器和镜像可以这样区别。镜像是 docker 生命周期中的构建和打包阶段，而容器则是启动和执行阶段。每个容器都包含一个软件镜像，而容器可以共享底层的只读镜像，通过写入自己特有的内容后添加新的镜像层，新增的镜像层和下层镜像一起又可以作为基础镜像被更上层的镜像使用。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来就好像是在一 个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安 全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。 仓库Docker Registry 用于存储和分发用户构建的镜像，可以分成公有私有两种。将镜像存储在仓库之中，就可以在其他服务器上使用这些镜像。 一个 Docker Registry 中可以包含多个仓库（ （ Tag ）；每个标签对应一个镜像。Repository）；每个仓库可以包含多个标签通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版 本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。 国外的仓库镜像官方、默认、高质量镜像 CoreOS 的 Quay.io Google 的镜像，Kubernetes 的镜像即此 国内的仓库镜像阿里云加速器 DaoCloud 加速器 时速云镜像仓库 网易镜像仓库 DaoCloud 镜像市场 阿里云镜像库 参考资料《Docker全攻略》《第一本Docker书》《Docker技术入门与实战》《Docker进阶与实战》Docker 入门教程八个Docker的真实应用场景","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"},{"name":"docker","slug":"docker","permalink":"http://shuiyujie.com/tags/docker/"}]},{"title":"RabbitMQ 安装和使用","slug":"分布式/消息队列/RabbitMQ-安装和使用","date":"2018-02-04T09:35:33.000Z","updated":"2019-10-18T16:16:14.546Z","comments":true,"path":"post/29003c34.html","link":"","permalink":"http://shuiyujie.com/post/29003c34.html","excerpt":"RabbitMQ 是一个消息队列，主要是用来实现应用程序的异步和解耦，同时也能起到消息缓冲，消息分发的作用。本文介绍RabbitMQ 安装和使用。","text":"RabbitMQ 是一个消息队列，主要是用来实现应用程序的异步和解耦，同时也能起到消息缓冲，消息分发的作用。本文介绍RabbitMQ 安装和使用。 RabbitMQ 是一个开源的AMQP实现，服务器端用Erlang语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。 可以把消息队列想象成邮局，你的笔友把信件投递到邮局，邮递员源源不断地进出邮局，把笔友的信送到你的手里。此时的笔友就是一个生产者（Product）,邮递员一次送信就是（Queue）,而你收信就像是消费者（Consumer）。 AMQPAMQP（Advanced Message Queuing Protocol，高级消息队列协议）的原始用途只是为金融界提供一个可以彼此协作的消息协议，而现在的目标则是为通用消息队列架构提供通用构建工具。消息中间件主要用于组件之间的解耦，消息的发送者无需知道消息使用者的存在，反之亦然。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。 RabbitMQ 则是一个开源的 AMQP 实现。 Rabbit 概念通常我们谈到队列服务, 会有三个概念： 发消息者、队列、收消息者，RabbitMQ 在这个基本概念之上, 多做了一层抽象, 在发消息者和 队列之间, 加入了交换器 (Exchange)。这样发消息者和队列就没有直接联系, 转而变成发消息者把消息给交换器, 交换器根据调度策略再把消息再给队列。 通过 RabbitMQ 官网 的示例中看到 RabbitMQ 有六种模式。 官网中有多种语言的实现，本文用 Java 来实现。采用 Springboot 集成 RabbitMQ。 CentOS 安装 RabbitMQ安装 Erlang、Elixir准备 yum update yum install epel-release yum install gcc gcc-c++ glibc-devel make ncurses-devel openssl-devel autoconf java-1.8.0-openjdk-devel git wget wxBase.x86_64 安装 Erlang wget http://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm rpm -Uvh erlang-solutions-1.0-1.noarch.rpm yum update yum install erlang 验证是否安装成功，输入命令：erl 安装 Elixir因为 EPEL 中的 Elixir 版本太老，所以下面是通过源码编译安装的过程： git clone https://github.com/elixir-lang/elixir.git cd elixir/ make clean test export PATH=”$PATH:/usr/local/elixir/bin” 验证是否安装成功，输入命令：iex 安装 RabbitMQ wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.1/rabbitmq-server-3.6.1-1.noarch.rpm rpm –import https://www.rabbitmq.com/rabbitmq-signing-key-public.asc yum install rabbitmq-server-3.6.1-1.noarch.rpm Rabitmq 管理至此已经安装完成，下面介绍启动和自动开机启动命令和配置 启动： systemctl start rabbitmq-server 开机自动启动： systemctl enable rabbitmq-server 查看 rabbitmq-server 状态： rabbitmqctl status 关闭： systemctl enable rabbitmq-server 可以直接通过配置文件的访问进行管理，也可以通过Web的访问进行管理。 通过Web进行管理,开启 Web 管理: rabbitmq-plugins enable rabbitmq_management chown -R rabbitmq:rabbitmq /var/lib/rabbitmq/ 注：先启动 RabbitMQ 访问：http://192.168.2.223:15672/，默认用户 guest ，密码 guest。 发现登录失败，由于账号guest具有所有的操作权限，并且又是默认账号，出于安全因素的考虑，guest用户只能通过localhost登陆使用。 我们新增一个用户： rabbitmqctl add_user admin 123456 rabbitmqctl set_user_tags admin administrator rabbitmqctl set_permissions -p / admin “.“ “.“ “.*” Springboot 集成 RabbitMQ假设现在已经按照前面的步骤完成了 RabbitMQ 的安装，现在开始使用 Springboot 集成 RabbitMQ。 基本配置IDEA 先新建一个 maven 项目，在 pom 文件中添加相关依赖: pom 文件1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.shuiyujie&lt;/groupId&gt; &lt;artifactId&gt;pom&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;pom&lt;/name&gt; &lt;!-- Spring Boot 启动父依赖 --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Spring Boot Test 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- rabbitmq --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; application.properties123456# rabbitmq 配置文件spring.rabbitmq.host=192.168.0.223# 默认端口spring.rabbitmq.port=5672spring.rabbitmq.username=adminspring.rabbitmq.password=123456 “Hello World” 现在我们的目标很简单就是创建一个生产者 P，和一个消费者 C，同时将 P 产生的消息放到队列中供 C 使用。 Queue 1234567891011import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class RabbitConfig &#123; @Bean public Queue helloQueue() &#123; return new Queue(&quot;hello&quot;); &#125;&#125; HelloSender 123456789101112@Controllerpublic class HelloSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = \"hello \" + new Date(); System.out.println(\"Sender : \" + context); this.rabbitTemplate.convertAndSend(\"hello\", context); &#125;&#125; HelloReceiver 12345678@Componentpublic class HelloReceiver &#123; @RabbitHandler @RabbitListener(queues = \"hello\") public void process(String hello) &#123; System.out.println(\"Receiver : \" + hello); &#125;&#125; 运行 123456789101112@RunWith(SpringRunner.class)@SpringBootTest(classes = HelloApplication.class)public class RabbitmqApplicationTests &#123; @Autowired private HelloSender helloSender; @Test public void hello() throws Exception &#123; helloSender.send(); &#125;&#125; 成功接收到消息 1Receiver : hello Thu Feb 01 22:21:39 CST 2018 注意：HelloReceiver的@RabbitListener(queues = &quot;hello&quot;)注解是方法级的，参照别的文章都是类级别的注解导致一直无法正常连接。 Work Queues Work Queues 模式在原来的基础上多增加了一个消费者。同理我们可以扩展三个、四个甚至更多的consumer。这样做的好处在于，当我们使用一个consumer的时候，当它收到一条消息进行处理的时候会发生阻塞。有多个consumer时，消息就可以分发给空闲的consumer进行处理。 生产者 123456789101112131415161718/** * Work 模式下的生产者 * * @author shui * @create 2018-02-04 **/@Controllerpublic class WorkSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send(int i) &#123; String context = \"work \"; System.out.println(\"Sender : \" + context + \"*****\" + i); this.rabbitTemplate.convertAndSend(\"work\", context); &#125;&#125; Queue 1234567@Configurationpublic class WorkConfig &#123; @Bean public Queue workQueue() &#123; return new Queue(\"work\"); &#125;&#125; 两个消费者 1234567891011121314151617@Componentpublic class WorkReceicer1 &#123; @RabbitHandler @RabbitListener(queues = \"work\") public void process(String message) &#123; System.out.println(\"Work Receiver1 : \" + message); &#125;&#125;@Componentpublic class WorkReceicer2 &#123; @RabbitHandler @RabbitListener(queues = \"work\") public void process(String message) &#123; System.out.println(\"Work Receiver2 : \" + message); &#125;&#125; 测试 1234567891011121314@RunWith(SpringRunner.class)@SpringBootTest(classes = Startup.class)public class RabbitMQDirectTest &#123; @Autowired private WorkSender workSender; @Test public void sendWorkTest() &#123; for (int i = 0; i &lt; 20; i++) &#123; workSender.send(i); &#125; &#125;&#125; 结果 1234567891011121314151617181920Work Receiver1 : work Work Receiver2 : work Work Receiver2 : work Work Receiver1 : work Work Receiver2 : work Work Receiver1 : work Work Receiver2 : work Work Receiver1 : work Work Receiver1 : work Work Receiver2 : work Work Receiver2 : work Work Receiver1 : work Work Receiver2 : work Work Receiver1 : work Work Receiver1 : work Work Receiver2 : work Work Receiver1 : work Work Receiver2 : work Work Receiver2 : work Work Receiver1 : work 发现消费得很平均，每个consumer处理一半的消息。 public/subscribe 从上面的两个例子我们看到producer产生的消息直接发送给queue，然后queue又直接将消息传给consumer。RabbitMQ 的亮点就在于改变了上面这种消息传递的方式，producer不会将消息直接传给queue而是传给exchanges再由exchangers传给queue。然而我们在前面的两个例子中并没有使用exchanges，那是因为 RabbitMQ 有默认的exchanges，只要我们传的参数是&quot;&quot;。在默认模式下，不需要将exchanges做任何绑定。除此之外exchanges有以下几种类型： Direct：direct 类型的行为是”先匹配, 再投送”. 即在绑定时设定一个 routing_key, 消息的 routing_key 匹配时, 才会被交换器投送到绑定的队列中去. Topic：按规则转发消息（最灵活） Headers：设置header attribute参数类型的交换机 Fanout：转发消息到所有绑定队列 Queue 以下使用的是Fanout Exchange转发消息到所有绑定队列。这里要配置两个queue，并且配置exchanges，并把queue和exchanges绑定。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * * public/subscribe 模式 * * @author shui * @create 2018-02-04 **/@Configurationpublic class FanoutConfig &#123; /************************************************************************ * 新建队列 fanout.A 、fanout.B************************************************************************/ @Bean public Queue AMessage() &#123; return new Queue(\"fanout.A\"); &#125; @Bean public Queue BMessage() &#123; return new Queue(\"fanout.B\"); &#125; /** * 建立一个交换机 * * @return */ @Bean FanoutExchange fanoutExchange() &#123; return new FanoutExchange(\"fanoutExchange\"); &#125; /************************************************************************ * 将 fanout.A 、 fanout.B 绑定到交换机 fanoutExchange 上************************************************************************/ @Bean Binding bindingExchangeA(Queue AMessage, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(AMessage).to(fanoutExchange); &#125; @Bean Binding bindingExchangeB(Queue BMessage, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(BMessage).to(fanoutExchange); &#125;&#125; 生产者 在创建producter的时候，要将他和exchanges绑定。 1234567891011@Controllerpublic class FanoutSender &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = \"hi, fanout msg \"; System.out.println(\"Sender : \" + context); this.rabbitTemplate.convertAndSend(\"fanoutExchange\",\"\", context); &#125;&#125; 两个消费者 123456789101112131415161718@Componentpublic class FanoutReceiveA &#123; @RabbitHandler @RabbitListener(queues = \"fanout.A\") public void process(String message) &#123; System.out.println(\"fanout Receiver A : \" + message); &#125;&#125;@Componentpublic class FanoutReceiveB &#123; @RabbitHandler @RabbitListener(queues = \"fanout.B\") public void process(String message) &#123; System.out.println(\"fanout Receiver B : \" + message); &#125;&#125; 测试 1234567891011@RunWith(SpringRunner.class)@SpringBootTest(classes = Startup.class)public class FanoutTest &#123; @Autowired private FanoutSender fanoutSender; @Test public void setFanoutSender() &#123; fanoutSender.send(); &#125;&#125; 结果 12fanout Receiver B : hi, fanout msg fanout Receiver A : hi, fanout msg Routing 在前面的Fanout模式下，消息会直接广播给queue。如果我们想让consumer处理某些特定的消息，就要让他接收消息的队列中没有其他类型的消息，所以能不能让queue只接收某些消息，而不接收另一些消息呢？ RabbitMQ 中有一个 Routingkey 的概念。在队列与交换机的绑定过程中添加Routingkey表示queue接收的消息需要带有Routingkey。 Topic Topic模式和Direct模式类似，Direct模式需要Routingkey完全匹配而Topic模式更加灵活，可以通过通配符进行配置。 在这种交换机模式下：路由键必须是一串字符，用句号（.） 隔开，例如：topic.A 路由模式必须包含一个星号*，主要用于匹配路由键指定位置的一个单词，比如说，一个路由模式是这样子：agreements..b.*，那么就只能匹配路由键是这样子的：第一个单词是 agreements，第四个单词是 b。 井号（#）就表示相当于一个或者多个单词；例如一个匹配模式是agreements.eu.berlin.#，那么，以agreements.eu.berlin开头的路由键都是可以的。 Queue and exchange 另个队列分别为 topic.A,topic.B,将他们绑定到 topicExchange 上。并且设置了规则，topic.A 必须是完全匹配的也就是Direct模式，topic.B 使用Topic模式，只要是Rouctingkey为 topic 开头的都可以接收。 12345678910111213141516171819202122232425262728293031@Configurationpublic class TopicConfig &#123; final static String message = \"topic.A\"; final static String messages = \"topic.B\"; @Bean public Queue queueMessage() &#123; return new Queue(TopicConfig.message); &#125; @Bean public Queue queueMessages() &#123; return new Queue(TopicConfig.messages); &#125; @Bean TopicExchange exchange() &#123; return new TopicExchange(\"topicExchange\"); &#125; @Bean Binding bindingExchangeMessage(Queue queueMessage, TopicExchange exchange) &#123; return BindingBuilder.bind(queueMessage).to(exchange).with(\"topic.message\"); &#125; @Bean Binding bindingExchangeMessages(Queue queueMessages, TopicExchange exchange) &#123; return BindingBuilder.bind(queueMessages).to(exchange).with(\"topic.#\"); &#125;&#125; 生产者 1234567891011121314151617181920212223@Controllerpublic class TopicSend &#123; @Autowired private AmqpTemplate rabbitTemplate; public void send() &#123; String context = \"hi, i am message 0\"; System.out.println(\"Sender : \" + context); this.rabbitTemplate.convertAndSend(\"topicExchange\", \"topic.1\", context); &#125; public void send1() &#123; String context = \"hi, i am message 1\"; System.out.println(\"Sender : \" + context); this.rabbitTemplate.convertAndSend(\"topicExchange\", \"topic.message\", context); &#125; public void send2() &#123; String context = \"hi, i am messages 2\"; System.out.println(\"Sender : \" + context); this.rabbitTemplate.convertAndSend(\"topicExchange\", \"topic.messages\", context); &#125;&#125; 消费者 1234567891011121314151617@Component@RabbitListener(queues = \"topic.A\")public class TopicReceiver &#123; @RabbitHandler public void process(String message) &#123; System.out.println(\"Topic Receiver1 : \" + message); &#125;&#125;@Component@RabbitListener(queues = \"topic.B\")public class TopicReceiver2 &#123; @RabbitHandler public void process(String message) &#123; System.out.println(\"Topic Receiver2 : \" + message); &#125;&#125; 测试 123456789101112131415161718192021@RunWith(SpringRunner.class)@SpringBootTest(classes = Startup.class)public class TopicTest &#123; @Autowired private TopicSend sender; @Test public void topic() throws Exception &#123; sender.send(); &#125; @Test public void topic1() throws Exception &#123; sender.send1(); &#125; @Test public void topic2() throws Exception &#123; sender.send2(); &#125;&#125; 结果 1234567Sender : hi, i am message 1Sender : hi, i am messages 2Sender : hi, i am message 0Topic Receiver1 : hi, i am message 1Topic Receiver2 : hi, i am message 1Topic Receiver2 : hi, i am messages 2Topic Receiver2 : hi, i am message 0 总结掌握 RabbitMQ 的核心在于如何使用好exchanges，它有默认模式&quot;&quot; , direct , topic , headers 和 fanout 这几种模式。 通过 RabbitMQ 的 routingkey 可以过滤交换机传递给队列的消息。fanout 模式下，需要队列和交换机的routingkey完全匹配，而在topic模式下，可以通过通配符进行配置，变得更加灵活。 安装参考：Install RabbitMQ server in CentOS 7 CentOS 7 下安装 RabbitMQ Install Erlang and Elixir in CentOS 7 rabbitmq——用户管理 Springboot 集成 RabbitMQ 参考RabbitMQ Tutorials Spring Boot 中使用 RabbitMQ springboot rabbitmq整合 Spring Boot系列(八)：RabbitMQ详解","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"消息队列","slug":"分布式/消息队列","permalink":"http://shuiyujie.com/categories/分布式/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://shuiyujie.com/tags/消息队列/"},{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://shuiyujie.com/tags/rabbitmq/"}]},{"title":"RabbitMQ消息生产与消费","slug":"分布式/消息队列/RabbitMQ消息生产与消费","date":"2018-02-04T08:18:23.000Z","updated":"2019-10-19T10:37:12.147Z","comments":true,"path":"post/7fc19c9b.html","link":"","permalink":"http://shuiyujie.com/post/7fc19c9b.html","excerpt":"对象 作用 ConnectionFactory 获取连接工厂 Connection 一个连接 Channel 数据通信信道，可发送和接收消息 Queue 具体的消息存储队列 Producer &amp; Consumer 生产者和消费者","text":"对象 作用 ConnectionFactory 获取连接工厂 Connection 一个连接 Channel 数据通信信道，可发送和接收消息 Queue 具体的消息存储队列 Producer &amp; Consumer 生产者和消费者 Maven 依赖12345&lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; Provider123456789101112131415161718192021222324252627282930import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class Procuder &#123; public static void main(String[] args) throws Exception &#123; //1 创建一个ConnectionFactory, 并进行配置 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"192.168.11.76\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); //2 通过连接工厂创建连接 Connection connection = connectionFactory.newConnection(); //3 通过connection创建一个Channel Channel channel = connection.createChannel(); //4 通过Channel发送数据 for(int i=0; i &lt; 5; i++)&#123; String msg = \"Hello RabbitMQ!\"; //1 exchange 2 routingKey channel.basicPublish(\"\", \"test001\", null, msg.getBytes()); &#125; //5 记得要关闭相关的连接 channel.close(); connection.close(); &#125;&#125; queue：队列的名称，字符串即可 durable：是否持久化 exclusive：是否独占，就是说这个队列只有我这一个 channel 可以监听，可以保证顺序消费 autoDelete：队列与 Exchange 脱离，就会自动删除 arguments：扩展参数，null 就好 Customer12345678910111213141516171819202122232425262728293031323334353637383940414243import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import com.rabbitmq.client.Envelope;import com.rabbitmq.client.QueueingConsumer;import com.rabbitmq.client.QueueingConsumer.Delivery;public class Consumer &#123; public static void main(String[] args) throws Exception &#123; //1 创建一个ConnectionFactory, 并进行配置 ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(\"192.168.11.76\"); connectionFactory.setPort(5672); connectionFactory.setVirtualHost(\"/\"); //2 通过连接工厂创建连接 Connection connection = connectionFactory.newConnection(); //3 通过connection创建一个Channel Channel channel = connection.createChannel(); //4 声明（创建）一个队列 String queueName = \"test001\"; channel.queueDeclare(queueName, true, false, false, null); //5 创建消费者 QueueingConsumer queueingConsumer = new QueueingConsumer(channel); //6 设置Channel channel.basicConsume(queueName, true, queueingConsumer); while(true)&#123; //7 获取消息 Delivery delivery = queueingConsumer.nextDelivery(); String msg = new String(delivery.getBody()); System.err.println(\"消费端: \" + msg); //Envelope envelope = delivery.getEnvelope(); &#125; &#125;&#125; queue，队列名称 autoAck，是否自动签收，返回 ack，保证高可用的时候 callback：具体的操作对象","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"消息队列","slug":"分布式/消息队列","permalink":"http://shuiyujie.com/categories/分布式/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://shuiyujie.com/tags/消息队列/"},{"name":"rabbitmq","slug":"rabbitmq","permalink":"http://shuiyujie.com/tags/rabbitmq/"}]},{"title":"Protobuf 编译 Java","slug":"Java/Protobuf-编译-Java","date":"2018-01-28T03:06:34.000Z","updated":"2019-05-14T13:51:40.215Z","comments":true,"path":"post/7e6ba807.html","link":"","permalink":"http://shuiyujie.com/post/7e6ba807.html","excerpt":"Google Protocol Buffers 简称 Protobuf，它提供了一种灵活、高效、自动序列化结构数据的机制，可以联想 XML，但是比 XML 更小、更快、更简单。仅需要自定义一次你所需的数据格式，然后用户就可以使用 Protobuf 编译器自动生成各种语言的源码，方便的读写用户自定义的格式化的数据。与语言无关，与平台无关，还可以在不破坏原数据格式的基础上，依据老的数据格式，更新现有的数据格式。","text":"Google Protocol Buffers 简称 Protobuf，它提供了一种灵活、高效、自动序列化结构数据的机制，可以联想 XML，但是比 XML 更小、更快、更简单。仅需要自定义一次你所需的数据格式，然后用户就可以使用 Protobuf 编译器自动生成各种语言的源码，方便的读写用户自定义的格式化的数据。与语言无关，与平台无关，还可以在不破坏原数据格式的基础上，依据老的数据格式，更新现有的数据格式。 官方下载地址： https://github.com/google/protobuf/releases/tag/v2.6.1 protobuf-2.6.1 百度云盘： 链接:http://pan.baidu.com/s/1hrK7m7a 密码:epnb 安装过程 解压文件 unzip protobuf-2.6.1.zip 进入目录 cd protobuf-2.6.1 自定义编译目录 ./configure –prefix= /Users/shui/company/protobuf 安装 make mak install 配置环境变量 vim .bash_profile 添加如下配置 export PROTOBUF=/Users/shui/company/protobufexport PATH=$PROTOBUF/bin:$PATH` 使配置生效 source .bash_profile 查看是否生效 protoc –version 安装结束，进行编译 指定需要进行编译的 proto 文件 指定编译产生的 java 文件存放的位置 假设文件名为 file.proto,文件放置在 filedir 目录下；产生的java文件也位于 filedir 目录下，编译命令如下： protoc /filedir/file/proto –java_out=/filedir 如果有很多需要编译的 proto 文件，可以进入项目目录 protoc *.proto –java_out=/filedir 参考： https://github.com/google/protobuf/tree/master/javahttp://blog.csdn.net/u013045971/article/details/50592998http://my.oschina.NET/KingPan/blog/283881?fromerr=8vajR5S9","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"protobuf","slug":"protobuf","permalink":"http://shuiyujie.com/tags/protobuf/"}]},{"title":"部署 RockerMQ 双 Master 模式","slug":"分布式/消息队列/部署-RockerMQ-双-Master-模式","date":"2018-01-25T13:57:27.000Z","updated":"2019-10-04T03:01:31.063Z","comments":true,"path":"post/6fc5439d.html","link":"","permalink":"http://shuiyujie.com/post/6fc5439d.html","excerpt":"本文介绍搭建双 master 的 RocektMQ 的集群。","text":"本文介绍搭建双 master 的 RocektMQ 的集群。 RocketMQ集群方式首先要部署一个 RocketMQ 的集群，以下集群方式摘自网络。 推荐的几种 Broker 集群部署方式，这里的 Slave 不可写但可读，类似于 Mysql 主备方式。 单个 Master这种方式风险较大，一旦Broker 重启或者宕机时，会导致整个服务不可用，不建议线上环境使用。 多 Master 模式一个集群无 Slave，全是 Master，例如 2 个 Master 或者 3 个 Master。 优点 配置简单，单个 Master 宕机或重启维护对应用无影响，在磁盘配置为 RAID10 时，即使机器宕机不可恢复情况下，由与 RAID10 磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢） 。性能最高。 缺点 单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到受到影响。 启动顺序 先启动 NameServer 再机器 A，启动第一个 Master 再机器 B，启动第二个 Master 多 Master 多 Slave 模式，异步复制每个 Master 配置一个 Slave，有多对Master-Slave，HA 采用异步复制方式，主备有短暂消息延迟，毫秒级。 优点 即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为 Master 宕机后，消费者仍然可以从 Slave 消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样。 缺点 Master 宕机，磁盘损坏情况，会丢失少量消息。 启动顺序 启动 NameServer 机器 A，启动第一个 Master 机器 B，启动第二个 Master 机器 C，启动第一个 Slave 机器 D，启动第二个 Slave 多 Master 多 Slave 模式，同步双写每个 Master 配置一个 Slave，有多对Master-Slave，HA 采用同步双写方式，主备都写成功，向应用返回成功。 优点 数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与 数据可用性都非常高 缺点 性能比异步复制模式略低，大约低 10%左右，发送单个消息的 RT 会略高。目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能 。 启动顺序 启动 NameServer 机器 A，启动第一个 Master 机器 B，启动第二个 Master 机器 C，启动第一个 Slave 机器 D，启动第二个 Slave 以上 Broker 与 Slave 配对是通过指定相同的brokerName 参数来配对，Master 的 BrokerId 必须是 0，Slave 的BrokerId 必须是大与 0 的数。另外一个 Master 下面可以挂载多个 Slave，同一 Master 下的多个 Slave通过指定不同的 BrokerId 来区分。部署双 master 模式的集群出于简单和可用的考虑，本文使用双 master 的方式部署 RocketMQ 集群。更多细节看上一篇文章RocketMQ Start，我们用上篇文章中相同的方式再部署一台主机。 服务器环境 序号 IP 用户名 角色 模式 1 192.168.0.222 root nameService1,brokerServer1 Master 2 192.168.0.255 Root nameService2,brokerServer2 Master2 修改 hosts vi /etc/hosts IP Name 192.168.0.222 rocketmq-nameserver1 192.168.0.222 rocketmq-master1 192.168.0.225 rocketmq-nameserver2 192.168.0.225 rocketmq-master2 修改配置文件修改目标文件： /usr/local/rocketmq/conf/2m-noslave 该目录下有两个文件,分别修改一个，与下面的配置文件的 brokerName 对应 broker-a.properties broker-b.properties 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#所属集群名字 brokerClusterName=rocketmq-cluster #broker名字，注意此处不同的配置文件填写的不一样brokerName=broker-a|broker-b#0 表示 Master，&gt;0 表示 Slave brokerId=0 #nameServer地址，分号分割namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876 #在发送消息时，自动创建服务器不存在的topic，默认创建的队列数defaultTopicQueueNums=4 #是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true #是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true #Broker 对外服务的监听端口 listenPort=10911 #删除文件时间点，默认凌晨 4点 deleteWhen=04 #文件保留时间，默认 48 小时fileReservedTime=120#commitLog每个文件的大小默认1GmapedFileSizeCommitLog=1073741824#ConsumeQueue每个文件默认存30W条，根据业务情况调整mapedFileSizeConsumeQueue=300000#destroyMapedFileIntervalForcibly=120000#redeleteHangedFileInterval=120000#检测物理文件磁盘空间diskMaxUsedSpaceRatio=88#存储路径 storePathRootDir=/usr/local/rocketmq/store #commitLog 存储路径 storePathCommitLog=/usr/local/rocketmq/store/commitlog #消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort#限制的消息大小imaxMessageSize=65536#flushCommitLogLeastPages=4 #flushConsumeQueueLeastPages=2 #flushCommitLogThoroughInterval=10000 #flushConsumeQueueThoroughInterval=60000#Broker 的角色 #- ASYNC_MASTER 异步复制Master #- SYNC_MASTER 同步双写Master #- SLAVE brokerRole=ASYNC_MASTER#刷盘方式 #- ASYNC_FLUSH 异步刷盘 #- SYNC_FLUSH 同步刷盘 flushDiskType=ASYNC_FLUSH#checkTransactionMessageEnable=false#发消息线程池数量 #sendMessageThreadPoolNums=128 #拉消息线程池数量 #pullMessageThreadPoolNums=128 配置存储路径从配置文件中可以看到需要配置一些文件的存储位置，只要文件中配置的目录存在即可 123456789101112#存储路径 storePathRootDir=/usr/local/rocketmq/store #commitLog 存储路径 storePathCommitLog=/usr/local/rocketmq/store/commitlog #消费队列存储路径存储路径storePathConsumeQueue=/usr/local/rocketmq/store/consumequeue#消息索引存储路径storePathIndex=/usr/local/rocketmq/store/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/rocketmq/store/abort 分别修改日志配置文件 mkdir -p /usr/local/rocketmq/logscd /usr/local/rocketmq/conf &amp;&amp; sed -i ‘s#${user.home}#/usr/local/rocketmq#g’ *.xml 修改 JVM 参数runserver.sh 12345JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms4g -Xmx4g -Xmn2g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;改成JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:PermSize=128m -XX:MaxPermSize=320m&quot; runbroker.sh 12345JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms8g -Xmx8g -Xmn4g&quot;改成：JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m&quot; tools.sh 12345JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms1g -Xmx1g -Xmn256m -XX:PermSize=128m -XX:MaxPermSize=128m&quot;改成JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:PermSize=128m -XX:MaxPermSize=128m&quot; 启动两台主机的 NameServer cd /usr/local/rocketmq/binnohup sh mqnamesrv &amp; 启动BrokerServer A nohup sh mqbroker -c /usr/local/rocketmq/conf/2m-noslave/broker-a.properties &gt;/dev/null 2&gt;&amp;1 &amp;netstat -ntlptail -f -n 500 /usr/local/rocketmq/logs/rocketmqlogs/broker.logtail -f -n 500 /usr/local/rocketmq/logs/rocketmqlogs/namesrv.log 启动BrokerServer A nohup sh mqbroker -c /usr/local/rocketmq/conf/2m-noslave/broker-b.properties &gt;/dev/null 2&gt;&amp;1 &amp;netstat -ntlptail -f -n 500 /usr/local/rocketmq/logs/rocketmqlogs/broker.logtail -f -n 500 /usr/local/rocketmq/logs/rocketmqlogs/namesrv.log 数据清理12345678910111213cd /usr/local/rocketmq/bin sh mqshutdown brokersh mqshutdown namesrv*等待停止 *rm -rf /usr/local/rocketmq/storemkdir /usr/local/rocketmq/storemkdir /usr/local/rocketmq/store/commitlogmkdir /usr/local/rocketmq/store/consumequeue mkdir /usr/local/rocketmq/store/index*重复以上步骤重启NameServer与BrokerServer*","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"消息队列","slug":"分布式/消息队列","permalink":"http://shuiyujie.com/categories/分布式/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://shuiyujie.com/tags/消息队列/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://shuiyujie.com/tags/RocketMQ/"},{"name":"MQ","slug":"MQ","permalink":"http://shuiyujie.com/tags/MQ/"}]},{"title":"Linux中几种常用的文件传输方式","slug":"Linux/Linux中几种常用的文件传输方式","date":"2018-01-21T12:14:35.000Z","updated":"2019-06-14T14:27:05.571Z","comments":true,"path":"post/a6fc1711.html","link":"","permalink":"http://shuiyujie.com/post/a6fc1711.html","excerpt":"整理scp,wget,ftp以及其他Linux中常用的文件传输方式的介绍。","text":"整理scp,wget,ftp以及其他Linux中常用的文件传输方式的介绍。 scp当没有安装web server和ftp server的时候或感觉上面的方法比较麻烦，那么用scp命令就会排上用场。 scp是什么？scp是secure copy的简写，用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。 scp有什么用？1、我们需要获得远程服务器上的某个文件，远程服务器既没有配置ftp服务器，没有开启web服务器，也没有做共享，无法通过常规途径获得文件时，只需要通过scp命令便可轻松的达到目的。 2、我们需要将本机上的文件上传到远程服务器上，远程服务器没有开启ftp服务器或共享，无法通过常规途径上传是，只需要通过scp命令便可以轻松的达到目的。 scp使用方法获取远程服务器上的文件 scp root@192.168.0.223:/apps/test/a.md /Users/shui/Desktop/b.md root@192.168.0.223 : 使用root用户登录服务器192.168.0.223 /apps/test/a.md:本机要传递过去的文件 /Users/shui/Desktop/b.md：远程主机目录 获取远程服务器上的目录 scp -r root@192.168.0.223:/apps/test/a /Users/shui/Desktop/b 将本地文件上传到服务器上 scp test.txt root@192.168.0.223:/apps/test/a.txt root@192.168.0.223 : 使用root用户登录远程服务器192.168.0.223 test.txt:本机要传递过去的文件 /apps/test/a.txt：远程主机目录和文件名 将本地目录上传到服务器上 scp -r dir1 root@192.168.0.223:/apps/test/a 当 scp 指定端口时 scp -P 2222 -r dir1 root@192.168.0.223:/apps/test/a 可能有用的几个参数 -v 和大多数 linux 命令中的 -v 意思一样 , 用来显示进度 . 可以用来查看连接 , 认证 , 或是配置错误 .-C 使能压缩选项 .-4 强行使用 IPV4 地址 .-6 强行使用 IPV6 地址 . 参考：scp 命令 Wget Linux系统中的wget是一个下载文件的工具，它用在命令行下。对于Linux用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。 wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。 wget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。 下载单个文件 wget http://www.minjieren.com/wordpress-3.1-zh_CN.zip 下载并以不同的文件名保存 wget -O tomcat.tar.gz http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.24/bin/apache-tomcat-8.5.24.tar.gz 后台下载 wget -b http://mirrors.hust.edu.cn/apache/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz 参考：每天一个linux命令（61）：wget命令 Ftp功能 ftp 命令使用文件传输协议（File Transfer Protocol, FTP）在本地主机和远程主机之间或者在两个远程主机之间进行文件传输。 FTP 协议允许数据在不同文件系统的主机之间传输。尽管这个协议在传输数据上提供了高适应性，但是它并没有尝试去保留一个特定文件系统上的文件属性（例如一个文件的保护模式或者修改次数）。而且 FTP 协议很少对一个文件系统的整体结构作假定，也不提供这样的功能，比如递归的拷贝子目录。在使用 ftp 命令时，需要注意 FTP 协议的这些特性。当需要保留文件属性或者需要递归的拷贝子目录时，可以使用 rcp/scp 等命令。 语法 ftp [-dignv][主机名称或IP地址] 参数： -d 详细显示指令执行过程，便于排错或分析程序执行的情形。-i 关闭互动模式，不询问任何问题。-g 关闭本地主机文件名称支持特殊字符的扩充特性。-n 不使用自动登陆。-v 显示指令执行过程。 Linux ftp命令 其他摘自：Linux 上的常用文件传输方式介绍与比较 综上所述各种文件传输方式的特征表现各有千秋，我们从以下几个方面综合对比，更深入地了解它们各自的特性。 传输性能wget 通过支持后台执行及断点续传提高文件传输效率 ； rsync 则以其高效的传输及压缩算法达到快传输的目的。 配置难度rcp 只需进行简单的配置，创建 .rhost 文件以及设置 /etc/hosts 文件中主机名与 IP 地址列表； wget 设置设置方便简单，只需在客户端指定参数执行命令即可； rsync 在使用前需要对服务端 /etc/rsyncd.conf 进行参数设定，配置内容相对复杂。 安全性能ftp、rcp 不保证传输的安全性，scp、rsync 则均可基于 ssh 认证进行传输，提供了较强的安全保障。 wget 也可通过指定安全协议做到安全传输。 通过上述的对比不难发现，每种文件传输方法基于其自身的特点与优势均有其典型的适用场景： ftp 作为最常用的入门式的文件传输方法，使用简单，易于理解，并且可以实现脚本自动化； rcp 相对于 ftp 可以保留文件属性并可递归的拷贝子目录； scp 利用 ssh 传输数据，并使用与 ssh 相同的认证模式，相对于 rcp 提供更强的安全保障； wget，实现递归下载，可跟踪 HTML 页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构，适合实现远程网站的镜像； curl 则适合用来进行自动的文件传输或操作序列，是一个很好的模拟用户在网页浏览器上的行为的工具； rsync 更适用于大数据量的每日同步，拷贝的速度很快，相对 wget 来说速度快且安全高效。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]},{"title":"RocketMQ Start","slug":"分布式/消息队列/RocketMQ-Start","date":"2018-01-21T09:39:45.000Z","updated":"2019-10-04T03:01:43.543Z","comments":true,"path":"post/20e9366a.html","link":"","permalink":"http://shuiyujie.com/post/20e9366a.html","excerpt":"RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点： 能够保证严格的消息顺序 提供丰富的消息拉取模式 高效的订阅者水平扩展能力 实时的消息订阅机制 亿级消息堆积能力 今天开始试着理解她。","text":"RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点： 能够保证严格的消息顺序 提供丰富的消息拉取模式 高效的订阅者水平扩展能力 实时的消息订阅机制 亿级消息堆积能力 今天开始试着理解她。 快速开始环境要求 64bit OS, Linux/Unix/Mac is recommended; 64bit JDK 1.8+; Maven 3.2.x Git 准备工作下载安装包 here 到服务器中，本文使用的是rocketmq-all-4.2.0。 unzip rocketmq-all-4.2.0-source-release.zipcd rocketmq-all-4.2.0/mvn -Prelease-all -DskipTests clean install -U 没有注意环境，maven 版本为 3.0.5，升级 maven 到 3.5.2&gt; cd distribution/target/apache-rocketmq启动 Name Server&gt; nohup sh bin/mqnamesrv &amp;&gt; tail -f ~/logs/rocketmqlogs/namesrv.log&gt; The Name Server boot success…启动 Broker&gt; nohup sh bin/mqbroker -n localhost:9876 &amp;&gt; tail -f ~/logs/rocketmqlogs/broker.log错误：启动不成功，看错误日志1234567891011121314151617181920212223## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 8589934592 bytes for committing reserved memory.# Possible reasons:# The system is out of physical RAM or swap space# In 32 bit mode, the process size limit was hit# Possible solutions:# Reduce memory load on the system# Increase physical memory or swap space# Check if swap backing store is full# Use 64 bit Java on a 64 bit OS# Decrease Java heap size (-Xmx/-Xms)# Decrease number of Java threads# Decrease Java thread stack sizes (-Xss)# Set larger code cache with -XX:ReservedCodeCacheSize=# This output file may be truncated or incomplete.## Out of Memory Error (os_linux.cpp:2640), pid=3437, tid=0x00007f378a5c2700## JRE version: (8.0_151-b12) (build )# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 compressed oops)# Core dump written. Default location: /apps/rocketmq-all-4.2.0/distribution/core or core.3437#原因：rocketmq默认jvm配置较高，导致内存不足 runserver.sh 12345JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms4g -Xmx4g -Xmn2g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot;改成JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:PermSize=128m -XX:MaxPermSize=320m&quot; runbroker.sh 12345JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms8g -Xmx8g -Xmn4g&quot;改成：JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m&quot; tools.sh 12345JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms1g -Xmx1g -Xmn256m -XX:PermSize=128m -XX:MaxPermSize=128m&quot;改成JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m -XX:PermSize=128m -XX:MaxPermSize=128m&quot; 再次启动 Broker 成功 The broker[main, 192.168.0.222:10911] boot success. serializeType=JSON and name server is localhost:9876 查看一下服务 Jps 1234[root@main apache-rocketmq]# jps4419 BrokerStartup4636 Jps4077 NamesrvStartup 发送接收消息 123456&gt; export NAMESRV_ADDR=localhost:9876&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ProducerSendResult [sendStatus=SEND_OK, msgId= ...&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ConsumerConsumeMessageThread_%d Receive New Messages: [MessageExt... 停止服务 12sh bin/mqshutdown brokersh bin/mqshutdown namesrv 使用 RocketMQ Console 监控在 Tomcat 中部署 RocketMQ Console，可以看到 RocketMQ 的运行情况。 RocketMQ 扩展项目地址将项目 clone 到本地，进入rocketmq-console，参考项目文档编译。 修改application.properties文件 rocketmq.config.namesrvAddr= 192.168.0.222:9876 编译运行文件 mvn clean package -Dmaven.test.skip=truejava -jar target/rocketmq-console-ng-1.0.0.jar 在浏览器访问192.168.0.222:8080即可访问监控界面，就能看到 RocketMQ 的基本信息。 客户端访问NameService 和 BrokerService已经启动，现在单机模式下用客户端来实现消息的订阅和发送。 Producer123456789101112131415161718192021222324252627282930public class TestProducer &#123; public static void main(String[] args) throws MQClientException, InterruptedException &#123; DefaultMQProducer producer = new DefaultMQProducer(\"DefaultCluster\"); producer.setNamesrvAddr(\"192.168.0.222:9876\"); producer.start(); for (int i = 0; i &lt; 100; i++) &#123; try &#123; &#123; Message msg = new Message(\"TopicTest1\", \"TagA\", \"key113\", \"Hello world\".getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.send(msg); System.out.printf(\"%s%n\", sendResult); QueryResult queryMessage = producer.queryMessage(\"TopicTest1\", \"key113\", 10, 0, System.currentTimeMillis()); for (MessageExt m : queryMessage.getMessageList()) &#123; System.out.printf(\"%s%n\", m); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; producer.shutdown(); &#125;&#125; Consumer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class PullConsumer &#123; private static final Map&lt;MessageQueue, Long&gt; OFFSE_TABLE = new HashMap&lt;MessageQueue, Long&gt;(); public static void main(String[] args) throws MQClientException &#123; DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(\"DefaultCluster\"); consumer.setNamesrvAddr(\"192.168.0.222:9876\"); consumer.start(); Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues(\"TopicTest1\"); for (MessageQueue mq : mqs) &#123; System.out.printf(\"Consume from the queue: %s%n\", mq); SINGLE_MQ: while (true) &#123; try &#123; PullResult pullResult = consumer.pullBlockIfNotFound(mq, null, getMessageQueueOffset(mq), 32); System.out.printf(\"%s%n\", pullResult); putMessageQueueOffset(mq, pullResult.getNextBeginOffset()); switch (pullResult.getPullStatus()) &#123; case FOUND: break; case NO_MATCHED_MSG: break; case NO_NEW_MSG: break SINGLE_MQ; case OFFSET_ILLEGAL: break; default: break; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; consumer.shutdown(); &#125; private static long getMessageQueueOffset(MessageQueue mq) &#123; Long offset = OFFSE_TABLE.get(mq); if (offset != null) return offset; return 0; &#125; private static void putMessageQueueOffset(MessageQueue mq, long offset) &#123; OFFSE_TABLE.put(mq, offset); &#125;&#125; 从控制台可以看到集群的基本信息，包括消息的产生和消费数量等。 RocketMQ 单机模式跑通，接下来使用双 master 模式。部署 RockerMQ 双 Master 模式 参考资料Apache RocketMQRocket-Externals安装rocketmq-consoleMaven 升级","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"消息队列","slug":"分布式/消息队列","permalink":"http://shuiyujie.com/categories/分布式/消息队列/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://shuiyujie.com/tags/消息队列/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://shuiyujie.com/tags/RocketMQ/"}]},{"title":"实现 SSH 免密登录","slug":"Linux/实现-SSH-免密登录","date":"2017-12-19T03:57:18.000Z","updated":"2019-05-14T13:52:27.621Z","comments":true,"path":"post/ea055f0d.html","link":"","permalink":"http://shuiyujie.com/post/ea055f0d.html","excerpt":"SSH (Secure Shell的) 是一种网络协议，用于计算机之间的加密登录。 通过使用SSH，你可以把所有传输的数据进行加密更加安全可靠。使用SSH，还有一个额外的好处就是传输的数据是经过压缩的，所以可以加快传输的速度。SSH 有很多功能，它既可以代替 Telnet，又可以为FTP、Pop、甚至为 PPP 提供一个安全的”通道”。","text":"SSH (Secure Shell的) 是一种网络协议，用于计算机之间的加密登录。 通过使用SSH，你可以把所有传输的数据进行加密更加安全可靠。使用SSH，还有一个额外的好处就是传输的数据是经过压缩的，所以可以加快传输的速度。SSH 有很多功能，它既可以代替 Telnet，又可以为FTP、Pop、甚至为 PPP 提供一个安全的”通道”。 主机之间通过 SSH 进行连接的时候需要输入密码进行校验。在部署分布式应用时，主机间要建立良好的通信，首先要做的就是配置 SSH 免密登录。 以下演示在centos 7中配置免密登录。 配置主机间的免密登录配置免密登录什么是免密登录呢？ 通常我们登录 SSH 是通过账号和免密来登录的，输入ssh username@ip-server然后输入密码。 如果每次都输入密码会很麻烦，而且要对多台主机进行自动化管理，每次都要输入密码不现实。我们可以配置公钥和密钥进行免密登录。免密登录做的事情其实就是通过 SSH 的公钥和密钥来校验身份信息。 首先你要知道每台主机有一份公钥和一份私钥。我们要做的事情可以用一张图来表示： 1.生成密匙对 ssh-keygen 之后可以在/root/.ssh中看到生成的密匙对 123[root@main /]# cd /root/.ssh/[root@main .ssh]# lsid_rsa id_rsa.pub 2.拷贝一份 A 的公钥给 B ssh-copy-id 192.168.0.10输入密码 此时在 B 的authorized_keys中就会有一份 A 的id_rsa.pub公钥信息。 注：第二步操作的做的事情其实就是一个拷贝密钥的工作，也可以手动拷贝，但是用上面的命令更方便。 3.最后我们就可以免密登录,也就是不输入密码 A 就可以登录 B ssh root@192.168.0.10 192.168.0.10 为 B 的 ip 地址 如果要退出登录，输入exit即可。 给主机起一个别名192.168.0.10 是 ip 地址，也就是说登录的时候我们还要输入一次 ip。我们可以给每个主机配置一个别名，用ssh ip-server的方式登录。 就像人有身份证也有名字一样，我们可以通过 ip 来辨识主机。给他一个别名就是给一个hostname。 可以用hostname来查看你的主机名，要改主机名改他的配置文件 vi /etc/hostname在其中替换为你要改的名字，比如 main 重启生效 reboot 这样主机名已经改掉了，还差一步。我们要让主机名和我们的 ip 关联在一起，修改/etc/hosts文件 vi /etc/hosts添加上 ip 和 hostname 的键值对 例如： 1234127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.0.11 main192.168.0.10 slave 两边都配置完成可以用ssh slave直接连接slave。如果你想自己免密连接自己那就按照上面的步骤给自己配置一份密匙就行了，动手试试吧。 SSH 协议是什么以及更多参考SSH 协议介绍 数字签名是什么 SSH原理与运用（一）：远程登录 SSH原理与运用（二）：远程操作与端口转发 如何在CentOS 7上修改主机名","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"},{"name":"ssh","slug":"ssh","permalink":"http://shuiyujie.com/tags/ssh/"},{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/tags/分布式/"}]},{"title":"Numpy 小结","slug":"Python/Numpy小结","date":"2017-12-10T10:41:29.000Z","updated":"2019-05-14T13:52:59.049Z","comments":true,"path":"post/a404f4e2.html","link":"","permalink":"http://shuiyujie.com/post/a404f4e2.html","excerpt":"NumPy 是 Python 语言的一个扩充程序库。支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库，也是学习 python 必学的一个库。","text":"NumPy 是 Python 语言的一个扩充程序库。支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库，也是学习 python 必学的一个库。 1. 读取文件numpy.genfromtxt() 用于读取 txt 文件，其中传入的参数依次为： 需要读取的 txt 文件位置，此处文件与程序位于同一目录下 分割的标记 转换类型，如果文件中既有文本类型也有数字类型，就先转成文本类型 help(numpy.genfromtxt)用于查看帮助文档：如果不想看 API 可以启动一个程序用 help 查看指令的详细用法 123456import numpyworld_alcohol = numpy.genfromtxt(\"world_alcohol.txt\", delimiter=\",\",dtype=str)print(type(world_alcohol))print(world_alcohol)print(help(numpy.genfromtxt)) 2. 构造 ndarraynumpy.array()构造 ndarraynumpy.array()中传入数组参数，可以是一维的也可以是二维三维的。numpy 会将其转变成 ndarray 的结构。 12vector = numpy.array([1,2,3,4])matrix = numpy.array([[1,2,3],[4,5,6]]) 传入的参数必须是同一结构,不是同一结构将发生转换。 123vector = numpy.array([1,2,3,4])array([1, 2, 3, 4]) 均为 int 类型 123vector = numpy.array([1,2,3,4.0])array([ 1., 2., 3., 4.]) 转为浮点数类型 123vector = numpy.array([1,2,'3',4])array(['1', '2', '3', '4'],dtype='&lt;U21') 转为字符类型 利用 .shape 查看结构能够了解 array 的结构，debug 时通过查看结构能够更好地了解程序运行的过程。 1234print(vector.shape)print(matrix.shape)(4,)(2, 3) 利用 dtype 查看类型1234vector = numpy.array([1,2,3,4])vector.dtypedtype('int64') ndim 查看维度一维 1234vector = numpy.array([1,2,3,4])vector.ndim1 二维 123456matrix = numpy.array([[1,2,3], [4,5,6], [7,8,9]])matrix.ndim2 size 查看元素数量12matrix.size9 3. 获取与计算numpy 能使用切片获取数据123matrix = numpy.array([[1,2,3], [4,5,6], [7,8,9]]) 根据条件获取numpy 能够依次比较 vector 和元素之间是否相同 1234vector = numpy.array([5, 10, 15, 20])vector == 10array([False, True, False, False], dtype=bool) 根据返回值获取元素 1234567vector = numpy.array([5, 10, 15, 20])equal_to_ten = (vector == 10)print(equal_to_ten)print(vector[equal_to_ten])[False True False False][10] 进行运算之后获取 12vector = numpy.array([5, 10, 15, 20])equal_to_ten_and_five = (vector == 10) &amp; (vector == 5) 12vector = numpy.array([5, 10, 15, 20])equal_to_ten_or_five = (vector == 10) | (vector == 5) 类型转换将整体类型进行转换 1234567vector = numpy.array([5, 10, 15, 20])print(vector.dtype)vector = vector.astype(str)print(vector.dtype)int64&lt;U21 求和sum() 能够对 ndarray 进行各种求和操作，比如分别按行按列进行求和 12345678910matrix = numpy.array([[1,2,3], [4,5,6], [7,8,9]])print(matrix.sum())print(matrix.sum(1))print(matrix.sum(0))45[ 6 15 24][12 15 18] sum(1) 是 sum(axis=1)) 的缩写，1表示按照 x轴方向求和，0表示按照y轴方向求和 4. 常用函数reshape生成从 0-14 的 15 个数字，使用 reshape(3,5) 将其构造成一个三行五列的 array。 1234567import numpy as nparr = np.arange(15).reshape(3, 5)arrarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) zeros生成指定结构的默认为 0. 的 array 12345np.zeros ((3,4))array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]]) ones生成一个三维的 array,通过 dtype 指定类型 123456789np.ones( (2,3,4), dtype=np.int32 )array([[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]) range指定范围和数值间的间隔生成 array，注意范围包左不包右 123np.arange(0,10,2)array([0, 2, 4, 6, 8]) random 随机数生成指定结构的随机数，可以用于生成随机权重 1234np.random.random((2,3))array([[ 0.86166627, 0.37756207, 0.94265883], [ 0.9768257 , 0.96915312, 0.33495431]]) 5. ndarray 运算元素之间依次相减相减 12345a = np.array([10,20,30,40])b = np.array(4)a - barray([ 6, 16, 26, 36]) 乘方12a**2array([ 100, 400, 900, 1600]) 开根号 1234np.sqrt(B)array([[ 1.41421356, 0. ], [ 1.73205081, 2. ]]) e 求方 1234np.exp(B)array([[ 7.3890561 , 1. ], [ 20.08553692, 54.59815003]]) 向下取整 12345a = np.floor(10*np.random.random((2,2)))aarray([[ 0., 0.], [ 3., 6.]]) 行列变换 1234a.Tarray([[ 0., 3.], [ 0., 6.]]) 变换结构 1234a.resize(1,4)aarray([[ 0., 0., 3., 6.]]) 6. 矩阵运算矩阵之间的运算 1234A = np.array( [[1,1], [0,1]] )B = np.array( [[2,0], [3,4]] ) 对应位置一次相乘 1234A*Barray([[2, 0], [0, 4]]) 矩阵乘法 12345print (A.dot(B))print(np.dot(A,B))[[5 4] [3 4]] 横向相加 12345678910111213a = np.floor(10*np.random.random((2,2)))b = np.floor(10*np.random.random((2,2)))print(a)print(b)print(np.hstack((a,b)))[[ 2. 3.] [ 9. 3.]][[ 8. 1.] [ 0. 0.]][[ 2. 3. 8. 1.] [ 9. 3. 0. 0.]] 纵向相加 123456print(np.vstack((a,b)))[[ 2. 3.] [ 9. 3.] [ 8. 1.] [ 0. 0.]] 矩阵分割 1234#横向分割print( np.hsplit(a,3))#纵向风格print(np.vsplit(a,3)) 7. 复制的区别地址复制通过 b = a 复制 a 的值，b 与 a 指向同一地址，改变 b 同时也改变 a。 123456789101112131415a = np.arange(12)b = aprint(a is b)print(a.shape)print(b.shape)b.shape = (3,4)print(a.shape)print(b.shape)True(12,)(12,)(3, 4)(3, 4) 复制值通过 a.view() 仅复制值，当对 c 值进行改变会改变 a 的对应的值，而改变 c 的 shape 不改变 a 的 shape 1234567891011121314a = np.arange(12)c = a.view()print(c is a)c.shape = 2,6c[0,0] = 9999print(a)print(c)False[9999 1 2 3 4 5 6 7 8 9 10 11][[9999 1 2 3 4 5] [ 6 7 8 9 10 11]] 完整拷贝a.copy() 进行的完整的拷贝，产生一份完全相同的独立的复制 1234567891011121314a = np.arange(12)c = a.copy()print(c is a)c.shape = 2,6c[0,0] = 9999print(a)print(c)False[ 0 1 2 3 4 5 6 7 8 9 10 11][[9999 1 2 3 4 5] [ 6 7 8 9 10 11]]","categories":[{"name":"Python","slug":"Python","permalink":"http://shuiyujie.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://shuiyujie.com/tags/python/"},{"name":"numpy","slug":"numpy","permalink":"http://shuiyujie.com/tags/numpy/"}]},{"title":"缓存雪崩、缓存穿透、缓存击穿是什么？如何解决？","slug":"分布式/Redis/缓存雪崩、缓存穿透、缓存击穿是什么？如何解决？","date":"2017-12-10T08:43:01.000Z","updated":"2019-10-11T03:56:27.106Z","comments":true,"path":"post/ef7b6f1b.html","link":"","permalink":"http://shuiyujie.com/post/ef7b6f1b.html","excerpt":"我们要使用缓存的时候，不可避免地要面对缓存雪崩、缓存穿透、缓存穿透的问题。如果做缓存没有考虑到这些问题，那是不合格的系统。推而广之可以说，引入任何一项技术，只考虑它的优点而没有考虑到它带来的弊端，那就是在挖坑。","text":"我们要使用缓存的时候，不可避免地要面对缓存雪崩、缓存穿透、缓存穿透的问题。如果做缓存没有考虑到这些问题，那是不合格的系统。推而广之可以说，引入任何一项技术，只考虑它的优点而没有考虑到它带来的弊端，那就是在挖坑。 缓存雪崩我们现在有一个系统。高峰期每秒有5000个请求发到系统 A 上。系统A用缓存抗住4000个请求，其中有1000个请求会打到 MySQL 上。 突然缓存挂掉了，这就导致原本缓存抗的4000个请求一下子都打在数据库上。如果我们用的是 MySQL 数据库，每秒5000个请求过来，MySQL 一下子就挂了。接着 DBA 重启 MySQL，还是 5000 个请求一下子打过来，MySQL 又挂了。数据库一挂，整个系统就挂了。这就是缓存雪崩。 缓存雪崩指的是由于缓存断掉之后，引起一些列的连锁反应，就像雪崩一样，一批服务都挂掉了。 缓存雪崩要怎么解决？ 事前：redis 做好高可用，主从架构、redis cluster，避免全线奔溃 事中：本地 ehcache 存储 + hystrix 限流&amp;降级，避免 MySQL 被打死 事后：redis 持久化，重启之后立即磁盘加载数据，快速恢复缓存 缓存穿透现在任然是高峰期每秒5000次请求，其中有4000次是黑客发过来。比如说我们缓存的是商品订单的id，现在黑客发过来的4000个请求id都为负数。 我们是没有负数的id的，所以请求过来无法命中缓存，这样一来黑客的4000个请求就完全穿透缓存，直接打到了 MySQL 上，MySQL 就这样挂了。 解决方案： 缓存空对象：缓存中没有查到就存一个空值到缓存中，比如 set -999 UNKNOW。 布隆过滤器：提前生成一个布隆过滤器，无法通过过滤器的直接返回空。 缓存击穿缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。 解决方式也很简单，可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。 方案 优点 缺点 互斥锁 1. 思路简单2. 保证一致性 1. 代码复杂度增加2. 存在死锁风险 永远不过期 基本杜绝热点key重建问题 1. 不保证一致性2. 逻辑时间增加维护成本和内存成本","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"Redis","slug":"分布式/Redis","permalink":"http://shuiyujie.com/categories/分布式/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://shuiyujie.com/tags/redis/"}]},{"title":"数据分布算法","slug":"分布式/Redis/数据分布算法","date":"2017-12-10T07:23:02.000Z","updated":"2019-10-11T06:05:06.842Z","comments":true,"path":"post/1b091299.html","link":"","permalink":"http://shuiyujie.com/post/1b091299.html","excerpt":"分布式数据处理系统第一个要解决的问题就是如何将数据进行拆分，利用多台计算机处理大规模数据。对于数据量很大的数据集，单机无法保存或者处理时，通过对数据集进行水平拆分，将不同的数据子集存放到不同的处理节点，这种对数据进行拆分的方式叫做分区（partition）。 其中常用的哈希算法有节点取余哈希算法，一致性哈希算法，以及虚拟槽分区算法。Redis Cluster 中用的正式虚拟槽分区算法。","text":"分布式数据处理系统第一个要解决的问题就是如何将数据进行拆分，利用多台计算机处理大规模数据。对于数据量很大的数据集，单机无法保存或者处理时，通过对数据集进行水平拆分，将不同的数据子集存放到不同的处理节点，这种对数据进行拆分的方式叫做分区（partition）。 其中常用的哈希算法有节点取余哈希算法，一致性哈希算法，以及虚拟槽分区算法。Redis Cluster 中用的正式虚拟槽分区算法。 节点取余哈希算法键值为 key 节点数量为 N ，利用公式 $$hash(key)%N$$计算出一个位于$$ [0, N - 1]$$ 区间的值，将这个 key 存放在对应的节点中。 节点取余哈希算法的优点在于简单，缺点在于节点伸缩时，数据节点关系会发生变化，导致数据迁移。举例来说就是原来有 3 个节点，用 3 取余，现在扩容到 4 个节点，用 4 取余，算出来的结果是不一样的。为了减小迁移率，建议翻倍扩容。 此外，使用节点取余哈希算法，一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。 一致性哈希算法在一致性哈希中，我们把所有的数据包括节点数据都放在一个哈希环上，我们除了需要计算要存储的数据的 key 的hash之外，还要计算节点的 hash。然后在存储时，选择一个跟 key 的 hash 最接近的节点（顺时针找到第一个大于等于该哈希值的节点），存储进去。 一致性哈希解决了由于集群变动而导致的数据迁移率高的问题。即如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 但是又引入了另一个复杂的问题，就是每个节点的负载不相同，因为每个节点的hash是根据IP计算出来的。一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。 为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 虚拟槽分区算法（redis slot）虚拟槽分区就是为了解决一致性哈希分区的不足而被创造的。 预设虚拟槽：redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。 服务端管理节点：如上图所示，redis cluster 中每个 master 都会持有部分 slot，如果增加一个 master 其他 master 就移动部分 hash slot 到新增节点上；如果挂掉一个节点，则该节点原来的 hash slot 会交给其他节点管理。 移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。同时，任何一台机器宕机，不会影响其他机器的。因为 key 找的是 hash slot，不是机器。 参考资料数据分区 redis数据分布 redis cluster 数据分布算法：hash+ 一致性 hash + redis cluster 的 hash slot)","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"Redis","slug":"分布式/Redis","permalink":"http://shuiyujie.com/categories/分布式/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://shuiyujie.com/tags/redis/"}]},{"title":"Reids与Memcached的区别","slug":"分布式/Redis/Reids与Memcached的区别","date":"2017-12-09T09:17:34.000Z","updated":"2019-10-04T12:42:32.241Z","comments":true,"path":"post/877d1a33.html","link":"","permalink":"http://shuiyujie.com/post/877d1a33.html","excerpt":"Redis 支持复杂的数据结构redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。 Redis 原生支持集群模式在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。","text":"Redis 支持复杂的数据结构redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。 Redis 原生支持集群模式在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比 memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"Redis","slug":"分布式/Redis","permalink":"http://shuiyujie.com/categories/分布式/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://shuiyujie.com/tags/redis/"}]},{"title":"maven实战","slug":"Java/maven-实战","date":"2017-12-06T13:11:20.000Z","updated":"2019-09-22T08:20:33.417Z","comments":true,"path":"post/cfedc026.html","link":"","permalink":"http://shuiyujie.com/post/cfedc026.html","excerpt":"Maven 是跨平台的项目管理工具，主要服务于基于Java平台的项目构建、依赖管理和项目信息管理。Maven 的主要思想是约定优于配置。通过将约定项目的目录结构，抽象项目的生命周期的方式，将程序员从繁琐的项目构建中解放出来。","text":"Maven 是跨平台的项目管理工具，主要服务于基于Java平台的项目构建、依赖管理和项目信息管理。Maven 的主要思想是约定优于配置。通过将约定项目的目录结构，抽象项目的生命周期的方式，将程序员从繁琐的项目构建中解放出来。 本文是《Maven 实战》的笔记归纳，找到一本好书就像发现一座宝藏，让人获益匪浅。 注：本文用的是 maven-3.5.0 版本。 Maven 优点和作用日常工作除了编写源代码，每天有相当一部分时间花在了项目构建中，他们包括项目的编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作。 项目自动化构建。Maven 提供一套规范以及一系列脚本，从清理、编译、测试到生成报告，再到打包和部署实现自动化构建。还提供了插件扩展的方式，进一步简化构建的过程。Maven 还能对项目进行编译、测试、打包，并且将项目生成的构建部署到仓库中。 Maven 是跨平台的。对外提供了一致的操作接口，无论在 windows 平台、Linux 平台还是 Mac 上都能用相同的命令进行操作。同时，Maven 项目目录结构、测试用例命名方式等内容都有既定的规则，只要遵循了这些成熟的规则，用户在项目间切换的时候就免去了额外的学习成本。 Maven 是依赖管理工具。Java 项目需要依赖许多的 jar 包，随着依赖的增多，版本不一致、版本冲突、依赖臃肿等问题都会接踵而来。Maven 通过仓库统一存储这些 jar 包，并通过 pom 文件来管理这些依赖。 Maven 是项目配置工具。Maven能帮助我们管理原本分散在项目中各个角落的项目信息，包括项目描述、开发者列表、版本控制系统地址、许可证、缺陷管理系统地址等。 Maven 的仓库 以上是 Maven 的概念模型，前面说过 Maven 能管理众多的 jar 包，并且梳理他们之间的依赖关系。Maven 通过 pom 文件和仓库进行实现。 仓库的作用如果没有 maven 我们要使用一个 jar 包要从项目的官网寻找下载 jar 到本地，然后再将 jar 包导入到项目中。这样存在几个问题： 去相应的网站寻找 jar 包费精力 下载之后当需要用到某一个 jar 包的时候还要在本地找 jar 包 依赖的 jar 包有多个版本要怎么管理… 最好的解决方式就是将这些 jar 包统一管理，每次只要去一个地方找就可以了。 Maven 就帮我们做了这样一件事情，他提供一个免费的中央仓库http://repo1.maven.org/maven2,该中央仓库包含了世界上大部分流行的开源项目。 我们可以从中央仓库下载需要的 jar 包，从中央仓库下载的 jar 包会统一保存在 maven 的本地仓库中。本地仓库在本机的.m2文件夹中。 本地仓库更多相关信息可以去搜索 maven 的安装教程。 更多种类的仓库 远程仓库除了中央仓库还有私服和其他公共仓库。 私服私服是一种特殊的远程仓库，它是架设在局域网内的仓库服务，私服代理广域网上的远程仓库，供局域网内的Maven用户使用。 上图是搭建私服的示意图。私服会将其他公共仓库的 jar 缓存到搭建的服务器上，局域网内的用户还可以将构建的项目直接放到私服上供其他开发人员使用。 架设私服是 Maven 推荐的做法，私服减少中央服务器的压力。如果没有私服，每次请求都需要向中央服务器请求，有了私服之后通过私服的服务器向中央仓库请求，局域网内的用户只需要向私服请求即可。 其他公共仓库比如 Maven 的中央仓库部署在国外，国内访问外网速度不够，我们可以在国内架设 Maven 的公共仓库。 如果仓库 X 可以提供仓库 Y 存储的所有内容，那么就可以认为 X 是 Y 的一个镜像，显然在国内我们需要一个中央仓库的镜像。http://maven.net.cn/content/groups/public/是中央仓库，http://repo1.maven.org/maven2/在中国的镜像。 Maven 默认是从中央仓库下载文件的，想要让其从其他地方下载文件就要进行配置，这里就需要操作 maven 的 setting.xml 文件了。 setting 文件在安装好 maven 的基础上，进入 maven 的安装目录，可以看到如下的目录结构： bin : mvn 的一些脚本文件 boot : 含有 plexus-classworlds 类加载器框架 conf : 配置文件 lib : maven 所使用的 jar 包（maven 基于 java 开发） setting.xml 文件就在conf目录中。这里的setting.xml是 maven 全局的配置文件，不建议修改。修改之后会影响 maven 的升级等操作。常用的做法是拷贝一份setting.xml到 maven 本地仓库的同一目录下，而本地仓库配置在用户目录的.m2文件夹中，此时的setting.xml就是用户级别的配置文件。 强烈建议遵循以上规范，避免不必要的麻烦。 自定义本地仓库位置接下来就来看setting.xml的一些配置了。 首先localRepository定义本地仓库位置，默认在用户目录下的.m2/repository中。 12345Default: $&#123;user.home&#125;/.m2/repository&lt;localRepository&gt; /path/to/local/repo&lt;/localRepository&gt; 配置多个远程仓库前面讲过有中央仓库和其他远程仓库，配置远程仓库就在repositories中配置。 12345678910111213141516&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jboss&lt;/id&gt; &lt;name&gt;JBoss Repository&lt;/name&gt; &lt;url&gt;http://repository.jboss.com/maven2/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;daily&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt;&lt;/repositories&gt; 在repositories元素下，可以使用repository子 元素声明一个或者多个远程仓库。 配置中央仓库镜像12345678＜settings＞ …… ＜mirrors＞ ＜mirror＞ ＜id＞maven.net.cn＜/id＞ ＜name＞one of the central mirrors in China ＜/name＞＜url＞ http://maven.net.cn/content/groups/public/ ＜/url＞ ＜mirrorOf＞central＜/mirrorOf＞ ＜/mirror＞ ＜/mirrors＞ …… ＜/settings＞ 配置私服作为镜像12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;maven.oschina.net&lt;/id&gt; &lt;name&gt;maven mirror in China&lt;/name&gt; &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 仓库搜索服务以下网站提供 Maven 仓库搜索功能。 Sonatype Nexus地址：http://repository.sonatype.org/ MVNrepository地址：http://mvnrepository.com/ 一般我就用最后一个搜索。 Maven 坐标现在有了仓库统一保管这些 jar 包，剩下的问题就是怎么取了。 不知道你有没有取快递的经验。我们可以这些 jar 包想象成是快递，仓库中保管着这些快递。我们去认领快递需要依靠快递单来确定，一张快递单上会有单号、我们的姓名、手机号等信息。依靠这些信息就不会领错快递了。 这里的快递单就像 Maven 中的 pom 文件，单子上的信息就像是 pom 文件中的坐标系。 Maven 项目规定的项目结构是这样的： src/main/java —— 存放项目的.java文件 src/main/resources —— 存放项目资源文件，如spring, hibernate配置文件 src/test/java —— 存放所有测试.java文件，如JUnit测试类 src/test/resources —— 测试资源文件 target —— 项目输出位置 pom.xml——maven项目核心配置文件 每个 maven 项目都有 pom.xml 文件。Maven坐标为各种构件引入了秩序，任何一个构件都必须明确定义自己的坐标。 一组 Maven坐标是通过一些元素定义的，它们是 groupId、artifactId、version、packaging、 classifier： groupId：定义当前Maven项目隶属的实际项目，通常为域名反写 artifactId：该元素定义实际项目中的一个 Maven项目（模块），推荐的做法是使用实际项目名称作为artifactId的前缀。 version：该元素定义Maven项目当前所处的 版本， packaging：该元素定义Maven项目的打包方 式。当不定义packaging的时候，Maven会使用默认值jar。 classifier：该元素用来帮助定义构建输出的一些附属构件。 通过坐标系我们来保证项目在 Maven 仓库中的唯一性，每次取也不会取错了。 Maven 依赖我们自己项目需要用别人的 jar 包，比如 spring。这就是我们的项目依赖于 spring，因此我们通过 pom 来配置这样的依赖关系，这样就能让项目有清晰的结构。 依赖的关系用用&lt;dependecy&gt;标签来表示依赖： 上图说明该项目依赖了 hibernate 等 依赖范围现在来考虑一种情况，我们在项目开发的过程中用到了 junit 进行测试，也就是说我们的项目依赖于 junit。在项目构建的过程中我们会把 junit 也打包在项目中。但是在生产环境中完全没有必要用到 junit，我们并不想将它发布到生产环境中。 我们可以每次在发布项目之前把他删除了对么？那如果依赖 servlet-api，我们只有在编译和测试项目的时候需要该依赖，但在运行项目的时候，由于容器已经提供，也不需要 Maven 重复地引入一遍。 所以最好是在编译、测试、运行的过程中需要用到什么 jar 包，就让 Maven 去打包什么。 maven 为此提供了scope标签表示依赖范围，表示该 jar 包在什么时候需要被使用。 compile：编译依赖范围，使用此依赖范围对于编译、测试、运行三种classpath都有效，即在编译、测试和运行时都要使用该依赖jar包； test：测试依赖范围，只对测试有效，表明只在测试的时候需要，在编译和运行时将无法使用该类依赖，如 junit； provided：已提供依赖范围。编译和测试有效，运行无效。如servlet-api，在项目运行时，tomcat等容器已经提供，无需Maven重复引入； runtime：运行时依赖范围。测试和运行有效，编译无效。如 jdbc 驱动实现，编译时只需接口，测试或运行时才需要具体的 jdbc 驱动实现； system：系统依赖范围，使用system范围的依赖时必须通过systemPath元素显示地指定依赖文件的路径，不依赖Maven仓库解析，所以可能会造成建构的不可移植，谨慎使用。 依赖传递依赖范围除了控制classpath，还会对依赖传递产生影响。如果A依赖B，B依赖C，则A对于B是第一直接依赖。B对于C是第二直接依赖。A对于C是传递性依赖。结论是：第一直接依赖的范围和第二直接依赖的范围决定了传递性依赖的范围。 第一列是第一直接依赖，第一行是第二直接依赖，中间表示传递性依赖范围。 此外 maven 还提供了option和exclusions来进一步管理依赖，分别称为可选依赖和排除依赖。 在依赖中添加 true/false 表示是否向下传递。 如上图所示，B 依赖于 X,Y 而 A 依赖于 B，如果 B 不希望将依赖传递给 A 则可以配置 B 中的 X,Y 依赖的optional为 true 来阻止依赖的传递。 再来看一种情况，A 依赖于 B，且 B 将他的依赖 C 传递给了 A。但是 A 依赖了 C 的另一个版本。这个时候 A 可以主动排除 B 给的 C 依赖，转而使用自己需要的版本，这就用到了exclusions标签。 用exclusions元素声明排除依赖，exclusions可以包 含一个或者多个exclusion子元素，因此可以排除一个或者多个传递性依赖。 所以我用主动和被动的方式来区分他们。 依赖冲突接上面的问题，如果 A 和 B 依赖 C 的不同版本，而且既没有配置可选依赖也没有配置排除依赖。两个版本都被解析显然是不对的，因为那会造成依赖重复，因此必须选择一个。 路径最近者优先。如果直接与间接依赖中包含有同一个坐标不同版本的资源依赖，以直接依赖的版本为准。 第一声明者优先。在依赖路径长度相等的前 提下，在POM中依赖声明的顺序决定了谁会被解析使用，顺序最靠前的那个依赖优胜。 上面例子中,A -&gt; C(1.10) 和 A -&gt; B -&gt; C(?),C(1.10)的路径短所以用它。 Maven 生命周期Maven的生命周期就是为了对所有的构建过程进行抽象和统一。这个生命周期包含了项目的 清理、初始化、编译、测试、打包、集成测试、 验证、部署和站点生成等几乎所有构建步骤。 初学者往往会以为Maven的生命周期是一个整体，其实不然。Maven拥有三套相互独立的生命周期，它们分别为clean、default和site。clean生命周期的目的是清理项目，default生命周期的目的是构建项目，而site生命周期的目的是建立项目站点。 clean 生命周期。clean生命周期的目的是清理项目，它包含三个阶段： pre-clean 执行一些需要在clean之前完成的工作 clean 移除所有上一次构建生成的文件 post-clean 执行一些需要在clean之后立刻完成的工作 mvn clean 中的clean就是上面的clean，在一个生命周期中，运行某个阶段的时候，它之前的所有阶段都会被运行，也就是说，mvn clean 等同于 mvn pre-clean clean ，如果我们运行 mvn post-clean ，那么 pre-clean，clean 都会被运行。这是Maven很重要的一个规则，可以大大简化命令行的输入。 default生命周期default生命周期定义了真正构建时所需要执 行的所有步骤，它是所有生命周期中最核心的部分，其包含的阶段如下： validate generate-sources process-sources generate-resources process-resources 复制并处理资源文件，至目标目录，准备打包。 compile 编译项目的源代码。 process-classes generate-test-sources process-test-sources generate-test-resources process-test-resources 复制并处理资源文件，至目标测试目录。 test-compile 编译测试源代码。 process-test-classes test 使用合适的单元测试框架运行测试。这些测试代码不会被打包或部署。 prepare-package package 接受编译好的代码，打包成可发布的格式，如 JAR 。 pre-integration-test integration-test post-integration-test verify install 将包安装至本地仓库，以让其它项目依赖。 deploy 将最终的包复制到远程的仓库，以让其它开发人员与项目共享。 运行任何一个阶段的时候，它前面的所有阶段都会被运行，这也就是为什么我们运行mvn install 的时候，代码会被编译，测试，打包。此外，Maven的插件机制是完全依赖Maven的生命周期的，因此理解生命周期至关重要。 site生命周期site生命周期的目的是建立和发布项目站点，Maven能够基于POM所包含的信息，自动生成一个友好的站点，方便团队交流和发布项目信息。 pre-site 执行一些需要在生成站点文档之前完成的工作site 生成项目的站点文档 post-site 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备 site-deploy 将生成的站点文档部署到特定的服务器上 这里经常用到的是site阶段和site-deploy阶段，用以生成和发布Maven站点，这可是Maven相当强大的功能，Manager比较喜欢，文档及统计数据自动生成，很好看。 命令与生命周期mvn clean：该命令调用clean生命周期的clean阶段。实际执行的阶段为clean生命周期的 pre-clean和clean阶段。 mvn test：该命令调用default生命周期的test阶段。实际执行的阶段为default生命周期的 validate、initialize等，直到test的所有阶段。这也解释了为什么在执行测试的时候，项目的代码能够自动得以编译。 mvn clean install：该命令调用clean生命周期 的clean阶段和default生命周期的install阶段。 mvn clean deploy site-deploy：该命令调用 clean生命周期的clean阶段、default生命周期的 deploy阶段，以及site生命周期的site-deploy阶段。 聚合与继承软件设计人员往往会采用各种方式对软件划分模块，以得到更清晰的设计及更高的重用性。当把Maven应用到实际项目中的时候，也需要将项目分成不同的模块。 简单的说就是有 A,B 两个模块，现在想要将他们统一管理。Maven 的聚合特性能够把项目的各个模块聚合在一起构建，而Maven的继承特性则能帮助抽取各模块相同的依赖和插件等配置，在简化POM的同时，还能促进各个模块配置的一致性。 聚合两个子模块希望同时构建。这时，一个简单的需求就会自然而然地显现出来：我们会想要一次构建两个项目，而不是到两个模块的目录下分别执行mvn命令。Maven聚合（或者称为多模块）这一特性就是为该需求服务的。 上图所示api是一个模块，cmd是一个模块他们都有各自的 pom 文件，其实每一个包都是一个子模块，而最底下的 pom 文件则是统一管理这些子模块。 他们的配置很简单，我们最好遵循规范。 api 的 pom.xml 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;groupId&gt;com.shuiyujie.fu&lt;/groupId&gt; &lt;artifactId&gt;sop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; ... cmd 的 pom.xml 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;groupId&gt;com.shuiyujie.fu&lt;/groupId&gt; &lt;artifactId&gt;sop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;cmd&lt;/artifactId&gt; 聚合 pom.xml 123456789101112131415161718192021&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.shuiyujie.fu&lt;/groupId&gt; &lt;artifactId&gt;pom&lt;/artifactId&gt; &lt;packaging&gt;sop&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;sop&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; ... &lt;modules&gt; &lt;module&gt;base&lt;/module&gt; &lt;module&gt;core&lt;/module&gt; &lt;module&gt;server&lt;/module&gt; &lt;module&gt;persist&lt;/module&gt; &lt;module&gt;api&lt;/module&gt; &lt;module&gt;impl&lt;/module&gt; &lt;module&gt;cmd&lt;/module&gt; &lt;/modules&gt; ... 观察上面的三个代码清单可以聚合 pom 文件中定义了 &lt;modules&gt; 标签，标签中包含的就是各个子模块，并且用子模块的artifactId来标记他们。 注意：聚合 pom 文件的打包方式，即 packaging 必须为 pom。 这样只需要构建聚合 pom 文件即可同时构建在其管理下的多个子模块。 继承消除重复。在面向对象世界中，程序员可以使用类继承在一定程度上消除重复，在Maven的世界 中，也有类似的机制能让我们抽取出重复的配 置，这就是POM的继承。 任然看上面的三个 pom.xml 代码清单，子模块都有一个parent标签，这就表明他们继承了一个 pom 文件，而parent标签下的其他标签就是一个坐标系，通过一个坐标系就能定位一个唯一的项目。 比如上面的子模块继承自聚合 pom 文件，所以此时聚合 pom 文件也是父类 pom 文件。 排除父类的依赖在继承的过程中我们考虑一种情形，我们希望在父类中统一控制 spring 的版本，然后子类继承自父类就可以使用统一版本的 spring 依赖了。但是有些子模块不需要依赖 spring，并不需要从父类继承 spring 的依赖。 我们可以使用dependencyManagement 标签。 父类 pom.xml 123456789101112131415161718192021222324252627282930313233343536&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 模块间依赖 start --&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;core&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;persist&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;impl&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;server&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;cmd&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 模块间依赖 end --&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 父类dependencyManagement中声明了各个子模块，子模块之间有的会需要相互引用，有的却并不需要。所以在父类中统一配置各个子模块的groupId,artifactId,version等基本信息。 在dependencyManagement中声明的依赖不会在当前pom中引入依赖，也不会再继承他的pom中引入依赖，他的作用只是声明了可能要引入依赖的一些通用信息。 如果要使用一个子模块要使用其他子模块就可以另外声明，但是不需要指定版本等通用信息，这样就可以减少依赖冲突的发生，代码如下： 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;groupId&gt;com.shuiyujie.fu&lt;/groupId&gt; &lt;artifactId&gt;sop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;api&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;project.parent.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;fu-persist&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; Maven常用命令1234567891011121314151617181920212223mvn -v //查看版本 mvn archetype:create //创建 Maven 项目 mvn compile //编译源代码 mvn test-compile //编译测试代码 mvn test //运行应用程序中的单元测试 mvn site //生成项目相关信息的网站 mvn package //依据项目生成 jar 文件 mvn install //在本地 Repository 中安装 jar mvn -Dmaven.test.skip=true //忽略测试文档编译 mvn clean //清除目标目录中的生成结果 mvn clean compile //将.java类编译为.class文件 mvn clean package //进行打包 mvn clean test //执行单元测试 mvn clean deploy //部署到版本仓库 mvn clean install //使其他项目使用这个jar,会安装到maven本地仓库中 mvn archetype:generate //创建项目架构 mvn dependency:list //查看已解析依赖 mvn dependency:tree //看到依赖树 mvn dependency:analyze //查看依赖的工具 mvn help:system //从中央仓库下载文件至本地仓库 mvn help:active-profiles //查看当前激活的profiles mvn help:all-profiles //查看所有profiles mvn help:effective -pom //查看完整的pom信息 总结：Maven 的思想Maven 的核心思想是约定优于配置。 首先，Maven 约定了项目的结构，我们不需要配置 Maven 编译、打包等操作时文件的位置。统一的项目结构降低了学习的成本，让我能将精力集中到了项目本身。 其次，Maven 抽象了项目构建的过程，将其分成一个个生命周期进行管理。通过命令和插件的形式进一步简化操作，又让我们从繁琐的操作解放出来。 参考《Maven 实战》 Maven远程仓库的各种配置 本文大部分内容来自于《Maven 实战》一书，想要了解一手信息强烈建议阅读。网上的其他文章基本上都是摘抄《Maven 实战》的部分内容。 所以还想说一遍：发现一本好书就像发现了一座宝藏。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"maven","slug":"maven","permalink":"http://shuiyujie.com/tags/maven/"}]},{"title":"使用Nexus配置Maven私有仓库","slug":"Java/使用Nexus配置Maven私有仓库","date":"2017-12-04T04:11:13.000Z","updated":"2019-10-06T02:06:14.938Z","comments":true,"path":"post/8dc6b8e4.html","link":"","permalink":"http://shuiyujie.com/post/8dc6b8e4.html","excerpt":"Nexus 是Maven仓库管理器，如果你使用Maven，你可以从Maven中央仓库 下载所需要的构件（artifact），但这通常不是一个好的做法，你应该在本地架设一个Maven仓库服务器，在代理远程仓库的同时维护本地仓库，以节省带宽和时间，Nexus就可以满足这样的需要。此外，他还提供了强大的仓库管理功能，构件搜索功能，它基于REST，友好的UI是一个extjs的REST客户端，它占用较少的内存，基于简单文件系统而非数据库。这些优点使其日趋成为最流行的Maven仓库管理器","text":"Nexus 是Maven仓库管理器，如果你使用Maven，你可以从Maven中央仓库 下载所需要的构件（artifact），但这通常不是一个好的做法，你应该在本地架设一个Maven仓库服务器，在代理远程仓库的同时维护本地仓库，以节省带宽和时间，Nexus就可以满足这样的需要。此外，他还提供了强大的仓库管理功能，构件搜索功能，它基于REST，友好的UI是一个extjs的REST客户端，它占用较少的内存，基于简单文件系统而非数据库。这些优点使其日趋成为最流行的Maven仓库管理器 安装配置Nexus 下载nexus https://www.sonatype.com/download-oss-sonatype 解压：tar -zxf nexus-3.5.2-01-unix.tar.gz 进入bin目录启动：./nexus run &amp; 出现如下界面启动成功 12345\\-------------------------------------------------Started Sonatype Nexus OSS 3.5.2-01\\------------------------------------------------- 访问http://10.211.55.7:8081/可以登录 默认端口号：8081 默认账号：admin 默认密码：admin123 配置修改123456789101112131415161718# 修改运行nexus3所使用的用户：[root@bigdata1 bin]#vi nexus.rcrun_as_user=”root”# 修改nexus3启动所使用的jdk版本[root@bigdata1 bin]#vi nexusINSTALL4J_JAVA_HOME_OVERRIDE=/data/program/software/java8# 修改nexus3默认端口[root@bigdata1 etc]# vi nexus-default.propertiesapplication-port=8282#修改nexus3数据以及相关日志的存储位置[root@bigdata1 etc]# vi nexus.vmoptions -XX:LogFile=./sonatype-work/nexus3/log/jvm.log -Dkaraf.data=./sonatype-work/nexus3 -Djava.io.tmpdir=./sonatype-work/nexus3/tmp 修改settings.xml配置，使用nexus私有库1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;localRepository&gt;$&#123;user.home&#125;/.m2/repository&lt;/localRepository&gt; &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; &lt;offline&gt;false&lt;/offline&gt; &lt;pluginGroups&gt; &lt;pluginGroup&gt;org.mortbay.jetty&lt;/pluginGroup&gt; &lt;pluginGroup&gt;org.jenkins-ci.tools&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt;这是Server的ID(不是登录进来的user)，与Maven想要连接上的repository/mirror中的id元素相匹配。username，password：这两个元素成对出现，表示连接这个server需要验证username和password。在nexus中，默认管理员用户名为admin，密码为admin123。这里使用两个服务器配置，分别对应release和snapshot。 &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://10.211.55.7:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://10.211.55.7:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; id，name：唯一的镜像标识和用户友好的镜像名称。id被用来区分mirror元素，并且当连接时候被用来获得相应的证书。 mirrorOf：镜像所包含的仓库的Id。例如，指向Maven central仓库的镜像(http://repo1.maven.org/maven2/)，设置这个元素为central。更多的高级映射例如repo1,repo2 或者,!inhouse都是可以的。没必要一定和mirror的id相匹配。在这里mirrorOf项当然应该使用，以表明是所有仓库都会被镜像到指定的地址。 url：镜像基本的URL，构建系统将使用这个URL来连接仓库。这里应该添nexus仓库的地址，地址可以在nexus仓库页面中找到。 123456789101112131415161718192021222324252627282930313233&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://nexus-releases&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://nexus-snapshots&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;url&gt;http://nexus-releases&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus-snapshots&lt;/id&gt; &lt;url&gt;http://nexus-snapshots&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; profile项代表maven的基本配置。按照maven的一贯尿性，很多xml的配置项都会有一个配置项的复数形式作为父节点，以保证该配置项可以配置多个。在profiles项中，当然也可以配置多个profile，不过在这里配一个就够了。下面介绍profile项的各个子节点。 id：用来确定该profile的唯一标识。 repositories/repository：用以规定依赖包仓库的相关信息。在下属节点中，id就不用多说了；URL是指仓库地址，这里使用伪造的地址，否则即使设置了mirror，maven也有可能会直接从中央仓库下载包；releases和snapshots放在一块说吧，这两个节点下属的enable节点用以规定对应的依赖包是否对当前策略有效，假如将snapshot的enable项设为disable，则不会下载snapshot包。 1234&lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/settings&gt; 用以规定当前启用的配置，将对应profile的ID加入到这一项即可使profile生效。 上传jar到nexus第一种方式： 1mvn deploy:deploy-file -DgroupId=com.alibaba -DartifactId=dubbo -Dversion=2.8.4 -Dpackaging=jar -Dfile=/Users/zhangyong/Documents/software/dubbo-2.8.4.jar -Durl=http://10.211.55.7:8081/repository/maven-releases/-DrepositoryId=nexus-releases DrepositoryId和settings.xml里配置的id一样 第二种方式： 代码的pom.xml中直接接入 1234567&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;maven-releases&lt;/name&gt; &lt;url&gt;http://10.211.55.7:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt; mvn deploy","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://shuiyujie.com/tags/maven/"}]},{"title":"Java 反射和动态代理","slug":"Java/JavaSE/Java-反射和动态代理","date":"2017-11-26T15:08:39.000Z","updated":"2019-05-14T13:51:28.488Z","comments":true,"path":"post/e0acc964.html","link":"","permalink":"http://shuiyujie.com/post/e0acc964.html","excerpt":"反射提供了一种机制——用来检测可用的方法，并返回方法名。通过类名我们就能获取 Class 对象，并通过 Class 对象获取对象相关的一些属性和方法等。 最常见到的地方就是在各种配置文件中的应用，它的使用就像传入一个字符串，然后字符串去获取了相应的对象。","text":"反射提供了一种机制——用来检测可用的方法，并返回方法名。通过类名我们就能获取 Class 对象，并通过 Class 对象获取对象相关的一些属性和方法等。 最常见到的地方就是在各种配置文件中的应用，它的使用就像传入一个字符串，然后字符串去获取了相应的对象。 反射在了解反射之前要知道组成 Java 程序的是一个个类，每一个类都有一个 Class 对象，他对应一个.class文件。 所有的类都是在被第一次被调用的时候加载到 JVM 中的，并且一个类的也不是一次性加载完成，是在用到某一部分的时候才加载的。比如一个类被加载的时候一定会先加载静态变量，所以我们把常量定义为静态的。 注：以上内容整理自《Java 编程思想》。 我们用反射的第一步就是要获取Class 对象，再去获取他的其他属性。可以使用Class.forName()方法获取。 123456789101112131415161718public class MyReflect &#123; public String className = null; @SuppressWarnings(\"rawtypes\") public Class dogClass = null; /** * 反射获取 Dog 类 * * @throws ClassNotFoundException */ @Before public void init() throws ClassNotFoundException &#123; className = \"com.shuiyujie.reflection.Dog\"; dogClass = Class.forName(className); &#125; 获取Class 对象之后，我们可以通过它创建一个实例对象 12345678910/** * 获得对象实例，调用无参构造 * * @throws IllegalAccessException * @throws InstantiationException */ @Test public void newInstance() throws IllegalAccessException, InstantiationException &#123; System.out.println(dogClass.newInstance()); &#125; 还可以通过它获取对象的构造方法、成员变量、成员方法等，方法又有公有和非公有之分，非公有的如果我们要强制获取要消除他的类型校验，这就是反射 API 要知道的大部分内容了，示例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 获取公共的成员变量 * @throws Exception */ @SuppressWarnings(&#123; \"rawtypes\", \"unchecked\" &#125;) @Test public void getPublicParams() throws Exception &#123; Constructor constructor = dogClass.getConstructor(Long.class,String.class); Object obj = constructor.newInstance(1L, \"Lucky\"); Field field = dogClass.getField(\"name\"); field.set(obj,\"Lucy\"); System.out.println(field.get(obj)); &#125; /** * 获取非公共的成员变量 * @throws Exception */ @SuppressWarnings(&#123; \"rawtypes\", \"unchecked\" &#125;) @Test public void getUnPublicParams() throws Exception &#123; Constructor constructor = dogClass.getConstructor(Long.class,String.class); Object obj = constructor.newInstance(1L, \"Lucky\"); Field field = dogClass.getDeclaredField(\"id\"); field.setAccessible(true); field.set(obj,3L); System.out.println(field.get(obj)); &#125; /** * 获取公共的成员方法 * @throws Exception */ @SuppressWarnings(&#123; \"rawtypes\", \"unchecked\" &#125;) @Test public void getPublicMethod() throws Exception &#123; System.out.println(dogClass.getMethod(\"toString\")); Object obj = dogClass.newInstance(); Method method = dogClass.getMethod(\"brak\"); method.invoke(obj); &#125; /** * 获取非公共的成员方法 * @throws Exception */ @SuppressWarnings(&#123; \"rawtypes\", \"unchecked\" &#125;) @Test public void getUnPublicMethod() throws Exception &#123; Object obj = dogClass.newInstance(); Method method = dogClass.getDeclaredMethod(\"dogBrak\"); method.setAccessible(true); method.invoke(obj); &#125; 此外他还有一些其他方法示例，更多可以参考 Class 类的 API 123456789101112131415161718192021@Test public void otherMethod() throws ClassNotFoundException &#123; //当前加载这个class文件的那个类加载器对象 System.out.println(dogClass.getClassLoader()); //获取某个类实现的所有接口 Class[] interfaces = dogClass.getInterfaces(); for (Class class1 : interfaces) &#123; System.out.println(class1); &#125; //反射当前这个类的直接父类 System.out.println(dogClass.getGenericSuperclass()); //判断当前的Class对象表示是否是数组 System.out.println(dogClass.isArray()); System.out.println(new String[3].getClass().isArray()); //判断当前的Class对象表示是否是枚举类 System.out.println(dogClass.isEnum()); System.out.println(Class.forName(\"com.shuiyujie.reflection.Animal\").isEnum()); //判断当前的Class对象表示是否是接口 System.out.println(dogClass.isInterface()); System.out.println(Class.forName(\"com.shuiyujie.reflection.AnimalInterface\").isInterface()); &#125; 动态代理动态代理指的是在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。实现动态代理的核心就是反射机制。 用一个场景来描述动态代理。比如一个订单系统，有一个生成订单的 service，当我们要搞活动进行促销的时候，生成订单的价格根据活动降低价格。 修改原有的生成订单的方法无疑是不可行的，因为系统的其他模块可能也调用了这个方法，甚至活动有多种的优惠方式只改一个也不够。这里就可以用动态代理来代理订单生成的方法。 代理我们也可以叫他中介，比如房产中介就是在客户之间起桥梁作用，动态代理就是在调用发和被调用方中间介入。 首先要有订单的接口和实现： 123456789101112public interface IOrderService &#123; int getClothOrder(String size);&#125;public class OrderServiceImpl implements IOrderService &#123; private int clothPrize = 500; @Override public int getClothOrder(String size) &#123; System.out.println(\"衣服的尺码为\" + size); return clothPrize; &#125;&#125; 用一个 Controller 调用他获得价格： 1234567public class OrderController &#123; @Test public void getOrder()&#123; IOrderService orderService = new OrderServiceImpl(); int price = orderService.getClothOrder(\"L\"); System.out.println(\"价格为\" + price + \"元\"); &#125; 输出： 12衣服的尺码为L价格为500元 在IOrderService和OrderServiceImpl不变的情况下通过IOrderService来实现打折的效果，就要用到动态代理了。 动态代理类： 123456789101112public class ProxyCharge &#123; public static &lt;T&gt; T getProxy(final int discountCoupon, final Class&lt;?&gt; interfaceClass, final Class&lt;?&gt; implementsClass) throws Exception &#123; return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class[]&#123;interfaceClass&#125;, (proxy, method, args) -&gt; &#123; Integer returnValue = (Integer) method.invoke( implementsClass.newInstance(), args); return returnValue - discountCoupon; &#125;); &#125;&#125; 关键是通过Proxy.newProxyInstance()获取一个原来OrderServiceImpl的代理对象。我们需要传入借口对象和实现对象。 获取代理对象之后可以使用反射中的invoke()调用原先方法，也能进行自己扩展。 代理方法调用： 123456@Testpublic void getChargeOrder() throws Exception &#123; IOrderService proxyService = ProxyCharge.getProxy(200,IOrderService.class,OrderServiceImpl.class); int price = proxyService.getClothOrder(\"L\"); System.out.println(\"价格为\" + price + \"元\");&#125; 输出： 12衣服的尺码为L价格为300元 总结：反射允许更加动态的编程风格，动态代理可以动态地获取代理对象并动态地处理对所代理对象的调用。 动态代理听上去很复杂但是他的实现方式是基本固定的，以上的示例代码是最简单的实现。同时动态代理是实现 RPC 框架不可或缺的一部分，现在温习一下为实现一个 RPC 框架做准备。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"}]},{"title":"正则表达式","slug":"Java/正则表达式","date":"2017-11-26T11:09:09.000Z","updated":"2019-09-22T14:46:42.634Z","comments":true,"path":"post/2f57a694.html","link":"","permalink":"http://shuiyujie.com/post/2f57a694.html","excerpt":"","text":"最全的常用正则表达式大全——包括校验数字、字符、一些特殊的需求等等 知道这20个正则表达式，能让你少写1,000行代码","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"http://shuiyujie.com/tags/正则表达式/"}]},{"title":"mybatis 配置和使用","slug":"Java/mybatis-配置和使用","date":"2017-11-25T11:09:09.000Z","updated":"2019-05-14T13:51:36.107Z","comments":true,"path":"post/5ed7ab06.html","link":"","permalink":"http://shuiyujie.com/post/5ed7ab06.html","excerpt":"MyBatis 是一款一流的支持自定义 SQL、存储过程和高级映射的持久化框架。MyBatis 几乎消除了所有的 JDBC 代码，也基本不需要手工去设置参数和获取检索结果。MyBatis 能够使用简单的 XML 格式或者注解进行来配置，能够映射基本数据元素、Map 接口和 POJOs（普通 java 对象）到数据库中的记录。 简而言之：Mybatis 是一个半自动化的持久化框架，帮助我们简化对数据库的操作。","text":"MyBatis 是一款一流的支持自定义 SQL、存储过程和高级映射的持久化框架。MyBatis 几乎消除了所有的 JDBC 代码，也基本不需要手工去设置参数和获取检索结果。MyBatis 能够使用简单的 XML 格式或者注解进行来配置，能够映射基本数据元素、Map 接口和 POJOs（普通 java 对象）到数据库中的记录。 简而言之：Mybatis 是一个半自动化的持久化框架，帮助我们简化对数据库的操作。 封装 SqlSessionMybatis 使用 SqlSession 调用他封装的一系列方法，而 SqlSession 则交由 SqlSessionFactory 进行管理。 注入 SqlSession第一步就是配置 SqlSessionFactory 和 SqlSession。用 spring 和容易就能实现，在 spring 的配置文件mybatis-spring.xml中注入 bean: 123456789&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot; /&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath*:mapper/*.xml&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;sqlSession&quot; class=&quot;org.mybatis.spring.SqlSessionTemplate&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;sqlSessionFactory&quot; /&gt; &lt;/bean&gt; 其中configLocation指向 mybatis 的配置文件，我们使用配置文件可以配置别名，别名可以用来指向一个实体类： 123456&lt;configuration&gt; &lt;typeAliases&gt; &lt;!-- 指定包名 --&gt; &lt;package name=&quot;com.shuiyujie.persist&quot;/&gt; &lt;/typeAliases&gt;&lt;/configuration&gt; 12345&lt;configuration&gt; &lt;typeAliases&gt; &lt;!-- 指定类名 --&gt; &lt;typeAlias alias=&quot;city&quot; type=&quot;com.shuiyujie.persist.City&quot;/&gt;&lt;/configuration&gt; 其中mapperLocations指定了 mybatis 的 Mapper 文件，配置之后会扫描指定包下的 XML 文件。dataSource指向配置的数据源。 封装 SqlSessionSqlSession 可以调用 mybatis 为我们封装的一系列增删改查操作，具体可以看参考文档。 我们可以将它封装一下变得更加友好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101@Componentpublic class DaoRouter implements IDaoRouter&#123; /** * 普通查询 * @param statement sql语句定义的id * @param parameters 参数 * @return */ public &lt;T&gt; List&lt;T&gt; query(String statement, Object parameters) &#123; return sqlSession.selectList(statement, parameters); &#125; /** * 分页查询，注意此处不需要在sql语句中定义startRow和endRow，如果定义，那就用上面的普通查询即可。 * @param statement * @param parameters * @param offset * @param limit * @return */ public &lt;T&gt; List&lt;T&gt; query(String statement, Object parameters, int offset, int limit) &#123; return sqlSession.selectList(statement, parameters,new RowBounds(offset,limit)); &#125; /** * 返回第一条记录查询结果，如果没有就返回null * @param statementName * @param parameters * @return */ public &lt;T&gt; T queryForObject(String statementName, Object parameters) &#123; return sqlSession.selectOne(statementName, parameters); &#125; /** * 更新数据 * @param statement * @param parameters * @return */ public int update(String statement, Object parameters) &#123; return sqlSession.update(statement, parameters); &#125; /** * 删除 * @param statement * @param parameters * @return */ public int delete(String statement, Object parameters) &#123; return sqlSession.delete(statement, parameters); &#125; /** * 新增数据，建议在定义sql语句的时候，使用selectKey保证可以返回新增后的主键 * @param statement * @param parameters * @return */ public int insert(String statement, Object parameters) &#123; return sqlSession.insert(statement, parameters); &#125; /** * 批量删除 * @param statementName * @param dataList */ public void deleteBatch(String statementName, Collection&lt;?&gt; dataList) &#123; for (Object data : dataList) &#123; this.delete(statementName, data); &#125; &#125; /** * 批量新增 * @param statementName * @param dataList */ public void insertBatch(String statementName, Collection&lt;?&gt; dataList) &#123; for (Object data : dataList) &#123; this.insert(statementName, data); &#125; &#125; /** * 批量修改 * @param statementName * @param dataList */ public void updateBatch(String statementName, Collection&lt;?&gt; dataList) &#123; for (Object data : dataList) &#123; this.update(statementName, data); &#125; &#125; @Autowired private SqlSession sqlSession;&#125; 查询首先创建两个实体类省和市，他们之间为一对多的关系。 123456789public class Province implements Serializable&#123; private Long id; private String name; private String description; private List&lt;City&gt; cityList; 1234567891011public class City implements Serializable&#123; private Long id; private Long provinceId; private String cityName; private String description; private Province province; 一对一查询现在要查询一个城市，同时查询出该城市所在的省。也就是获取 City 对象的时候，同时返回他的 province 成员变量。我们要使用resultMap中的association元素。 采用关联查询的方式，sql 语句如下： 123select * from city c LEFT JOIN province p ON c.province_id = p.id 这种情况下有可能是关联查询一整个对象，比如 province 的所有变量；也有可能只想查询 province 的少数变量，比如说查个名字。 提供有两种解决方案。 association 关联一个对象查询关联查询时在查出 city 表中的结果集时也会查出 province 表的结果集，得到两个结果集之后要做的事情就是将他们各自映射到各自的对象中，mybatis 采用在resultMap中指定association的方式来实现以上效果。 注：字段名出现重复的现象可以为其指定唯一的别名 City.java 123456789101112131415public class City implements Serializable &#123; @Id @GeneratedValue private Long id; @Column(nullable = false) private Long provinceId; @Column(nullable = false) private String cityName; private String description; private Province province; private Province province;在 Ctiy 中加一个 Province 对象，查询 City 时也查询 Province。 CityMapper.xml 123456789101112131415 &lt;resultMap id=\"BaseResultMap\" type=\"city\"&gt; &lt;result column=\"id\" property=\"id\" /&gt; &lt;result column=\"province_id\" property=\"provinceId\" /&gt; &lt;result column=\"city_name\" property=\"cityName\" /&gt; &lt;result column=\"description\" property=\"description\" /&gt; &lt;association property=\"province\" column=\"province_id\" javaType=\"province\" resultMap=\"Province.provinceResult\"/&gt; &lt;/resultMap&gt;&lt;select id=\"loadCity\" resultMap=\"BaseResultMap\" &gt; select * from city c LEFT JOIN province p ON c.province_id = p.id &lt;/select&gt; ProvinceMapper.xml 1234&lt;select id=\"loadProvinceAndCity\" resultMap=\"provinceResult\" parameterType=\"Long\"&gt; SELECT * FROM province WHERE id = #&#123;id&#125; &lt;/select&gt; 重点来看CityMapper.xml的association property：表示变零名称，即 City 对象中的 province 属性 column：关联的字段 javaType:关联对象的类型 resultMap:关联对象的结果集 扩展 ResultMap 来扩展字段这次不查整个 Province 只是查询 Province 的一个 name，可以不用association。 CityVO.java 12public class CityVO extends City &#123; private String provinceName; 给City.java一个包装类，添加成员变量 provinceName。 扩展 ResultMap: 12345678910&lt;resultMap id=\"cityResultMap\" type=\"city\"&gt; &lt;result column=\"id\" property=\"id\" /&gt; &lt;result column=\"province_id\" property=\"provinceId\" /&gt; &lt;result column=\"city_name\" property=\"cityName\" /&gt; &lt;result column=\"description\" property=\"description\" /&gt; &lt;/resultMap&gt; &lt;resultMap id=\"cityProvinceResultMap\" extends=\"cityResultMap\" type=\"cityVO\"&gt; &lt;result column=\"name\" property=\"provinceName\" /&gt; &lt;/resultMap&gt; 注意cityProvinceResultMap: extends:表示继承关系，是被继承 ResultMap 的扩展 column: sql 语句查询出来的需要映射的字段名称 property: 实体类中对应的成员变量的名称 XML 查询语句: 12345&lt;select id=\"loadCityAndProvinceName\" resultMap=\"cityProvinceResultMap\" &gt; select c.*,p.name from city c LEFT JOIN province p ON c.province_id = p.id &lt;/select&gt; 集合嵌套查询现在要在查询省的时候，返回该省下市的信息。也就是获取 Province 对象的时候，同时返回他的 cityList 成员变量。我们要使用resultMap中的collection元素。 ProvinceMapper.xml 1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"Province\"&gt; &lt;resultMap id=\"provinceResult\" type=\"province\"&gt; &lt;result column=\"id\" property=\"id\"/&gt; &lt;result column=\"name\" property=\"name\"/&gt; &lt;result column=\"description\" property=\"description\"/&gt; &lt;collection property=\"cityList\" javaType=\"ArrayList\" column=\"id\" ofType=\"city\" select=\"City.loadCityForProvince\"/&gt; &lt;/resultMap&gt; &lt;select id=\"loadProvinceAndCity\" resultMap=\"provinceResult\" parameterType=\"Long\"&gt; SELECT * FROM province WHERE id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; CityMapper.xml 12345&lt;mapper namespace=\"City\"&gt; &lt;select id=\"loadCityForProvince\" resultType=\"city\"&gt; select * from city WHERE province_id = #&#123;id&#125;; &lt;/select&gt; 重点关注ProvinceMapper.xml的collection部分，他表示返回集合类型 12&lt;collection property=\"cityList\" javaType=\"ArrayList\" column=\"id\" ofType=\"city\" select=\"City.loadCityForProvince\"/&gt; property:匹配的属性名称，也就是Province中的cityList成员变量 javaType：集合的类型(可以省略) column:关联的字段 ofType:集合中包含元素的类型，这里用了别名 select：关联查询的方法 集合迭代器查询比如想查询几个省中的市，可以传入一个包含省数据的 list，然后使用 mybatis 迭代遍历这个 list 集合查询获取所有下属的市。 要用到一个动态 sql 的标签&lt;foreach&gt;,XML 如下： 12345678&lt;select id=\"loadCityByProvinceList\" resultType=\"city\"&gt; select * from city WHERE province_id IN &lt;foreach item=\"item\" index=\"index\" collection=\"list\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/select&gt; 插入插入设置主键自增12345678&lt;insert id=&quot;insertXXX&quot; parameterType=&quot;fileNameVO&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt; INSERT INTO file_name ( ... )VALUES( ... ) &lt;/insert&gt; useGeneratedKeys：是否自增 keyProperty：主键名称 注解方式调用 sql注解方式开发 12345package org.mybatis.example;public interface BlogMapper &#123; @Select(\"SELECT * FROM blog WHERE id = #&#123;id&#125;”) Blog selectBlog(int id); &#125; 简单的 sql 语句可以用注解，但是复杂的不适宜用注解开发的方式。一般采用 XML 方式进行开发，有必要知道有注解开发这种方式。","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"mybatis","slug":"mybatis","permalink":"http://shuiyujie.com/tags/mybatis/"}]},{"title":"Java 多线程基础知识","slug":"Java/Java 并发编程/【Java并发编程】Java-多线程基础知识","date":"2017-11-20T12:11:14.000Z","updated":"2019-09-11T14:59:25.191Z","comments":true,"path":"post/1d67cce0.html","link":"","permalink":"http://shuiyujie.com/post/1d67cce0.html","excerpt":"计算机发明之初，单个 CPU 在同一时间点只能执行单个任务，也就是单任务阶段。紧接着发展为多任务阶段，在单个 CPU 上能执行多个进程，但是此处的并行执行并不是指在同一时间执行多个任务，而是指由系统对进程进行调度轮流使用 CPU。 接着发展到现在的多线程阶段，一个程序内部能够运行多个线程，每个线程都可以被看做运行在一个 CPU 上，此时计算机真正做到了在同一时间点能够执行多个任务。","text":"计算机发明之初，单个 CPU 在同一时间点只能执行单个任务，也就是单任务阶段。紧接着发展为多任务阶段，在单个 CPU 上能执行多个进程，但是此处的并行执行并不是指在同一时间执行多个任务，而是指由系统对进程进行调度轮流使用 CPU。 接着发展到现在的多线程阶段，一个程序内部能够运行多个线程，每个线程都可以被看做运行在一个 CPU 上，此时计算机真正做到了在同一时间点能够执行多个任务。 进程和线程进程和线程怎么界定？一个进程包含着一个或多个线程。 当我们启动一个应用程序的时候，操作系统将加载这个应用程序并为当前程序开辟一块独立的内存空间，这片空间就专门用来负责这个程序的运行。这就是一个进程，我们可以把它想象成一个车间。 假设我们启动的程序是微信，我们可以使用微信和多个人聊天并且互不干扰。同时和不同的人聊天就是将进程划分成多个区域，每个区域就是一个线程。如果把进程比作车间，线程就像车间中的工人，工人们各司其职维持车间的正常运转。 多线程运行原理多线程就是在一个进程中开启多个线程，让多个线程去实现不同功能。多线程最常见的就是 GUI 程序，比如说使用 IDE 运行程序的同时我们还能用 IDE 继续写代码，这就用到了多线程。 多线程运行则是利用 CPU 在线程中做时间片切换。CPU 负责运行程序且一次只能运行一个程序，多线程的实现依靠 CPU 在线程间快速切换，由于切换的时间很快就像同时运行多个线程一样。 多线程切换过程中，它需要先存储当前线程的本地的数据，程序指针等，然后载入另一个线程的本地数据，程序指针等，最后才开始执行。所以多线程一般能提高程序运行效率，但也不能无节制地开启线程。 Java 实现多线程的两种方式多线程的两种创建方式分别为继承 Thread 类以及实现 Runnable 接口。一般用实现 Runnable 接口的方式。 我们通过调用线程的 start() 方法新建一个线程并启动它，下面是两种主要实现方式的示例。 继承 Thread 类123456789101112131415161718public class MyThread1 extends Thread &#123; private int count = 5; @Override public void run() &#123; for (int i = 5; i &gt; 0 &amp;&amp; count&gt;0; i--,count--) &#123; System.out.println(\"count----\" + count); &#125; &#125; public static void main(String[] args) &#123; new MyThread1().start(); new MyThread1().start(); new MyThread1().start(); &#125;&#125; 可以创建一个实例直接 start()，因为 Thread 类中定义了 start() 方法。 实现 Runnable 接口123456789101112131415161718public class MyThread2 implements Runnable&#123; private int count = 5; @Override public void run() &#123; for (int i = 5; i &gt; 0 &amp;&amp; count&gt;0; i--,count--) &#123; System.out.println(\"count----\" + count); &#125; &#125; public static void main(String[] args) &#123; MyThread2 myThread2 = new MyThread2(); new Thread(myThread2).start(); new Thread(myThread2).start(); new Thread(myThread2).start(); &#125;&#125; 但是他无法直接创建实例之后直接 start(),查看源码可以看到 Runnable 接口只有一个 run() 方法没有 start()方法，所以他必须由 Thread 来启动。 两者优缺点比较实现 Runable() 避免由于 Java 的单继承特性而带来的局限Java 使用 extends 关键字实现继承，可以理解成全盘接受了父类的特性。使用 implents 关键字实现接口，对类的功能进行拓展。要注意的是 Java 的继承是单继承，也就是只能继承一个父类。但是每个类都能实现多个接口。 当一个类需要实现多线程的时候，如果他已经继承了其他的父类则不能再继承 Thread 类，但是即使他实现了其他接口，他任然能实现 Runable 接口创建多线程。 但是当我们访问当前线程时需要使用Thread.currentThread()方法。 继承 Thread 类编写简单，方便理解通过继承 Thread 类的方法实现多线程，调用当前类只需要使用 this 关键字即可进行访问。但是使用这种方式之后就不能继承其他父类。 因此一般都是用实现 Runable() 接口的形式。本篇中使用显示创建线程的方式，而创建线程更好的方式是使用线程池。 多线程 run() 和 start() 的区别通过运行run()方法和start()方法，看看输出结果有什么区别。 run() 方法1234567891011121314151617181920212223public class MyThread3 extends Thread &#123; private int count = 5; @Override public void run() &#123; for (int i = 5; i &gt; 0 &amp;&amp; count&gt;0; i--,count--) &#123; System.out.println(this.currentThread().getName() + \"----count----\" + count); &#125; &#125; public static void main(String[] args) &#123; MyThread3 myThread1 = new MyThread3(); myThread1.setName(\"Thread1\"); myThread1.run(); MyThread3 myThread2 = new MyThread3(); myThread2.setName(\"Thread2\"); myThread2.run(); &#125;&#125; 输出结果为： 12345678910thread1----count----5thread1----count----4thread1----count----3thread1----count----2thread1----count----1thread2----count----5thread2----count----4thread2----count----3thread2----count----2thread2----count----1 Thread1 和 Thead2 只是在主线程上顺序执行，并没有开启新的线程。 start() 方法如果将 run() 改成 start()输出结果为: 1234567thread2---count----5thread3---count----5thread1---count----5thread3---count----3thread3---count----1thread2---count----4thread1---count----2 可以看到两个线程在抢占资源，成功创建了两个线程。 结果对比创建了两个相同的线程对象，但是结果有明显的差异。run() 的运行结果始终是顺次递减，线程1执行完之后再执行线程2，表明他们是在主线程上顺序运行，并没有开启新的线程。 但是使用start()方法则可以看到线程1和线程2在竞争资源，说明成功开启了两个线程。 线程 start() 和 run() 的区别先看一下 Runnable 接口 123public interface Runnable &#123; public abstract void run();&#125; 其中只有一个 run() 方法，注释是这么写的： When an object implementing interface Runnable is used to create a thread 所以用于实现了 run() 方法的对象可以被用于创建一个线程 再看 Thread 类 12345678910public class Thread implements Runnable &#123; /* What will be run. */ private Runnable target; @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; Thread 实现了 Runable 的接口,可以传入一个 Runnable，执行 Runnable 的 run() 方法,他说: If this thread was constructed using a separate Runnable run object, then that Runnable object’s run method is called; otherwise, this method does nothing and returns. 说明 Thread 类的作用就是用于运行 Runable 的实例，将实现了 Runable 方法的实例传入 Thread 类就可以运行该实例的run()方法 再看start() 方法的说明： 使用 start() 让线程会调用run()之后在虚拟机上执行 Causes this thread to begin execution; the Java Virtual Machine calls the run method of this thread. 12345678public synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException(); /*把线程加到线程组中，表示这个线程将会被执行*/ group.add(this); ... &#125; run() 只是在当前线程中启动，start() 才是真正新建一个线程并启动。 参考进程与线程的一个简单解释","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"},{"name":"Java并发编程","slug":"Java/Java并发编程","permalink":"http://shuiyujie.com/categories/Java/Java并发编程/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://shuiyujie.com/tags/并发编程/"}]},{"title":"说说zookeeper一般都有哪些使用场景？","slug":"分布式/zookeeper/说说zookeeper一般都有哪些使用场景？","date":"2017-11-15T10:17:16.000Z","updated":"2019-10-11T12:44:54.165Z","comments":true,"path":"post/ddbe5b18.html","link":"","permalink":"http://shuiyujie.com/post/ddbe5b18.html","excerpt":"本文介绍 zookeeper 的几个应用场景：分布式协调、分布式锁、元数据/配置信息管理、HA高可用性。","text":"本文介绍 zookeeper 的几个应用场景：分布式协调、分布式锁、元数据/配置信息管理、HA高可用性。 分布式协调 zookeeper 最开始就是用来解决分布式应用如何互相协调这样一个问题的。分布式协调是 zookeeper 的典型应用。 具体来说是起到这样的作用，系统 A 保存订单之后直接给用户返回结果，同时发一个消息到 MQ 里面，系统 B 订阅 MQ 里的消息，然后正式修改库存。这里有一个问题就是怎么让系统 A 知道系统 B 已经成功对库存进行了修改？ 这里就用 zookeeper 来做分布式协调，具体包括下面三个步骤： 系统 A 创建一个订单 orderId = 1，同时在 zk 中对 orderId=1 的 node 注册一个监听； 系统 B 更新订单之后修改 A 订阅的 node 的状态，比如说约定好修改为 finish_update； 系统 A 发现它订阅的节点发生了变化，就知道系统 B 成功对库存进行操作。 分布式锁 比如说现在连续有两个请求要对一个数据进行操作，系统 A 和系统 B 各自同时接到了一个请求，但是只能一台机器执行完另外一台再执行。 这个时候就可以用到分布式锁，假设系统 A 稍快一步，接到请求之后先从 zk 中获取了分布式锁，就可以在 zk 中创建一个 znode，并执行相关操作。此时系统 B 也尝试创建这个 znode 但是会发现创建不了，就会注册一个监听，等别人释放掉锁，它再来执行。 元数据/配置信息管理zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等都用 zk 来做一些元数据、配置信息的管理，dubbo 也推荐使用 zk 来作为注册中心。 HA高可用性zk 也可以用来做高可用，比如说系统 A 比较重要，我们用一台机器备用。系统 A 的机器挂了，备用机器立刻补位。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"zookeeper","slug":"分布式/zookeeper","permalink":"http://shuiyujie.com/categories/分布式/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://shuiyujie.com/tags/zookeeper/"}]},{"title":"zookeeper 的安装和配置","slug":"分布式/zookeeper/zookeeper的安装和配置","date":"2017-11-14T09:18:25.000Z","updated":"2019-10-04T12:37:49.128Z","comments":true,"path":"post/530b5399.html","link":"","permalink":"http://shuiyujie.com/post/530b5399.html","excerpt":"ZooKeeper 是一个分布式协调服务。其本身就是一个高可用的分布式程序，只需半数以上节点存活即可继续使用，所以使用中往往配置奇数台主机。他主要能提供主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务等。 本文演示在 CentOS 7 虚拟机部署和配置 zookeeper。","text":"ZooKeeper 是一个分布式协调服务。其本身就是一个高可用的分布式程序，只需半数以上节点存活即可继续使用，所以使用中往往配置奇数台主机。他主要能提供主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务等。 本文演示在 CentOS 7 虚拟机部署和配置 zookeeper。 下载首先需要下载安装包。前去下载地址下载安装包，本文使用的 zookeeper 版本是zookeeper-3.4.10。 下载完成之后将安装包传到服务器，我将其传到 apps 目录下： scp zookeeper-3.4.10.tar.gz root@192.168.2.222:/apps/ 解压安装包 sudo tar -zxvf zookeeper-3.4.10.tar.gz -C /apps/ 重命名解压文件 sudo mv zookeeper-3.4.10 zookeeper 配置环境变量修改配置文件 vi /etc/profile添加如下内容：# zookeeperexport ZOOKEEPER_HOME=/apps/zookeeperexport PATH=\\$PATH:$ZOOKEEPER_HOME/bin 注：ZOOKEEPER_HOME 为你自己的文件位置。 手动加载配置文件： source /etc/profile 修改 zookeeper 配置文件配置文件放置在 zookeeper/conf 目录下，查看目录文件有三个，其中zoo_sample.cfg是范例配置文件： 12$lsconfiguration.xsl log4j.properties zoo_sample.cfg 复制zoo_sample.cfg并重命名zoo.cfg,zoo.cfg就是需要的配置文件： cp zoo_sample.cfg zoo.cfgvi zoo.cfg 配置文件中添加或修改如下内容： 12345678# 数据目录dataDir=/home/hadoop/zookeeper/data# 日志目录dataLogDir=/home/hadoop/zookeeper/log# 包括自己在内的所有主机名称server.1=slave1:2888:3888 (主机名, 心跳端口、数据端口)server.2=slave2:2888:3888server.3=slave3:2888:3888 注意：slave1、slave2、slave3 是主机名即 hostname。所以需要配置主机名和 ip 的映射。 新建对应的文件夹: mkdir -m 755 zookeeper/datamkdir -m 755 zookeeper/log 配置 myid： echo 1 &gt; zookeeper/data/myid 给三台主机的名称分别配置为1,2,3。 最后：在另外两台主机上使用任意方法重复如上配置并修改主机名配置完成 启动 zookeeper进入启动目录 cd /apps/zookeeper/bin 启动 zkServer.sh start 查看状态 zkServer.sh status 12345Using config: /apps/zookeeper/bin/../conf/zoo.cfgMode: leader----Using config: /apps/zookeeper/bin/../conf/zoo.cfgMode: follower 以上，zookeeper 的基本安装配置完毕。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"zookeeper","slug":"分布式/zookeeper","permalink":"http://shuiyujie.com/categories/分布式/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://shuiyujie.com/tags/zookeeper/"}]},{"title":"zookeeper的基本概念","slug":"分布式/zookeeper/zookeeper的基本概念","date":"2017-11-14T08:19:55.000Z","updated":"2019-10-04T12:39:42.427Z","comments":true,"path":"post/e7973620.html","link":"","permalink":"http://shuiyujie.com/post/e7973620.html","excerpt":"本文介绍 zookeeper 的一些基本概念。","text":"本文介绍 zookeeper 的一些基本概念。 数据模型 分层结构 树形结构中的每个节点叫做Znode 每个Znode都有数据ͧ(byte[]类型ͨ)，也可以有子节点 节点路径 斜线分隔:/Zoo/Duck 没有相对路径 通过数据结构stat来孓储数据的变化 ACL的变化和时间戳 数据发生变化时，版本号会递增 可以对Znode中的数据进行读写操作 数据节点(Znode) 不是机器的意思 Zk树形结构中的数据节点，用于存储数据 持久节点：一旦创建,除非主动调用删除操作,否则一直存储在zk上 临时节点：与客户端的会话绑定，一旦客户端绘画是小，这个客户端创建的所有临时节点都会被移除 SEQUENTIAL Znode：创建临时节点时，如果设置属性 SEQUENTIAL，则会自动在节点名后面追加一个整型数字 持久节点启动一个 zkCli 创建一个持久节点，即使创建节点的会话关闭，该节点也不会被删除。 1234[zk: localhost:2181(CONNECTED) 10] create /node nodedataCreated /node[zk: localhost:2181(CONNECTED) 11] ls /[node, zookeeper] 临时节点启动一个 zkCli 之后创建临时节点 1234[zk: localhost:2181(CONNECTED) 3] create -e /node1 node1dataCreated /node1[zk: localhost:2181(CONNECTED) 4] ls /[zookeeper, node1] 将创建临时节点的会话关闭之后，临时节点将会消失。(可以再起一个 zkCli 验证) 顺序节点创建数据节点 123456[zk: localhost:2181(CONNECTED) 12] create -s /node/seq 321Created /node/seq0000000000[zk: localhost:2181(CONNECTED) 13] create -s /node/seq 322Created /node/seq0000000001[zk: localhost:2181(CONNECTED) 15] ls /node[seq0000000000, seq0000000001] 集群角色 Leader: 为客户端提供读和写服务 Follower: 提供读服务，所有写服务都需要转交给Leader角色，参与选举 Observer: 提供读服，不参与选举过程，一般是为了增强zk集群的读请求并发能力 会发(Session) Zk的客户与zk的服务端之间的连接 通过心跳检测保持客户端连接的存活 接收来自服务端的watch事件通知 可以设置超时时间 版本 Version: 当前 Znode 的版本 Cversion：当前 Znode 的子节点的版本 Aversion：当前 Znode 的 ANL(访问控制)版本 Watcher 作用于 Znode 节点上 多种事件通知：数据更新，子节点状态等 ACL Access Control Lists 类似于linux/unix的权限控制 CREATE：创建子节点的权限 READ:获取节点数据和子节点列表的权限 WRITE：更新节点数据的权限 DELETE：删除子节点的权限 ADMIN：设置节点ACL的权限 CREATE和DELETE是针对子节点的权限控制。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"zookeeper","slug":"分布式/zookeeper","permalink":"http://shuiyujie.com/categories/分布式/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://shuiyujie.com/tags/zookeeper/"}]},{"title":"zookeepre源码编译","slug":"分布式/zookeeper/zookeepre源码编译","date":"2017-11-14T08:18:25.000Z","updated":"2019-10-04T12:39:51.579Z","comments":true,"path":"post/5ddebf1a.html","link":"","permalink":"http://shuiyujie.com/post/5ddebf1a.html","excerpt":"","text":"我们首先 fork 一份 zookeeper 在 github 上的项目，地址为：https://github.com/apache/zookeeper。再将 fork 之后的项目 clone 到本地。 我们不使用最新版本的代码，而是使用 3.4.11 这个版本的，所以我们根据 tag，checkout 一个分支出来： 1git checkout -b zookeeper3.4.11 release-3.4.11 由于 zookeeper 是用 ant 构建的，所以要安装 ant。我使用的是 macOS，直接用 homebrew 安装比较方便： 12brew updatebrew install ant 之后用 ant 构建一个 eclipse 项目 1ant eclipse 最后，将导入项目。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"zookeeper","slug":"分布式/zookeeper","permalink":"http://shuiyujie.com/categories/分布式/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://shuiyujie.com/tags/zookeeper/"}]},{"title":"CentOS 7 配置","slug":"Linux/CentOS-配置","date":"2017-11-14T01:57:16.000Z","updated":"2019-05-14T13:52:10.372Z","comments":true,"path":"post/99732d8a.html","link":"","permalink":"http://shuiyujie.com/post/99732d8a.html","excerpt":"本文说明如何在 Parallels 虚拟机上安装 CentOS 7 以及对其的一些基本配置，几个常用配置文件的说明和 JDK, Tomcat 和 Mysql 的安装。","text":"本文说明如何在 Parallels 虚拟机上安装 CentOS 7 以及对其的一些基本配置，几个常用配置文件的说明和 JDK, Tomcat 和 Mysql 的安装。 CentOS 7 基本配置安装 centos 7首先下载镜像文件，我装的是 CentOS-7-x86_64-Minimal-1708.iso。我从网易的镜像下载，下载完后上传了一份到百度云盘： 网易镜像地址：http://mirrors.163.com/centos/7/isos/x86_64/百度云盘链接:http://pan.baidu.com/s/1gf7w1uN 密码:5lbx 之后按照引导即可完成，有问题可以参看 Mac利用PD虚拟机安装Centos7。 更换 yum 源yum 是一种包管理工具，就像 Java 中常用的 maven，或者安卓中用的 Gradle，再或者 Node.js 中的 npm。通过他我们能够统一地下载安装软件。但是由于网络原因下载缓慢，所以要把源换成国内的。更换过程如下： 备份： mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 去下面的源下载对应版本repo文件, 放入/etc/yum.repos.d/ repo文件下载地址： 网易:http://mirrors.163.com/.help/centos.html阿里:http://mirrors.aliyun.com/repo/ 运行以下命令生成缓存 yum clean allyum makecache yum 源配置完成 通过 yum 安装一些基本软件net-tools 提供dig, nslookup, ipconfig等，用于配置网络： yum install net-tools 添加 wget 下载文件： yum install wget 创建一个普通用户并赋予 root 权限用普通账号进行登录可以避免 root 用户进行错误操作，而且用普通用户登录就像给服务器建立了两道墙，必须先用普通用户登录再设置能用 root 账号登录，所以后面还要配置禁止 root 用户用过 SSH 登录。 创建普通用户 useradd shuipasswd shui输入密码 这个普通用户有时也需要使用 root 权限，所以讲他加入到sudoers 用户组，允许其使用sudo临时调用 root 权限 echo ‘shui ALL=(ALL) ALL’&gt;&gt; /etc/sudoerstail -1 /etc/sudoersshui ALL=(ALL) ALL 禁止 root 使用 ssh 登入进入配置文件： /etc/ssh/sshd_config 找到如下语句进行修改 PermitRootLogin yes 把它改成 PermitRootLogin no 重启 sshd systemctl restart sshd.service 这样别人就要必须要获取普通用户账号密码，然后才能破解 root 将防火墙换成 iptablesCentOS 7.0 默认使用的是 firewall 作为防火墙，常用的是 iptables。先关闭 firewall 再安装 iptables。 关闭 firewall 1234#停止firewallsystemctl stop firewalld.service #禁止firewall开机启动systemctl disable firewalld.service 安装 iptables yum -y install iptables-services 启动 iptables 1234#重启防火墙使配置生效systemctl restart iptables.service #设置防火墙开机启动systemctl enable iptables.service 关闭 SELinux查看 SELinux 状态 /usr/sbin/sestatus -v | grep SELinuxSELinux status: enabled # 表示为开启状态 永久关闭，重启生效 vi /etc/selinux/config#修改内容如下SELINUX=disabled 更改系统语言查看当前系统语言 12345localectlSystem Locale: LANG=zh_CN.UTF-8 VC Keymap: cn X11 Layout: cn 查看系统中存在的语言列表，因为很长通过 grep 来查找需要的语言是否存在 12345678localectl list-locales | grep USen_USen_US.iso88591en_US.iso885915en_US.utf8.. 设置自己要改的语言 1localectl set-locale LANG=en_US.UTF-8 参考：CentOS yum 源的配置与使用 安裝 CentOS 7 後必做的七件事 Change system language in centos 7 一些配置文件的修改修改主机名可以用hostname来查看你的主机名，修改主机名配置文件： vi /etc/hostname 重启生效 reboot 修改 ip 地址 vi /etc/sysconfig/network-scripts/ifcfg-eth0 在网卡中配置如下内容可以设置静态 ip 12345678DEVICE=eth0TYPE=EthernetONBOOT=yesBOOTPROTO=staticIPADDR=192.168.0.11NETMASK=255.255.255.0DNS1=192.168.0.1DNS2=8.8.8.8 配置完之后保存重启网络 service network restart 设置 ip 和 hostname 的映射 vi /etc/hosts 添加上 ip 和 hostname 的键值对，举例前面设置hostname为 main 设置ip为 192.168.0.11 在最后追加一行： 123127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.0.11 main 之后可以用主机名进行访问 安装 JDK将 JDK 安装包上传，上传可以使用 scp,rz 从本机上传，也可以直接下载。 解压 JDK 安装包： tar -zxvf jdk-8u151-linux-x64.tar.gz -C /usr/local 配置环境变量： 12345vi /etc/profile# 追加内容如下export JAVA_HOME=/usr/local/jdk1.8.0_151export PATH=$PATH:$JAVA_HOME/bin 加载环境变量： source /etc/profile 更多 linux配置java环境变量(详细) 安装 Tomcat点击此处下载安装包。本文使用的版本是apache-tomcat-8.5.23.tar.gz，下载后再安装包传到 Linux 主机。 解压安装包： tar -zxvf apache-tomcat-8.5.23.tar.gz -C /usr/local/ 启动 tomacat /usr/local/apache-tomcat-8.5.23/bin/startup.sh 查看 tomcat 进程 ps -ef | grep tomcat 访问http://192.168.2.224:8080/即主机 ip + tomcat 端口。小猫出现表示成功。 安装 MysqlMysql 安装之前先留个心，安装过程中可能会需要记录密码，有些默认没有密码，请留心有没有提示记录密码，然而没看见也没关系就是要再折腾一下。本文演示用yum安装 mysql 数据库。 配置YUM源1234567891011# 下载mysql源安装包shell&gt; wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm# 安装mysql源shell&gt; yum localinstall mysql57-community-release-el7-8.noarch.rpm# 检查mysql源是否安装成功yum repolist enabled | grep &quot;mysql.*-community.*&quot;# 成功显示如下mysql-connectors-community/x86_64 MySQL Connectors Community 42mysql-tools-community/x86_64 MySQL Tools Community 51mysql57-community/x86_64 MySQL 5.7 Community Server 227 安装MySQL yum install mysql-community-server 启动 mysql systemctl start mysqld 查看MySQL的启动状态 systemctl status mysqld 修改 root 默认密码mysql安装完成之后，在/var/log/mysqld.log文件中给root生成了一个默认密码。通过下面的方式找到root默认密码，然后登录mysql进行修改： grep ‘temporary password’ /var/log/mysqld.log2017-11-15T16:26:37.970235Z 1 [Note] A temporary password is generated for root@localhost: d54aqgZr69&gt;d 此时默认的密码就是d54aqgZr69&gt;d，用 root 身份登录之后修改密码： mysql -uroot -pALTER USER ‘root‘@’localhost’ IDENTIFIED BY ‘MyNewPass1!’; 注：mysql 的密码需要有一定复杂度 更多请参考:CentOS7下安装MySQL5.7安装与配置（YUM）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]},{"title":"Linux 权限","slug":"Linux/Linux-权限","date":"2017-11-13T11:50:41.000Z","updated":"2019-06-29T15:38:33.233Z","comments":true,"path":"post/40efde90.html","link":"","permalink":"http://shuiyujie.com/post/40efde90.html","excerpt":"Linux 是多用户的操作系统，同一时间可以有多个用户同时操作同一台计算机。为了让用户和用户之间不相互影响，必须要有一种机制来保障每一个用户的行为不会越界对其他用户造成不必要的影响。","text":"Linux 是多用户的操作系统，同一时间可以有多个用户同时操作同一台计算机。为了让用户和用户之间不相互影响，必须要有一种机制来保障每一个用户的行为不会越界对其他用户造成不必要的影响。 用户和用户组了解 Linux 的权限管理首先要了解用户和组的概念。Linux 引入了用户和用户组的概念，一个用户可以拥有多个文件和目录， 用户对这个文件或目录的访问权限拥有控制权。同时用户可以属于一个或者多个用户组，用户组中的用户拥有同属于这个组的对文件的访问控制权。 Linux 会给每个用户分配一个 uid（标识用户） 和 gid (标识用户组)，在创建用户的时候默认就会创建一个和用户名同名的用户组，我可以用id命令来查看： iduid=0(root) gid=0(root) 组=0(root) 环境=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 这些信息都是配置在配置文件中的，我们有必要了解这些信息在哪些配置文件中,他们主要配置在三个文件中/etc/shadow,/etc/passwd和/etc/group。 用户帐户 定义在/etc/passwd 文件里面，用户组定义在/etc/group 文件里面。当用户帐户和用户组创建以后， 这些文件随着文件/etc/shadow 的变动而修改，文件/etc/shadow 包含了关于用户密码的信息。 对于每个用户帐号，文件/etc/passwd 定义了用户（登录）名、uid、gid、帐号的真实姓名、家目录 和登录 shell。如果你查看一下文件/etc/passwd 和文件/etc/group 的内容，你会注意到除了普通 用户帐号之外，还有超级用户（uid 0）帐号，和各种各样的系统用户。 基本的用户管理创建一个普通用户添加用户 useradd shui 设置密码 passwd shui 按提示输入密码即可 这样就会创建一个新的用户，我们可以在/etc/group中看到新建的默认用户组的信息shui:x:1000:也可以再/etc/shadow,/etc/passwd看到相关信息。 为用户配置sudo权限配置sudo权限就是将永不加入到sudoers用户组中，这是一个特殊的用户组，在这个用户组中的用户可以通过在命令前面添加sudo临时获取root的权限。 用root编辑 vi /etc/sudoers 在文件的如下位置，为hadoop添加一行即可 root ALL=(ALL) ALLshui ALL=(ALL) ALL 或者使用如下语句也是相同效果 echo ‘shui ALL=(ALL) ALL’&gt;&gt; /etc/sudoers 然后，hadoop用户就可以用sudo来执行系统级别的指令 文件权限的操作前面说过用户具有对文件的的访问控制权，访问控制权具有可读、可写、可操作三种，我们来具体看一下 linux 文件权限的描述格式解读用ll命令来看一下根目录下的文件信息,我们复制其中的一部分： 1234dr-xr-xr-x. 5 root root 4096 11月 2 10:34 bootdrwxr-xr-x. 20 root root 3140 11月 2 10:36 devdrwxr-xr-x. 74 root root 8192 11月 2 20:48 etcdrwxr-xr-x. 3 root root 18 11月 2 19:03 home 每个文件的信息大同小异，关注最前面的十个字符，他就表示文件的操作权限。 以/etc目录的信息为例drwxr-xr-x. 74 root root 8192 11月 2 20:48 etc 第一位表示文件类型,d 表示这是一个文件夹，文件属性还有以下内容 后面的九位分别代表用户权限、用户组权限和其他用户权限 r:表示可读 read w:表示可惜 write x:表示可执行 excute /etc 目录的权限 rwxr-xr-x 就表示，root 用户具有可读可写可操作的权限，文件所有者的组成员可以访问该目录，但是不能新建、重命名、删除文件，其他成员可以访问该目录，但是不能新建、重命名、删除文件。 chmod 更改权限只有root和文件的所有者才能更改文件的权限，更改文件的权限有两种方式，一种是使用符号，另一种是使用八进制数字。 符号方式修改文件权限符号即前面rwx对应的含义，ugoa则分别表示用户(user)、用户组(group)、其他人(other)和所有(all),一组示例来演示： 123chmod g-rw haha.dat 表示将haha.dat对所属组的rw权限取消chmod o-rw haha.dat 表示将haha.dat对其他人的rw权限取消chmod u+x haha.dat 表示将haha.dat对所属用户的权限增加x 八进制的方式来修改权限一个八进制数字可以表示三个二进制数，二进制的 111 对应八进制的 7，而 111 正好可以表示 rwx 的含义，1 则代表有权限 0 则代表没有权限。 八进制 二进制 符号 0 000 — 1 001 —-x 2 010 -w- 3 011 -wx 4 100 r– 5 101 r-x 6 110 rw- 7 111 rwx 例如： chmod 664 haha.dat修改成 rw-rw-r– 一些人喜欢使用八进制表示法，而另一些人则非常喜欢符号表示法。符号表示法的优点是， 允许你设置文件模式的某个属性，而不影响其他的属性。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://shuiyujie.com/categories/Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]},{"title":"zookeeper概述","slug":"分布式/zookeeper/zookeeper的概述","date":"2017-11-13T08:18:25.000Z","updated":"2019-10-04T12:39:25.296Z","comments":true,"path":"post/6e7f6d53.html","link":"","permalink":"http://shuiyujie.com/post/6e7f6d53.html","excerpt":"Q: zookeeper 解决了什么问题？ 相对于开发在一台计算机上运行的单个程序，如何让一个应用中多个独立的程序协同工作是一件非常困难的事情。开发这样的应用，很容易让很多开发人员陷入如何使多个程序协同工作的逻辑中，最后导致没有时间更好地思考和实现他们自己的应用程序逻辑；又或者开发人员对协同逻辑关注不够，只是用很少的时间开发了一个简单脆弱的主协调器，导致不可靠的单一失效点。","text":"Q: zookeeper 解决了什么问题？ 相对于开发在一台计算机上运行的单个程序，如何让一个应用中多个独立的程序协同工作是一件非常困难的事情。开发这样的应用，很容易让很多开发人员陷入如何使多个程序协同工作的逻辑中，最后导致没有时间更好地思考和实现他们自己的应用程序逻辑；又或者开发人员对协同逻辑关注不够，只是用很少的时间开发了一个简单脆弱的主协调器，导致不可靠的单一失效点。 Q: zookeeper 是怎么解决应用中多个独立程序协同工作的问题的？ ZooKeeper从文件系统API得到启发，提供一组简单的API，使得开发人员可以实现通用的协作任务，包括选举主节点、管理组内成员关系、管理元数据等。ZooKeeper包括一个应用开发库（主要提供Java和C两种语言的API）和一个用Java实现的服务组件。ZooKeeper的服务组件运行在一组专用服务器之上，保证了高容错性和可扩展性。 当你决定使用ZooKeeper来设计应用时，最好将应用数据和协同数据独立开。比如，网络邮箱服务的用户对自己邮箱中的内容感兴趣，但是并不关心由哪台服务器来处理特定邮箱的请求。在这个例子中，邮箱内容就是应用数据，而从邮箱到某一台邮箱服务器之间的映射关系就是协同数据（或称元数据）。整个ZooKeeper服务所管理的就是后者 Q: zookeeper的应用场景？ Apache HBase：HBase是一个通常与Hadoop一起使用的数据存储仓库。在HBase中，ZooKeeper用于选举一个集群内的主节点，以便跟踪可用的服务器，并保存集群的元数据。 Apache Kafka：Kafka是一个基于发布-订阅（pub-sub）模型的消息系统。其中ZooKeeper用于检测崩溃，实现主题（topic）的发现，并保持主题的生产和消费状态。 Apache Solr：Solr是一个企业级的搜索平台。Solr的分布式版本命名为SolrCloud，它使用ZooKeeper来存储集群的元数据，并协作更新这些元数据。 Yahoo！Fetching Service：Yahoo！Fetching Service是爬虫实现的一部分，通过缓存内容的方式高效地获取网页信息，同时确保满足网页服务器的管理规则（比如robots.txt文件）。该服务采用ZooKeeper实现主节点选举、崩溃检测和元数据存储。 Facebook Messages：Facebook推出的这个应用（http://on.fb.me/1a7uViK）集成了email、短信、Facebook聊天和Facebook收件箱等通信通道。该应用将ZooKeeper作为控制器，用来实现数据分片、故障恢复和服务发现等功能。 Q: zookeeper不适用的场景？整个ZooKeeper的服务器集群管理着应用协作的关键数据。ZooKeeper不适合用作海量数据存储。对于需要存储海量的应用数据的情况，我们有很多备选方案，比如说数据库和分布式文件系统等。因为不同的应用有不同的需求，如对一致性和持久性的不同需求，所以在设计应用时，最佳实践还是应该将应用数据和协同数据独立开。 Q: 使用 zookeeper 时需要注意的问题？ 消息延迟：消息传输可能会发生任意延迟，比如，因为网络拥堵。这种任意延迟可能会导致不可预期的后果。比如，根据基准时钟，进程P先发送了一个消息，之后另一个进程Q发送了消息，但是进程Q的消息也许会先完成传送。 处理器性能：操作系统的调度和超载也可能导致消息处理的任意延迟。当一个进程向另一个进程发送消息时，整个消息的延时时间约等于发送端消耗的时间、传输时间、接收端的处理时间的总和。如果发送或接收过程需要调度时间进行处理，消息延时会更高。 时钟偏移：使用时间概念的系统并不少见，比如，确定某一时间系统中发生了哪些事件。处理器时钟并不可靠，它们之间也会发生任意的偏移。因此，依赖处理器时钟也许会导致错误的决策。 Q: ZooKeeper的成功和注意事项不得不指出，完美的解决方案是不存在的，我们重申ZooKeeper无法解决分布式应用开发者面对的所有问题，而是为开发者提供了一个优雅的框架来处理这些问题。多年以来，ZooKeeper在分布式计算领域进行了大量的工作。Paxos算法[1]和虚拟同步技术（virtual synchrony）[2]给ZooKeeper的设计带来了很大影响，通过这些技术可以无缝地处理所发生的某些变化或情况，并提供给开发者一个框架，来应对无法自动处理的某些情况。 ZooKeeper最初由雅虎研究院开发，用于处理大量的大型分布式应用。我们注意到，这些应用在分布式协作方面的处理方式并不妥当，这些系统的部署存在单点故障问题或很脆弱，另一方面，开发者在分布式协作方面花费了大量的时间和精力，导致开发者没有足够的资源来关注应用本身的功能逻辑。我们还注意到，这些应用都在基本协作方面有相同的需求。因此，我们开始着手设计一套通用的解决方案，通过某些关键点让我们可以一次实现就能应用于大多数不同的应用中。ZooKeeper已经被证实更加通用，其受欢迎程度超越了我们的想象。 多年来，我们发现人们可以很容易地部署ZooKeeper集群，轻松通过这个集群开发应用，但实际上，在使用ZooKeeper时，有些情况ZooKeeper自身无法进行决策而是需要开发者自己做出决策，有些开发者并不完全了解这些。编写本书的其中一个目的就是让开发者了解如何更有效地使用ZooKeeper，以及为什么需要这样做。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://shuiyujie.com/categories/分布式/"},{"name":"zookeeper","slug":"分布式/zookeeper","permalink":"http://shuiyujie.com/categories/分布式/zookeeper/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://shuiyujie.com/tags/zookeeper/"}]},{"title":"Ubuntu配置JDK","slug":"Java/Ubuntu配置JDK","date":"2017-09-03T13:13:12.000Z","updated":"2019-09-03T13:16:07.640Z","comments":true,"path":"post/1d66a96c.html","link":"","permalink":"http://shuiyujie.com/post/1d66a96c.html","excerpt":"","text":"wget下载jdk8 1wget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz\" 解压 1tar xzf jdk-8u141-linux-x64.tar.gz 配置环境变量 vim /etc/profile 1234JAVA_HOME=/home/shui/java/jdk1.8.0_141JRE_HOME=/home/shui/java/jdk1.8.0_141/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin 使配置生效 123export JAVA_HOME JRE_HOME CLASS_PATH PATHsource /etc/profile","categories":[{"name":"Java","slug":"Java","permalink":"http://shuiyujie.com/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://shuiyujie.com/tags/java/"},{"name":"linux","slug":"linux","permalink":"http://shuiyujie.com/tags/linux/"}]}]}